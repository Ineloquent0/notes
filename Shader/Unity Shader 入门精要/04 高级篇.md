高级篇涵盖了一些 Shader 的高级用法，例如，如何实现屏幕特效、利用法线和深度缓冲，以及非真实感渲染等，同时，我们还会介绍一些针对移动平台的优化技巧。


# 第12章 屏幕后处理效果

**屏幕后处理效果（screen post-processing effects）** 是游戏中实现屏幕特效的常见方法。

可参考文档：
* [Unity Image Effects](https://docs.unity3d.com/510/Documentation/Manual/comp-ImageEffects.html)
* [GPU Gems 3](https://developer.nvidia.com/gpugems/gpugems3/contributors)



## 12.1 建立一个基本的屏幕后处理脚本系统

屏幕后处理通常指在渲染完整个场景得到屏幕图像后，再对这个图像进行一系列操作，实现各种屏幕特效。如景深（Depth of Field）、运动模糊（Motion Blur）等。

**OnRenderImage** 函数是 Unity 内置的渲染管线中的一个函数，用于得到渲染后的屏幕图像，即抓取屏幕。它的函数声明如下：
``` csharp
// Unity 会把当前渲染得到的图像存储在第一个参数对应的源渲染纹理中，
// 通过函数中的一系列操作后，再把目标渲染纹理显示到屏幕上
MonoBehaviour.OnRenderImage(RenderTexture src, RenderTexture dest)
```

**Graphics.Blit** 函数可以实现图像的拷贝，并对图像进行一些操作。它有3种函数声明：
``` csharp
// src: 源纹理
// dest: 目标渲染纹理，如果为 null，则渲染到屏幕上
public static void Blit(Texture src, RenderTexture dest)

// mat: 材质，用于对图像进行操作
// pass: 默认为 -1，将会调用 Shader 内的所有 Pass。否则，只会调用给定索引的 Pass
public static void Blit(Texture src, RnederTexture dest, Material mat, int pass = -1)

public static void Blit(Texture src, Material mat, int pass = -1)
```

默认情况下，OnRenderImage 函数会在所有不透明和透明的 Pass 执行完毕后被调用，以便对场景中所有游戏对象都产生影响。但有时，会希望在不透明的 Pass 执行完毕后立即调用 OnRenderImage 函数，从而不对透明物体产生任何影响。可在 OnRenderImage 函数前添加 **ImageEffectOpaque** 属性实现这样的目的。


实现屏幕后处理效果过程：
1. 在摄像机中添加一个用于屏幕后处理的脚本。在脚本中，我们会实现 OnRenderImage 函数来获取当前屏幕的渲染纹理。
2. 调用 Graphics.Blit 函数使用特定的 Shader 对当前图像进行处理，再把返回的渲染纹理显示到屏幕上。
3. 对于复杂的屏幕特效，可能需要多次调用 Graphics.Blit 函数来对上一步输出结果进行下一步处理。


在进行屏幕后处理之前，我们需要检查一系列条件是否满足（如平台是否支持渲染纹理和屏幕特效，是否支持当前Shader等）。为此我们创建了一个用于屏幕后处理效果的基类 PostEffectsBase.cs。

``` csharp
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

// 编辑器状态下运行
[ExecuteInEditMode]
// 需要在相机上添加组件
[RequireComponent(typeof(Camera))]
public class PostEffectsBase : MonoBehaviour
{
    private void Start()
    {
        CheckResources();
    }


    /// <summary>
    /// 检查各种资源和条件是否满足
    /// </summary>
    protected void CheckResources()
    {
        bool isSupported = CheckSupport();

        if (!isSupported)
        {
            NotSupported();
        }
    }

    /// <summary>
    /// 检查平台是否支持
    /// </summary>
    protected bool CheckSupport()
    {
        if(!SystemInfo.supportsImageEffects || !SystemInfo.supportsRenderTextures)
        {
			Debug.LogWarning("This platform does not support image effects or render textures.");
			return false;
		}
		
		return true;
    }

    /// <summary>
    /// 平台不支持时的处理
    /// </summary>
    protected void NotSupported()
    {
        enabled = false;
    }

    /// <summary>
    /// 每个屏幕后处理效果通常需要指定一个Shader来创建一个用于处理渲染纹理的材质
    /// </summary>
    /// <param name="shader">该特效需要使用的Shader</param>
    /// <param name="material">后处理的材质</param>
    /// <returns>Shader可用的话则返回该Shader对应的材质，否则返回null</returns>
    protected Material CheckShaderAndCreateMaterial(Shader shader, Material material)
    {
        if (shader == null)
            return null;

        if (shader.isSupported && material && material.shader == shader)
            return material;
        
        if (!shader.isSupported)
            return null;
        else{
            material = new Material(shader);
            material.hideFlags = HideFlags.DontSave;
            if(material)
                return material;
        }
        return null;
    }
}
```



## 12.2 调整屏幕的亮度、饱和度和对比度

新建 C# 脚本 BrightnessSaturationAndContrast.cs，并继承 12.1 节中的基类：

``` csharp
using UnityEngine;

public class BrightnessSaturationAndContrast : PostEffectsBase
{
    public Shader briSatConShader;
    private Material briSatConMaterial;
    public Material material
    {
        get
        {
            briSatConMaterial = CheckShaderAndCreateMaterial(briSatConShader, briSatConMaterial);
            return briSatConMaterial;
        }
    }

    // 提供调整亮度、饱和度、对比度的参数
    [Range(0.0f, 3.0f)]
    public float brightness = 1.0f;

    [Range(0.0f, 3.0f)]
    public float saturation = 1.0f;

    [Range(0.0f, 3.0f)]
    public float contrast = 1.0f;


    private void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        // 检查材质是否可用，如果可用，就把参数传递给材质，再调用 Graphics.Blit 进行处理
        // 否则直接把原图显示到屏幕上
        if (material != null)
        {
            material.SetFloat("_Brightness", brightness);
            material.SetFloat("_Saturation", saturation);
            material.SetFloat("_Contrast", contrast);
            Graphics.Blit(src, dest, material);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }
}

```

新建 Shader 命名为 Chapter12-BrightnessSaturationAndContrast ：

``` hlsl
Shader "Unity Shaders Book/Chapter 12/Brightness Saturation And Contrast"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _Brightness ("Brightness", Float) = 1
        _Saturation ("Saturation", Float) = 1
        _Contrast ("Contrast", Float) = 1
    }
    SubShader
    {
        // 定义用于屏幕后处理的 Pass
        Pass
        {
            // 屏幕后处理 Shader 的“标配”
            ZTest Always  Cull Off  ZWrite Off


            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            
            #include "UnityCG.cginc"


            struct v2f
            {
                float2 uv : TEXCOORD0;
                float4 vertex : SV_POSITION;
            };

            sampler2D _MainTex;
            half _Brightness;
            half _Saturation;
            half _Contrast;

            v2f vert (appdata_img v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                o.uv = v.texcoord;
                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                fixed4 renderTex = tex2D(_MainTex, i.uv);

                // 亮度
                fixed3 finalColor = renderTex.rgb * _Brightness;

                // 计算饱和度
                // 饱和度为0时，颜色为黑白
                fixed luminance = dot(finalColor, fixed3(0.2125, 0.7154, 0.0721));
                fixed3 luminanceColor = fixed3(luminance, luminance, luminance);
                finalColor = lerp(luminanceColor, finalColor, _Saturation);

                // 计算对比度
                // 对比度为0时，颜色为灰色
                fixed3 avgColor = fixed3(0.5, 0.5, 0.5);
                finalColor = lerp(avgColor, finalColor, _Contrast);

                return fixed4(finalColor, renderTex.a);
            }
            ENDCG
        }
    }
    // 关闭 Fallback
    FallBack Off
}
```



## 12.3 边缘检测

边缘检测原理是利用一些边缘检测算子对图像进行 **卷积（convolution）** 操作。

### 12.3.1 什么是卷积

在图像处理中，卷积操作指的就是使用一个 **卷积核（kernel）** 对一张图像中的每个像素进行系列操作。卷积核通常是一个四方形网格结构（例如2x2、3x3的方形区域），该区域内每个方格都有一个权重值。
当对图像中的某个像素进行卷积时，我们会把卷积核的中心放置于该像素上，如图12.4所示，翻转核之后再依次计算核中每个元素和其覆盖的图像像素值的乘积并求和，得到的结果就是该位置的新像素值。

![图12.4 卷积核与卷积](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.4.jpg)

卷积操作可以实现很多常见的图像处理效果，例如图像模糊、边缘检测等。例如：均值模糊可以使用3x3的卷积核，核内每个元素的值均为1/9。


### 12.3.2 常见的边缘检测算子

如果相邻像素之间存在差别明显的颜色、亮度、纹理等属性，可以认为他们之间有一条边界，这种相邻像素之间的差异用 **梯度（gradient）** 来表示。边缘处的梯度绝对值会比较大，基于这样的理解，有几种不同的边缘检测算子被先后提出来。

![图12.5 3种常见的边缘检测算子](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.5.jpg)

它们都包含了两个方向的卷积核，分别用于检测水平方向和竖直方向上的边缘信息。在进行边缘检测时，我们需要对每个像素分别进行一次卷积计算，得到两个方向上的梯度值 Gx 和 Gy，而整体的梯度可按下面的公式计算而得：

$$
G = \sqrt{G_x^2 + G_y^2}
$$

由于上述计算包含了开根号操作，出于性能考虑，我们有时会使用绝对值操作来代替开根号操作：

$$
G = |G_x| + |G_y|
$$

当得到梯度 G 后，我们就可以根据此来判断哪些像素对应了边缘（梯度值越大，越有可能是边缘点）。


### 12.3.3 实现

新建脚本 EdgeDetection.cs，并继承 12.1 节中的基类：

``` csharp
using UnityEngine;

public class EdgeDetection : PostEffectsBase
{
    public Shader edgeDetectShader;

    private Material edgeDetectMaterial;
    public Material material{
        get{
            edgeDetectMaterial = CheckShaderAndCreateMaterial(edgeDetectShader, edgeDetectMaterial);
            return edgeDetectMaterial;
        }
    }

    // 提供用于调整边缘线强度、描边颜色以及背景颜色的参数
    // 当 edgesOnly 为 1.0 时，仅显示边缘线，不显示原渲染图像，否则边缘线会叠加到原渲染图像上
    [Range(0.0f, 1.0f)]
    public float edgeOnly = 0.0f;
    public Color edgeColor = Color.black;
    public Color backgroundColor = Color.white;

    private void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (material!= null){
            material.SetFloat("_EdgeOnly", edgeOnly);
            material.SetColor("_EdgeColor", edgeColor);
            material.SetColor("_BackgroundColor", backgroundColor);
            Graphics.Blit(src, dest, material);
        }
        else{
            Graphics.Blit(src, dest);
        }
    }
}
```

新建 Shader 命名为 Chapter12-EdgeDetection：

``` hlsl
Shader "Unity Shaders Book/Chapter 12/Edge Detection"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _EdgeOnly ("Edge Only", Range(0, 1)) = 0
        _EdgeColor ("Edge Color", Color) = (0, 0, 0, 1)
        _BackgroundColor ("Background Color", Color) = (1, 1, 1, 1)
    }
    SubShader
    {
        // 定义用于屏幕后处理的 Pass
        Pass
        {
            // 屏幕后处理 Shader 的“标配”
            ZTest Always  Cull Off  ZWrite Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            
            #include "UnityCG.cginc"

            sampler2D _MainTex;
            // 主纹理的纹素大小（例如：一张512 * 512的纹理，纹素大小为1/512）
            // 利用纹素，做相邻区域内纹理采样时，计算各相邻区域的纹理坐标
            half4 _MainTex_TexelSize;
            fixed _EdgeOnly;
            fixed4 _EdgeColor;
            fixed4 _BackgroundColor;

            struct v2f
            {
                float4 vertex : SV_POSITION;
                // 定义一个维数为9的数组，对应了Sobel算子采样时需要的9个邻域纹理坐标
                half2 uv[9] : TEXCOORD0;
            };

            v2f vert (appdata_img v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);

                // 把计算采样纹理坐标的代码从片元着色器转移到顶点着色器中，可以减少运算，提高性能
                half2 uv = v.texcoord;
                o.uv[0] = uv + half2(-1, -1) * _MainTex_TexelSize.xy;
                o.uv[1] = uv + half2(0, -1) * _MainTex_TexelSize.xy;
                o.uv[2] = uv + half2(1, -1) * _MainTex_TexelSize.xy;
                o.uv[3] = uv + half2(-1, 0) * _MainTex_TexelSize.xy;
                o.uv[4] = uv + half2(0, 0) * _MainTex_TexelSize.xy;
                o.uv[5] = uv + half2(1, 0) * _MainTex_TexelSize.xy;
                o.uv[6] = uv + half2(-1, 1) * _MainTex_TexelSize.xy;
                o.uv[7] = uv + half2(0, 1) * _MainTex_TexelSize.xy;
                o.uv[8] = uv + half2(1, 1) * _MainTex_TexelSize.xy;

                return o;
            }

            // 计算明度
            fixed luminance(fixed3 color){
                return 0.2125 * color.r + 0.7154 * color.g + 0.0721 * color.b;
            }

            half Sobel(v2f i){
                // Sobel算子
                const half Gx[9] = { -1, -2, -1, 
                                     0, 0, 0, 
                                     1, 2, 1 };
                const half Gy[9] = { -1, 0, 1, 
                                     -2, 0, 2,
                                     -1, 0, 1 };
                
                half texColor;
                half edgeX = 0;
                half edgeY = 0;

                // 依次对9个像素进行采样
                for(int it = 0; it < 9; it++){
                    // 计算亮度值，再与卷积核Gx和Gy中对应的权重相乘后，叠加到各自的梯度值上
                    texColor = luminance(tex2D(_MainTex, i.uv[it]).rgb);
                    edgeX += Gx[it] * texColor;
                    edgeY += Gy[it] * texColor;
                }

                // 用1减去两个方向的梯度值的绝对值，得到edge，值越小，表明越可能是边缘点
                half edge = 1 - abs(edgeX) - abs(edgeY);
                return edge;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                // 使用 Sobel 函数计算当前像素的梯度值
                half edge = Sobel(i);

                // 根据梯度值分别计算背景为原图和纯色下的颜色值
                fixed4 withEdgeColor = lerp(_EdgeColor, tex2D(_MainTex, i.uv[4]), edge);
                fixed4 onlyEdgeColor = lerp(_EdgeColor, _BackgroundColor, edge);

                return lerp(withEdgeColor, onlyEdgeColor, _EdgeOnly);
            }
            ENDCG
        }
    }
    // 关闭 Fallback
    FallBack Off
}
```

效果：
<center>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.3.jpg" alt="图12.3 左图：12.2节得到的结果。右图：进行边缘检测得到的结果"  width=360/>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.6.jpg" alt="图12.6 只显示边缘的屏幕效果"  width=240 />
</center>


## 12.4 高斯模糊

模糊的实现有很多种方法，例如：
* 均值模糊：卷积核中的各个元素值都相等，且相加等于1，也就是其邻域内各个像素值的平均值。
* 中值模糊：邻域内对所有像素排序后的中值替换掉原颜色。

一个更高级的模糊方法是高斯模糊。效果：

![图12.7 左图：原效果，右图：高斯模糊效果](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.7.jpg)

### 12.4.1 高斯滤波

高斯模糊同样利用了卷积计算，它使用的卷积核名为高斯核。高斯核是一个正方形大小的滤波核，其中每个元素都是基于下面的高斯方程：

$$
G(x,y) = \frac{1}{2\pi\sigma^2}e^{-\frac{(x^2+y^2)}{2\sigma^2}}
$$

* $\sigma$ ：标准方差（一般为1），它控制着高斯核的模糊程度。
* $x$ 和 $y$ ：当前位置到卷积核中心的整数距离。

要构建一个高斯核，我们只需要计算高斯核中各个位置对应的高斯值。为了保证滤波后的图像不会变暗，我们需要对高斯核中的权重进行归一化，即让每个权重除以所有权重的和，这样可以保证所有权重的和为1。因此，高斯函数中 $e$ 前面的系数实际不会对结果有任何影响。

高斯方程很好地模拟了邻域每个像素对当前处理像素的影响程度——距离越近，影响越大。高斯核的维数越高，模糊程度越大。

使用一个 N x N 的高斯核对图像进行卷积滤波，需要进行 N x N x W x H（W、H分别是图像的宽和高）次纹理采样，N 不断增大，采样次数会变得很大。
我们可以把这个二维高斯函数可以拆分成两个一维函数。使用两个一维的高斯核先后对图像进行滤波，得到的结果和直接使用二位高斯核是一样的，但采样次数只需要 2 x N x W x H。

![图12.8 一个5×5大小的高斯核。左图显示了标准方差为1的高斯核的权重分布。我们可以把这个二维高斯核拆分成两个一维的高斯核（右图）](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.8.jpg)


在本节,我们将会使用上述 5x5 的高斯核对原图像进行高斯模糊。我们将先后调用两个 Pass：
* 第一个 Pass 将会使用竖直方向的一维高斯核对图像进行滤波；
* 第二个 Pass 再使用水平方向的一维高斯核对图像进行滤波，得到最终的目标图像。

在实现中，我们还将利用图像缩放来进一步提高性能，并通过调整高斯滤波的应用次数来控制模糊程度（次数越多，图像越模糊）。



### 12.4.2 实现

新建脚本 GaussianBlur.cs，并继承 12.1 节中的基类：

``` csharp
using UnityEngine;

public class GaussianBlur : PostEffectsBase
{
    public Shader gaussianBlurShader;

    private Material gaussianBlurMaterial;
    public Material material{
        get{
            gaussianBlurMaterial = CheckShaderAndCreateMaterial(gaussianBlurShader, gaussianBlurMaterial);
            return gaussianBlurMaterial;
        }
    }

    // 提供调整高斯模糊迭代次数、模糊范围、和缩放系数的参数
    [Range(0, 4)]
    public int iterations = 3;

    [Range(0.2f, 3.0f)]
    public float blurSpread = 0.6f;

    // 降采样，值越大，性能越好，但过大可能会造成像素化
    [Range(1, 8)]
    public int downsample = 2;

    // // 版本1
    // public void OnRenderImage(RenderTexture src, RenderTexture dest)
    // {
    //     if (material != null)
    //     {
    //         int rtW = src.width;
    //         int rtH = src.height;

    //         // 分配一块缓冲区，因为高斯模糊需要调用两个 Pass，
    //         // 我们需要使用一块中间缓存存储第一个Pass执行完毕后得到的模糊结果
    //         RenderTexture buffer = RenderTexture.GetTemporary(rtW, rtH, 0);

    //         // 使用Shader中的第一个Pass对src进行处理，并将结果存储在buffer中
    //         Graphics.Blit(src, buffer, material, 0);
    //         // 使用Shader中的第二个Pass对buffer进行处理，返回最终的屏幕图像
    //         Graphics.Blit(buffer, dest, material, 1);

    //         // 释放中间缓存
    //         RenderTexture.ReleaseTemporary(buffer);
    //     }
    //     else
    //     {
    //         Graphics.Blit(src, dest);
    //     }
    // }

    // // 版本2 利用缩放对图像进行降采样，从而减少需要处理的像素数量，提高性能
    // public void OnRenderImage(RenderTexture src, RenderTexture dest)
    // {
    //     if (material != null)
    //     {
    //         // 利用缩放对图像进行降采样，从而减少需要处理的像素数量，提高性能
    //         int rtW = src.width / downsample;
    //         int rtH = src.height / downsample;

    //         // 分配一块缓冲区，因为高斯模糊需要调用两个 Pass，
    //         // 我们需要使用一块中间缓存存储第一个Pass执行完毕后得到的模糊结果
    //         RenderTexture buffer = RenderTexture.GetTemporary(rtW, rtH, 0);
    //         // 双线性过滤，防止降采样后的纹理像素看上去不连贯，丢失严重
    //         buffer.filterMode = FilterMode.Bilinear;

    //         // 使用Shader中的第一个Pass对src进行处理，并将结果存储在buffer中
    //         Graphics.Blit(src, buffer, material, 0);
    //         // 使用Shader中的第二个Pass对buffer进行处理，返回最终的屏幕图像
    //         Graphics.Blit(buffer, dest, material, 1);

    //         // 释放中间缓存
    //         RenderTexture.ReleaseTemporary(buffer);
    //     }
    //     else
    //     {
    //         Graphics.Blit(src, dest);
    //     }
    // }

    // 版本3 考虑了高斯模糊的迭代次数
    public void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (material != null)
        {
            // 利用缩放对图像进行降采样，从而减少需要处理的像素数量，提高性能
            int rtW = src.width / downsample;
            int rtH = src.height / downsample;

            // 分配一块缓冲区，因为高斯模糊需要调用两个 Pass，
            // 我们需要使用一块中间缓存存储第一个Pass执行完毕后得到的模糊结果
            RenderTexture buffer = RenderTexture.GetTemporary(rtW, rtH, 0);
            // 双线性过滤，防止降采样后的纹理像素看上去不连贯，丢失严重
            buffer.filterMode = FilterMode.Bilinear;

            Graphics.Blit(src, buffer);

            for (int i = 0; i < iterations; i++)
            {
                material.SetFloat("_BlurSize", 1.0f + i * blurSpread);

                RenderTexture buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0);

                // 第一个Pass，纵向模糊
                Graphics.Blit(buffer, buffer1, material, 0);

                // 释放buffer，将结果值buffer1存储到buffer中，重新分配buffer1
                RenderTexture.ReleaseTemporary(buffer);
                buffer = buffer1;
                buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0);

                // 第二个Pass，横向模糊
                Graphics.Blit(buffer, buffer1, material, 1);

                RenderTexture.ReleaseTemporary(buffer);
                buffer = buffer1;
            }

            // 最后一步，将模糊后的图像输出到屏幕上
            Graphics.Blit(buffer, dest);
            // 释放中间缓存
            RenderTexture.ReleaseTemporary(buffer);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }
}

```

新建 Shader 命名为 Chapter12-GaussianBlur：

``` hlsl
Shader "Unity Shaders Book/Chapter 12/Gaussian Blur"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _BlurSize ("Blur Size", Float) = 1
    }
    SubShader
    {
        // 定义可复用的代码块，可在不同Pass中调用
        CGINCLUDE

        #include "UnityCG.cginc"

        sampler2D _MainTex;
        // 主纹理的纹素大小（例如：一张512 * 512的纹理，纹素大小为1/512）
        // 利用纹素，做相邻区域内纹理采样时，计算各相邻区域的纹理坐标
        half4 _MainTex_TexelSize;
        float _BlurSize;

        struct v2f
        {
            float4 vertex : SV_POSITION;
            half2 uv[5] : TEXCOORD0;
        };

        v2f vertBlurVertical (appdata_img v)
        {
            v2f o;
            o.vertex = UnityObjectToClipPos(v.vertex);

            // 把计算采样纹理坐标的代码从片元着色器转移到顶点着色器中，可以减少运算，提高性能
            // 构建一个纵向高斯核
            half2 uv = v.texcoord;
            o.uv[0] = uv;
            // _BlurSize控制邻域像素之间的采样距离，_BlurSize值越大，模糊程度越高，但采样数不会受到影响
            // 但过大的_BlurSize值会造成虚影
            o.uv[1] = uv + float2(0.0, _MainTex_TexelSize.y * 1.0) * _BlurSize;
            o.uv[2] = uv - float2(0.0, _MainTex_TexelSize.y * 1.0) * _BlurSize;
            o.uv[3] = uv + float2(0.0, _MainTex_TexelSize.y * 2.0) * _BlurSize;
            o.uv[4] = uv - float2(0.0, _MainTex_TexelSize.y * 2.0) * _BlurSize;

            return o;
        }

        v2f vertBlurHorizontal (appdata_img v)
        {
            v2f o;
            o.vertex = UnityObjectToClipPos(v.vertex);

            // 把计算采样纹理坐标的代码从片元着色器转移到顶点着色器中，可以减少运算，提高性能
            // 构建一个横向高斯核
            half2 uv = v.texcoord;
            o.uv[0] = uv;
            // _BlurSize控制邻域像素之间的采样距离，_BlurSize值越大，模糊程度越高，但采样数不会受到影响
            // 但过大的_BlurSize值会造成虚影
            o.uv[1] = uv + float2(_MainTex_TexelSize.x * 1.0, 0.0) * _BlurSize;
            o.uv[2] = uv - float2(_MainTex_TexelSize.x * 1.0, 0.0) * _BlurSize;
            o.uv[3] = uv + float2(_MainTex_TexelSize.x * 2.0, 0.0) * _BlurSize;
            o.uv[4] = uv - float2(_MainTex_TexelSize.x * 2.0, 0.0) * _BlurSize;

            return o;
        }

        fixed4 fragBlur (v2f i) : SV_Target
        {
            // 由于对称性，只需记录3个高斯权重
            float weight[3] = {0.4026, 0.2442, 0.0545};

            fixed3 sum = tex2D(_MainTex, i.uv[0]).rgb * weight[0];

            for (int it = 1; it < 3; it++){
                sum += tex2D(_MainTex, i.uv[it*2-1]).rgb * weight[it];
                sum += tex2D(_MainTex, i.uv[it*2]).rgb * weight[it];
            }

            return fixed4(sum, 1.0);
        }

        ENDCG

        // 屏幕后处理 Shader 的“标配”
        ZTest Always  Cull Off  ZWrite Off

        Pass
        {
            NAME "GAUSSIAN_BLUR_VERTICAL"

            CGPROGRAM
            #pragma vertex vertBlurVertical
            #pragma fragment fragBlur
            
            ENDCG
        }
        Pass
        {
            NAME "GAUSSIAN_BLUR_HORIZONTAL"

            CGPROGRAM
            #pragma vertex vertBlurHorizontal
            #pragma fragment fragBlur
            
            ENDCG
        }
    }
    // 关闭 Fallback
    FallBack Off
}
```


## 12.5 Bloom 效果

Bloom 效果是让画面中较亮的区域“扩散”到周围的区域中，造成一种朦胧的效果。

![图12.10 左边为原图，右边为 Bloom 效果](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.10.jpg)

Bloom 的实现非常简单：
1. 首先根据一个阈值提取出图像中的较亮区域，把它们存储在一张渲染纹理中；
2. 利用高斯模糊对这张渲染纹理进行模糊处理，模拟光线扩散的效果；
3. 最后将其和原图像进行混合，得到最终的效果。

新建脚本 Bloom.cs：

``` csharp
using UnityEngine;

public class Bloom : PostEffectsBase
{
    public Shader bloomShader;
    private Material bloomMaterial;
    public Material material
    {
        get
        {
            bloomMaterial = CheckShaderAndCreateMaterial(bloomShader, bloomMaterial);
            return bloomMaterial;
        }
    }

    // 由于 Bloom 效果是建立在高斯模糊的基础上，因此参数几乎与高斯模糊相同。

    // 提供调整高斯模糊迭代次数、模糊范围、和缩放系数的参数
    [Range(0, 4)]
    public int iterations = 3;

    [Range(0.2f, 3.0f)]
    public float blurSpread = 0.6f;

    [Range(1, 8)]
    public int downsample = 2;

    // 控制提取较亮区域时使用的阈值
    [Range(0.0f, 4.0f)]
    public float luminanceThreshold = 0.6f;


    public void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (material != null)
        {
            material.SetFloat("_LuminanceThreshold", luminanceThreshold);
            // 利用缩放对图像进行降采样，从而减少需要处理的像素数量，提高性能
            int rtW = src.width / downsample;
            int rtH = src.height / downsample;

            // 分配一块缓冲区，因为高斯模糊需要调用两个 Pass，
            // 我们需要使用一块中间缓存存储第一个Pass执行完毕后得到的模糊结果
            RenderTexture buffer = RenderTexture.GetTemporary(rtW, rtH, 0);
            // 双线性过滤，防止降采样后的纹理像素看上去不连贯，丢失严重
            buffer.filterMode = FilterMode.Bilinear;

            // 第一个Pass，提取图像中较亮区域
            Graphics.Blit(src, buffer, material, 0);

            for (int i = 0; i < iterations; i++)
            {
                material.SetFloat("_BlurSize", 1.0f + i * blurSpread);

                RenderTexture buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0);

                Graphics.Blit(buffer, buffer1, material, 1);

                RenderTexture.ReleaseTemporary(buffer);
                buffer = buffer1;
                buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0);

                Graphics.Blit(buffer, buffer1, material, 2);

                RenderTexture.ReleaseTemporary(buffer);
                buffer = buffer1;
            }

            // 将 buffer 传递给材质
            material.SetTexture("_Bloom", buffer);
            // 使用 Shader 中的第四个 Pass 来进行最后的混合，将结果存储在目标渲染纹理中
            Graphics.Blit(src, dest, material, 3);

            // 释放临时缓存
            RenderTexture.ReleaseTemporary(buffer);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }
}

```

新建 Shader 命名为 Chapter12-Bloom：

``` hlsl
Shader "Unity Shaders Book/Chapter 12/Bloom"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _Bloom ("Bloom (RGB)", 2D) = "black" {}
        _LuminanceThreshold ("Luminance Threshold", Float) = 0.5
        _BlurSize ("Blur Size", Float) = 1
    }
    SubShader
    {
        // 定义可复用的代码块，可在不同Pass中调用
        CGINCLUDE

        #include "UnityCG.cginc"

        sampler2D _MainTex;
        // 主纹理的纹素大小（例如：一张512 * 512的纹理，纹素大小为1/512）
        // 利用纹素，做相邻区域内纹理采样时，计算各相邻区域的纹理坐标
        half4 _MainTex_TexelSize;
        sampler2D _Bloom;
        float _LuminanceThreshold;
        float _BlurSize;

        struct v2f
        {
            float4 vertex : SV_POSITION;
            half2 uv : TEXCOORD0;
        };

        // 定义提取较亮区域需要用的顶点片元着色器
        v2f vertExtractBright (appdata_img v)
        {
            v2f o;
            o.vertex = UnityObjectToClipPos(v.vertex);
            o.uv = v.texcoord;

            return o;
        }
        
        fixed luminance(fixed3 color)
        {
            return 0.2125 * color.r + 0.7154 * color.g + 0.0721 * color.b;
        }

        fixed4 fragExtractBright (v2f i) : SV_Target
        {
            fixed4 color = tex2D(_MainTex, i.uv);
            // 将采样得到的亮度值减去阈值，并把结果截取到0-1之间
            fixed val = clamp(luminance(color.rgb) - _LuminanceThreshold, 0.0, 1.0);

            // 把该值和原像素相乘，得到提取后的亮部区域
            return color * val;
        }


        // 定义混合亮部图像和原图像的顶点片元着色器
        struct v2fBloom
        {
            float4 vertex : SV_POSITION;
            half4 uv : TEXCOORD0;
        };

        v2fBloom vertBloom (appdata_img v)
        {
            v2fBloom o;
            o.vertex = UnityObjectToClipPos(v.vertex);
            o.uv.xy = v.texcoord;
            o.uv.zw = v.texcoord;

            #if UNITY_UV_STARTS_AT_TOP
            if (_MainTex_TexelSize.y < 0.0)
                o.uv.w = 1.0 - o.uv.w;
            #endif

            return o;
        }

        fixed4 fragBloom (v2fBloom i) : SV_Target
        {
            return tex2D(_MainTex, i.uv.xy) + tex2D(_Bloom, i.uv.zw);
        }


        ENDCG

        // 屏幕后处理 Shader 的“标配”
        ZTest Always  Cull Off  ZWrite Off

        Pass
        {
            CGPROGRAM
            #pragma vertex vertExtractBright
            #pragma fragment fragExtractBright
            
            ENDCG
        }

        // 通过 UsePass 语义指明上一节中高斯模糊定义的两个 Pass
        UsePass "Unity Shaders Book/Chapter 12/Gaussian Blur/GAUSSIAN_BLUR_VERTICAL"
        UsePass "Unity Shaders Book/Chapter 12/Gaussian Blur/GAUSSIAN_BLUR_HORIZONTAL"
        
        Pass
        {
            CGPROGRAM
            #pragma vertex vertBloom
            #pragma fragment fragBloom
            
            ENDCG
        }
    }
    // 关闭 Fallback
    FallBack Off
}
```


## 12.6 运动模糊

运动模糊是真实世界中相机的一种效果，如果在摄像机曝光时，拍摄场景发生了变化，就会产生模糊的画面。在计算机图像中，由于不存在曝光现象，渲染出来的图像往往都是棱角分明，缺少运动模糊。

运动模糊实现有多种方法：
* **累计缓存（Accumulation Buffer）** ：当物体快速移动产生多张图像后，取它们之间的平均值作为最后的运动模糊图像。然而，这种方法性能消耗很大，因为想要获取多张帧图像往往意味着我们需要在同一帧里渲染多次场景。

* **速度缓存（Velocity Buffer）** ：这个缓存中存储了各个像素当前的运动速度，然后利用该值来决定模糊的方向和大小。


我们将使用类似第一种方法，不需要在一帧中把场景渲染多次，但需要保存之前的渲染结果，不断把当前的渲染图像叠加到之前的渲染图像中，从而产生一种运动轨迹的视觉效果。这种方法性能比累计缓存好，但模糊效果可能会略有影响。

新建脚本 MotionBlur.cs：

``` csharp
using UnityEngine;

public class MotionBlur : PostEffectsBase
{
    public Shader motionBlurShader;
    private Material motionBlurMaterial;
    public Material material
    {
        get
        {
            motionBlurMaterial = CheckShaderAndCreateMaterial(motionBlurShader, motionBlurMaterial);
            return motionBlurMaterial;
        }
    }

    // 定义运动模糊在混合图像时使用的模糊参数
    [Range(0.0f, 0.9f)]
    public float blurAmount = 0.5f;

    // 定义一个 RenderTexture 用于存储之前图像叠加的结果
    private RenderTexture accumulationTexture;

    // 当脚本不运行时，立即销毁 accumulationTexture
    // 因为我们希望下一次开始应用运动模糊时重新叠加图像
    void OnDisable()
    {
        DestroyImmediate(accumulationTexture);
    }

    private void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (material != null)
        {
            // 如果 accumulationTexture 为空，或者大小与 src 不匹配，则创建一个适合当前分辨率的 accumulationTexture
            if (accumulationTexture == null || accumulationTexture.width != src.width || accumulationTexture.height != src.height)
            {
                DestroyImmediate(accumulationTexture);
                accumulationTexture = new RenderTexture(src.width, src.height, 0);
                accumulationTexture.hideFlags = HideFlags.HideAndDontSave;
                Graphics.Blit(src, accumulationTexture);
            }

            // MarkRestoreExpected 函数表明我们需要进行一个渲染纹理的恢复操作
            // 恢复操作：发生在渲染到纹理而该纹理又没有提前清空或销毁的情况
            accumulationTexture.MarkRestoreExpected();

            material.SetFloat("_BlurAmount", 1.0f - blurAmount);

            // 把当前屏幕图像叠加到 accumulationTexture 中
            Graphics.Blit(src, accumulationTexture, material);
            Graphics.Blit(accumulationTexture, dest);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }

}
```

新建 Shader 命名为 Chapter12-MotionBlur：

``` hlsl
Shader "Unity Shaders Book/Chapter 12/Motion Blur"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _BlurAmount ("Blur Amount", Float) = 1.0
    }
    SubShader
    {
        CGINCLUDE

        #include "UnityCG.cginc"

        sampler2D _MainTex;
        fixed _BlurAmount;

        struct v2f
        {
            float2 uv : TEXCOORD0;
            float4 vertex : SV_POSITION;
        };

        v2f vert (appdata_img v)
        {
            v2f o;
            o.vertex = UnityObjectToClipPos(v.vertex);
            o.uv = v.texcoord;
            return o;
        }

        // 对当前图像进行采样，将其A通道的值设为_BlurAmount，方便后面混合时控制透明通道
        fixed4 fragRGB (v2f i) : SV_Target
        {
            return fixed4(tex2D(_MainTex, i.uv).rgb, _BlurAmount);
        }

        // 直接返回采样结果
        half4 fragA (v2f i) : SV_Target
        {
            return tex2D(_MainTex, i.uv);
        }

        ENDCG

        // 后处理“标配”
        ZTest Always Cull Off ZWrite Off

        // 第一个Pass，更新渲染纹理的RGB通道，
        // 通过_BlurAmount设置A通道来混合图像，但不会把A通道写入渲染纹理
        Pass
        {
            Blend SrcAlpha OneMinusSrcAlpha
            ColorMask RGB

            CGPROGRAM

            #pragma vertex vert
            #pragma fragment fragRGB

            ENDCG
        }

        // 用当前图像的透明通道作为混合后的图像的透明通道
        Pass
        {
            Blend One Zero
            ColorMask A
            
            CGPROGRAM

            #pragma vertex vert
            #pragma fragment fragA

            ENDCG
        }
    }
    FallBack Off
}
```



----------------------------------------------------------------



# 第13章 使用深度和法线纹理

很多时候我们不仅需要当前屏幕的颜色信息，还希望得到深度和法线信息。例如，在进行边缘检测时，直接利用颜色信息会使检测到的边缘信息受物体纹理和光照等外部因素的影响，得到很多我们不需要的边缘点。一种更好的方法是，我们可以在深度纹理和法线纹理上进行边缘检测，这些图像不会受纹理和光照影响，而仅仅保存了当前渲染物体的模型信息，通过这样的方式检测出来的边缘更加可靠。


## 13.1 获取深度和法线纹理

### 13.1.1 背后的原理

深度纹理实际是一张渲染纹理，里面存储的是高精度的深度值，数值的范围是[0, 1]，且是非线性分布的。

这些深度值来自于顶点变换后得到的归一化的设备坐标（Normalized Device Coordinates，NDC），NDC 每个坐标的范围是[-1, 1]。

为什么是非线性分布的？
顶点从模型空间变换到其次裁剪坐标系下，是通过在顶点着色器中乘以 MVP 矩阵得到的，在变换的最后一步，如果使用了透视投影类型的相机，这个投影矩阵就是非线性的。

Unity 中透视投影对顶点的变换过程：

![图13.1 在透视投影中，投影矩阵首先对顶点进行了缩放。在经过齐次除法后，透视投影的裁剪空间会变换到一个立方体。图中标注了4个关键点经过投影矩阵变换后的结果](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.1.jpg)

> 注意：投影过程建立在Unity对坐标系的假定上（观察空间为右手坐标系），使用列矩阵在矩阵右侧进行相乘，且得到NDC后z分量范围在[-1, 1]之间；在类似DirectX这样的图形接口中，变换后z分量在[0, 1]之间。

使用正交摄像机时投影变换的过程：

![图13.2 在正交投影中，投影矩阵对顶点进行了缩放。在经过齐次除法后，正交投影的裁剪空间会变换到一个立方体。图中标注了4个关键点经过投影矩阵变换后的结果](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.2.jpg)


由于 NDC 中 z 分量的范围在 [-1, 1]，为了让这些值能够存储在一张图像中，我们需要将其映射到 [0, 1] 之间： 
$d = 0.5 * z_{ndc} + 0.5$，其中 d 对应深度纹理中的像素值， $z_{ndc}$ 是 NDC 中的 z 分量。

Unity 是怎么得到这样一张深度纹理的呢？
* 在 Unity 中，深度纹理可以直接来自于真正的深度缓存，也可以是由一个单独的Pass渲染而得，这取决于使用的渲染路径和硬件。
* 通常来讲当使用延迟渲染路径(包括遗留的延迟渲染路径)时，深度纹理理所当然可以访问到，因为延迟渲染会把这些信息渲染到 G-bufer 中。
* 当无法直接获取深度缓存时，深度和法线纹理是通过一个单独的 Pass 渲染而得的。具体实现是：
    * Unity会使用着色器替换（Shader Replacement）技术选择那些渲染类型（RenderType）为 Opaque 的物体
    * 判断它们使用的渲染队列是否小于等于2500（内置的Background、Geometry和AlphaTest 渲染队列均在此范围内）
    * 如果满足条件，就把它渲染到深度和法线纹理中。
    * 因此，要想让物体能够出现在深度和法线纹理中，就必须在 Shader 中设置正确的 RenderType 标签

在Unity中，我们可以选择让一个摄像机生成一张深度纹理或是一张深度+法线纹理。
* 只需要一张单独的深度纹理时
    * Unity会直接获取深度缓存或是按之前讲到的着色器替换技术，选取需要的不透明物体，并使用它投射阴影时使用的Pass来得到深度纹理。
    * 如果 Shader 中不包含这样一个 Pass，那么这个物体就不会出现在深度纹理中。
    * 深度纹理的精度通常是24位或16位，这取决于使用的深度缓存的精度。
* 如果选择生成一张深度+法线纹理时
    * Unity会创建一张和屏幕分辨率相同、精度为32位（每个通道为8位）的纹理，其中观察空间下的法线信息会被编码进纹理的 R 和 G 通道，而深度信息会被编码进 B 和 A 通道。
    * 法线信息的获取在延迟渲染中是可以非常容易就得到的，Unity 只需要合并深度和法线缓存即可。
    * 而在前向渲染中，默认情况下是不会创建法线缓存的，因此 Unity 底层使用了一个单独的 Pass 把整个场景再次渲染一遍来完成。这个 Pass 在 buildin_shaders-xxx/DefaultResources/Camera-DepthNormalTexture.shader 文件中可以找到。



### 13.1.2 如何获取

在脚本中设置摄像机的 depthTextureMode，然后再在 Shader 中直接访问特定的纹理属性即可。

``` csharp
// 获取深度纹理
camera.depthTextureMode = DepthTextureMode.Depth;
// 获取深度+法线纹理
camera.depthTextureMode = DepthTextureMode.DepthNormals;
// 同时产生一张深度和深度+法线纹理
camera.depthTextureMode |= DepthTextureMode.Depth;
camera.depthTextureMode |= DepthTextureMode.DepthNormals;
```

在 Shader 中，我们可以通过 _CameraDepthTexture 访问深度纹理，或者通过 _CameraDepthNormalsTexture 访问深度+法线纹理。

当在 Shader 中访问到深度纹理后，我们就可以使用当前像素的纹理坐标对它进行采样。绝大多数情况可以直接通过 tex2D() 函数来采样，但在某些平台（如 PS3 和 PS2）上，需要进行一些特殊处理。Unity 提供了统一的宏 SAMPLE_DEPTH_TEXTURE 用来处理平台差异造成的问题：
```
// i.uv 是一个 float2 类型的变量，对应了当前像素的纹理坐标
float d = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv);
```

类似的宏还有 SAMPLE_DEPTH_TEXTURE_PROJ、SAMPLE_DEPTH_TEXTURE_LOD，可以在 Unity 内置的 HLSLSupport.cginc 文件中找到。

SAMPLE_DEPTH_TEXTURE_PROJ 接受两个参数：深度纹理和一个float3或float4类型的纹理坐标，它的内部使用了 tex2Dproj 这样的函数进行投影纹理采样，纹理坐标的前两个分量会除以最后一个分量，再进行纹理采样。如果提供了第四个分量，还会进行一次比较，通常用于阴影的实现中。 SAMPLE_DEPTH_TEXTURE_PROJ 的第二个参数通常有顶点着色器输出差值得到的屏幕坐标：

```
// i.scr 是顶点着色器中通过调用ComputeScreenPos(o.pos)得到的屏幕坐标
float d = SAMPLE_DEPTH_TEXTURE_PROJ(_CameraDepthTexture, UNITY_PROJ_COORD(i.scrPos));
```

通过纹理采样得到的深度值往往是非线性的，在我们的计算过程中通常是需要线性的深度值，也就是说，我们需要把投影后的深度值变换到线性空间下，例如视角空间下的深度值。我们可以通过倒推顶点变换的过程来进行这个转换。

下面我们以透视投影为例，推导如何由深度纹理中的深度信息计算得到视角空间下的深度值。

由4.6.7节可知，当使用透视投影的矩阵 $P_{clip}$ 对视角空间下 一个顶点进行变换后，裁剪空间下顶点的 z 和 w 分量为：

$$
z_{clip} = z_{view} \frac{Far + Near}{Far - Near} - \frac{2 * Near * Far}{Far - Near}
$$

$$
w_{clip} = -z_{view} 
$$

其中，Far 和 Near 分别是远近裁剪平面的距离。
然后，通过齐次除法可以得到 NDC 下的 z 分量：

$$
z_{ndc} = \frac{z_{clip}}{w_{clip}} 
= \frac{Far + Near}{Far - Near} + \frac{2 * Near * Far}{(Far - Near) * z_{view}}
$$

在 13.1.1 节中我们知道，深度纹理中的深度值是通过下面的公式由 NDC 计算而得的：

$$
d = 0.5 * z_{ndc} + 0.5
$$

由上面这些式子，我们可以推导出用 d 表示而得的 $z_{view}$ 的表达式：

$$
z_{view} = \frac{1}{\frac{Far - Near}{Near * Far}d - \frac{1}{Near}}
$$

由于在 Unity 中使用的视角空间中，摄像机正向对应的 z 值均为负值，因此为了得到深度值的正数表示，我们需要对上面的结果取反：

$$
z'_{view} = \frac{1}{\frac{Far - Near}{Near * Far}d + \frac{1}{Near}}
$$

它的取值范围就是视锥体深度范围 [Near, Far]。如果我们想得到范围在 [0, 1] 之间的深度值，只需要把上面的结果除以 Far 即可。这样，0 表示该点与摄像机位于同一位置，1 表示该点位于视锥体的远裁剪平面上。

$$
z_{01} = \frac{1}{\frac{Far - Near}{Near}d + \frac{Far}{Near}}
$$

Unity 提供了两个辅助函数为我们进行上述计算过程 —— LinearEyeDepth 和 Linear01Depth。
* LinearEyeDepth：把深度纹理的采样结果转换到视角空间下的深度值，也就是 $z'_{view}$ 。
* Linear01Depth：返回一个范围在 [0, 1] 之间的线性深度值，也就是 $z_{01}$ 。

这两个函数内部使用了内置的 _ZBufferParams 变量来得到远近裁剪平面的距离。

如果需要获取深度+法线纹理，可直接使用 tex2D 函数对 _CameraDepthNormalsTexture 进行采样，得到里面存储的深度和法线信息。
Unity 提供了辅助函数 DecodeDepthNormal 来对这个采样结果进行解码，从而得到深度值和法线方向，其在UnityCG.cginc里的定义：

```
// enc: 对深度+法线纹理的采样结果，是Unity对其编码后的结果
// xy分量存储视角空间下的法线信息，zw分量是深度信息
// depth：解码后的深度值，范围在[0, 1]，为线性深度值
// normal：解码后的视角空间下的法线方向
inline void DecodeDepthNormal(float4 enc, out float depth, out float3 normal)
{
    depth = DecodeFloatRG(enc.zw);
    normal = DecodeViewNormalStereo(enc);
}
```


### 13.1.3 查看深度和法线纹理

使用 **帧调试器（Frame Debugger）** 可以查看深度和法线纹理。

![图13.3 使用 Frame Debugger 查看深度纹理（左）和深度 + 法线纹理（右）](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.3.jpg)

使用帧调试器查看到的深度纹理是非线性空间的深度值，而深度+法线纹理都是由 Unity 编码后的结果。有时，显示出线性空间下的深度信息或解码后的法线方向会更加有用。此时，我们可以自行在片元着色器中输出转换或解码后的深度和法线值：

```
// 输出线性深度值
float depth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv);
float linearDepth = Linear01Depth(depth);
return fixed4(linearDepth, linearDepth, linearDepth, 1.0);

// 输出法线方向
fixed3 normal = DecodeViewNormalStereo(tex2D(_CameraDepthNormalsTexture, i.uv).xy);
return fixed4(normal * 0.5 + 0.5, 1.0);
```

查看深度纹理时，如果画面几乎是全黑或全白时，可以将相机的远裁剪平面的距离（Unity默认为1000）调小，使之刚好覆盖场景的所在区域即可。因为若裁剪平面的距离过大，会导致距离相机较近的物体会被映射到非常小的深度值，导致看起来全黑（场景为封闭区域比较常见）；相反若场景为开放区域，物体距离相机较远，则会导致画面几乎全白。

![图13.4 左图：线性空间下的深度纹理。右图：解码后并且被映射到[0, 1]范围内的视角空间下的法线纹理](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.4.jpg)



## 13.2 再谈运动模糊

在12.6节中，我们通过混合多张屏幕图像来模拟运动模糊的效果。但是，另一种应用更加广泛的技术则是使用速度映射图。

速度映射图中存储了每个像素的速度，然后使用这个速度来决定模糊的方向和大小。速度缓冲的生成有多种方法：
* 一种方法是把场景中所有物体的速度渲染到一张纹理中。
   * 缺点：需要修改场景中所有物体的 Shader 代码，使其添加计算速度的代码并输出到一个渲染纹理中。
* [《GPU Gems3》第27章](https://developer.nvidia.com/gpugems/gpugems3/part-iv-image-effects/chapter-27-motion-blur-post-processing-effect) 中介绍了一种生成速度映射图的方法：
   1. 利用深度纹理在片元着色器中为每个像素计算其在世界空间下的位置，这是通过使用当前的视角\*投影矩阵的逆矩阵对 NDC 下的顶点坐标进行变换得到的。
   2. 当得到世界空间中的顶点坐标后，我们使用前一帧的视角\*投影矩阵对其进行变换，得到该位置在前一帧中的 NDC 坐标。
   3. 然后，我们计算前一帧和当前帧的位置差，生成该像素的速度。
   * 优点：可以在一个屏幕后处理步骤中完成整个效果的模拟；
   * 缺点：需要在片元着色器中进行两次矩阵乘法的操作，对性能有所影响。


新建脚本 MotionBlurWithDepthTexture.cs，继承 12.1 节中的基类：

``` csharp
using UnityEngine;

public class MotionBlurWithDepthTexture : PostEffectsBase
{
    public Shader motionBlurShader;
    private Material motionBlurMaterial;
    public Material material
    {
        get{
            motionBlurMaterial = CheckShaderAndCreateMaterial(motionBlurShader, motionBlurMaterial);
            return motionBlurMaterial;
        }
    }

    // 定义运动模糊时模糊图像使用的大小
    [Range(0.0f, 1.0f)]
    public float blurSize = 0.5f;

    // 需要摄像机的视角和投影矩阵
    private Camera myCamera;
    public Camera camera{
        get{
            if(myCamera == null)
                myCamera = GetComponent<Camera>();
            return myCamera;
        }
    }

    // 定义一个变量来保存上一帧摄像机的视角*投影矩阵
    private Matrix4x4 previousViewProjectionMatrix;

    void OnEnable(){
        // 需要获取相机的深度纹理
        camera.depthTextureMode |= DepthTextureMode.Depth;
    }

    void OnRenderImage(RenderTexture src, RenderTexture dest){
        if(material != null){
            material.SetFloat("_BlurSize", blurSize);

            // 设置上一帧的视角*投影矩阵
            material.SetMatrix("_PreviousViewProjectionMatrix", previousViewProjectionMatrix);
            // 设置当前帧的视角*投影矩阵的逆矩阵
            Matrix4x4 currentViewProjectionMatrix = camera.projectionMatrix * camera.worldToCameraMatrix;
            material.SetMatrix("_CurrentViewProjectionInverseMatrix", currentViewProjectionMatrix.inverse);
            previousViewProjectionMatrix = currentViewProjectionMatrix;

            Graphics.Blit(src, dest, material);
        }
        else{
            Graphics.Blit(src, dest);
        }
    }

}

```

新建 Shader 命名为 Chapter13-MotionBlurWithDepthTexture：

``` hlsl
Shader "Unity Shaders Book/Chapter 13/Motion Blur With Depth Texture"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _BlurSize ("Blur Size", Float) = 1.0
    }
    SubShader
    {
        CGINCLUDE
        
        #include "UnityCG.cginc"

        sampler2D _MainTex;
        // 主纹理的纹素大小，用于对深度纹理的采样坐标进行平台差异化处理
        half4 _MainTex_TexelSize;
        // Unity 传递的深度纹理
        sampler2D _CameraDepthTexture;
        // 脚本传递而来的矩阵
        float4x4 _CurrentViewProjectionInverseMatrix;
        float4x4 _PreviousViewProjectionMatrix;
        half _BlurSize;

        struct v2f
        {
            float4 vertex : SV_POSITION;
            half2 uv : TEXCOORD0;
            half2 uv_depth : TEXCOORD1;
        };

        v2f vert (appdata_img v)
        {
            v2f o;
            o.vertex = UnityObjectToClipPos(v.vertex);
            o.uv = v.texcoord.xy;
            o.uv_depth = v.texcoord.xy;

            // 处理平台差异导致的图像翻转问题，以便在类似 DirectX 的平台上开启了抗锯齿仍可得到正确的结果
            #if UNITY_UV_STARTS_AT_TOP
                if (_MainTex_TexelSize.y < 0)
                    o.uv_depth.y = 1 - o.uv_depth.y;
            #endif

            return o;
        }
        
        fixed4 frag (v2f i) : SV_Target
        {
            // 使用内置宏SAMPLE_DEPTH_TEXTURE和纹理坐标对深度纹理进行采样，得到深度值
            float d = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv_depth);
            // 构建像素的NDC坐标H，需要把深度值d重新映射回NDC，用原映射的反函数 d * 2 - 1 即可
            float4 H = float4(i.uv.x * 2 - 1, i.uv.y * 2 - 1, d * 2 - 1, 1);
            // 通过视角*投影矩阵的逆矩阵，得到视角空间下的坐标
            float4 D = mul(_CurrentViewProjectionInverseMatrix, H);
            // 除以w分量，将其转为世界坐标
            float4 worldPos = D / D.w;

            // 当前帧视角空间下的坐标
            float4 currentPos = H;
            // 将世界坐标通过前一帧的视角*投影矩阵进行变换，得到前一帧在NDC下的坐标
            float4 previousPos = mul(_PreviousViewProjectionMatrix, worldPos);
            previousPos /= previousPos.w;

            // 计算当前帧和前一帧的屏幕空间下的位置差，得到该像素的速度
            float2 velocity = (currentPos.xy - previousPos.xy) / 2.0f;

            float2 uv = i.uv;
            float4 color = tex2D(_MainTex, uv);
            // 使用速度值对它的邻域像素进行采样，相加后取平均值得到模糊的效果。
            // _BlurSize 控制采样距离
            uv += velocity * _BlurSize;
            for(int it = 1; it < 3; it++, uv += velocity * _BlurSize)
            {
                float4 currentColor = tex2D(_MainTex, uv);
                color += currentColor;
            }
            
            color /= 3.0f;

            return color;
        }

        ENDCG

        Pass
        {
            // 屏幕后处理标配
            ZTest Always Cull Off ZWrite Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            ENDCG
        }
    }
    FallBack Off
}
```


## 13.3 全局雾效

**雾效（Fog）** 是游戏中常用的一种效果。

**Unity 内置的雾效** 可以产生基于距离的线性或指数雾效。
* 实现：在 Shader 中添加 `#pragma multi_compile_fog` 指令，并在顶点着色器输出结构体中声明 `UNITY_FOG_COORDS(x)`，在顶点着色器中使用 `UNITY_TRANSFER_FOG(o,o.vertex)` 指令来传输雾效坐标。在片元着色器中，使用 `UNITY_APPLY_FOG(i.fogCoord, color)` 指令来应用雾效。
* 缺点：不仅需要为场景中所有物体添加相关渲染代码，而且能够实现的效果非常有限。

**基于屏幕后处理的全局雾效** ：
基于屏幕后处理的全局雾效的关键是，根据深度纹理来重建每个像素在世界空间下的位置。
* 首先对图像空间下的视锥体射线（冲摄像机出发，指向图像上的某点的射线）进行插值，这条射线存储了该像素在世界空间下到摄像机的方向信息。
* 然后，我们把该射线和线性化后的视角空间下的深度值相乘，再加上摄像机的世界位置，就可以得到该像素在世界空间下的位置。
* 优点：自由度高。

### 13.3.1 重建世界坐标

我们只需要知道摄像机在世界空间下的位置，以及世界空间下的该像素相对于摄像机的偏移量，把它们相加就可以得到该像素的世界坐标：
``` hlsl
// _WorldSpaceCameraPos 是摄像机的世界坐标
// linearDepth * interpolatedRay 可以计算得到该像素相对于摄像机的偏移量
// lineardDepth 是由深度纹理得到的线性深度值
// interpolatedRay 是由顶点着色器输出并插值后得到的射线
float4 worldPos = _WorldSpaceCameraPos + lineardDepth * interpolatedRay;
```

interpolatedRay 求法：

interpolatedRay 来源于对近裁剪平面的4个角的某个特定向量的插值，这四个向量包含了它们到摄像机的方向和距离信息，我们可以利用摄像机的近裁剪平面距离、FOV、纵横比计算而得。

![图13.6 计算interpolatedRay](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.6.jpg)

为了方便计算，我们可以先计算 toTop 和 toRight 两个向量：

$$
halfHeight = Near * tan(\frac{FOV}{2})
$$

$$
toTop = camera.up * halfHeight
$$

$$
toRight = camera.right * halfHeight * aspect
$$

其中，Near 是近裁剪平面的距离，FOV 是竖直方向的视角范围，camera.up 和 camera.right 分别对应了摄像机的正上方和正右方。

得到这两个辅助向量后，我们就可以计算4个角相对于摄像机的方向了：

$$
TL = camera.forward * Near + toTop - toRight
$$

$$
TR = camera.forward * Near + toTop + toRight
$$

$$
BL = camera.forward * Near - toTop - toRight
$$

$$
BR = camera.forward * Near - toTop + toRight
$$

上面求得的4个向量不仅包含了方向信息，它们的模对应了4个点到摄像机的空间距离。由于我们得到的线性深度值并非是点到摄像机的欧式距离，而是在 z 方向上的距离，因此，我们不能直接使用深度值和4个角的单位方向的乘积来计算它们到摄像机的偏移量，如图13.7所示。

![图13.7 采样得到的深度值并非是点到摄像机的欧式距离](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.7.jpg)

想要把深度值转换成到摄像机的欧式距离也很简单，我们以 TL 点为例，根据相似三角形原理，TL 所在的射线上，像素的深度值和它到摄像机的实际距离的比等于近裁剪平面的距离和 TL 向量的模的比，即：

$$
\frac{depth}{dist} = \frac{Near}{|TL|}
$$

由此可得，我们需要的 TL 距离摄像机的欧式距离 dist：

$$
dist = \frac{|TL|}{Near} * depth
$$

由于4个点相互对称，因此其他3个向量的模和 TL 相等，即我们可以使用同一个因子和单位向量相乘，得到它们对应的向量值：

$$
scale = \frac{|TL|}{|Near|}
$$

$$
Ray_{TL} = \frac{TL}{|TL|} * scale, Ray_{TR} = \frac{TR}{|TR|} * scale, \\ 
Ray_{BL} = \frac{BL}{|BL|} * scale, Ray_{BR} = \frac{BR}{|BR|} * scale
$$

屏幕后处理的是使用特定的材质去渲染一个刚好填充整个屏幕的四边形面片。这个四边形面片的4个顶点就对应了近裁剪平面的4个角。因此，我们可以把上面的计算结果传递给顶点着色器，顶点着色器根据当前的位置选择它所对应的向量，然后再将其输出，经插值后传递给片元着色器得到 interpolatedRay，我们就可以直接利用本节一开始提到的公式重建该像素在世界空间下的位置了。


### 13.3.2 雾的计算

在简单的雾效实现中，我们需要计算一个雾效系数 f，作为混合原始颜色和雾的颜色的混合系数：
`float3 afterFog = f * fogColor + (1 - f) * origColor;`

这个雾效系数 f 有很多计算方法。在 Unity 内置的雾效实现中，支持三种雾的计算方式：
* 线性（Linear）： $f = \frac{d_{max} - \lvert z \rvert}{d_{max} - d_{min}}$ ，d_min 和 d_max 分别表示受雾影响的最小距离和最大距离。
* 指数（Exponential）： $f = e ^ {- d \cdot \lvert z \rvert}$ ，d 是控制雾的浓度的参数。
* 指数的平方（Exponential Squared）： $f = e ^ {- (d \cdot \lvert z \rvert) ^ 2}$ ，d 是控制雾的浓度的参数。

本节将使用类似线性雾的计算方式，计算基于高度的雾效。具体方法是，当给定一点在世界空间下的高度 y 后，f 的计算公式为：
$ f = \frac{H_{end} - y}{H_{end} - H_{start}}$ ，其中 $H_{start}$ 和 $H_{end}$ 分别表示受雾影响的起始高度和终止高度。


### 13.3.3 实现

新建脚本 FogWithDepthTexture.cs，继承 12.1 节中的基类：

``` csharp
using UnityEngine;

public class FogWithDepthTexture : PostEffectsBase
{
    public Shader fogShader;
    private Material fogMaterial;
    public Material material
    {
        get{
            fogMaterial = CheckShaderAndCreateMaterial(fogShader, fogMaterial);
            return fogMaterial;
        }
    }

    // 需要相机相关参数，如近裁剪平面的距离、FOV等，还有相机世界空间下的前方、上方、和右方等方向
    private Camera myCamera;
    public Camera camera
    {
        get{
            if(myCamera == null)
                myCamera = GetComponent<Camera>();
            return myCamera;
        }
    }

    private Transform myCameraTransform;
    public Transform cameraTransform
    {
        get{
            if(myCameraTransform == null)
                myCameraTransform = camera.transform;
            return myCameraTransform;
        }
    }

    // 控制雾的浓度
    [Range(0.0f, 3.0f)]
    public float fogDensity = 1.0f;
    // 控制雾的颜色
    public Color fogColor = Color.white;

    // 雾的起止高度
    public float fogStart = 0.0f;
    public float fogEnd = 2.0f;

    void OnEnable()
    {
        // 需要获取相机的深度纹理
        camera.depthTextureMode |= DepthTextureMode.Depth;
    }
    
    void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if(material != null)
        {
            // 存储近裁剪平面的四个角对应的向量
            Matrix4x4 frustumCorners = Matrix4x4.identity;

            // 获取需要的参数
            float fov = camera.fieldOfView;
            float near = camera.nearClipPlane;
            float far = camera.farClipPlane;
            float aspect = camera.aspect;

            // 用之前学的公式计算近裁剪平面的四个角对应的向量
            float halfHeight = near * Mathf.Tan(fov * 0.5f * Mathf.Deg2Rad);
            Vector3 toRight = cameraTransform.right * halfHeight * aspect;
            Vector3 toTop = cameraTransform.up * halfHeight;

            Vector3 topLeft = cameraTransform.forward * near + toTop - toRight;
            float scale = topLeft.magnitude / near;

            topLeft.Normalize();
            topLeft *= scale;

            Vector3 topRight = cameraTransform.forward * near + toTop + toRight;
            topRight.Normalize();
            topRight *= scale;

            Vector3 bottomRight = cameraTransform.forward * near - toTop + toRight;
            bottomRight.Normalize();
            bottomRight *= scale;

            Vector3 bottomLeft = cameraTransform.forward * near - toTop - toRight;
            bottomLeft.Normalize();
            bottomLeft *= scale;

            // 存储四个角对应的向量到矩阵中
            frustumCorners.SetRow(0, bottomLeft);
            frustumCorners.SetRow(1, bottomRight);
            frustumCorners.SetRow(2, topRight);
            frustumCorners.SetRow(3, topLeft);

            // 将参数传递给材质
            material.SetMatrix("_FrustumCornersRay", frustumCorners);
            material.SetMatrix("_ViewProjectionInverseMatrix", (camera.projectionMatrix * camera.worldToCameraMatrix).inverse);

            material.SetFloat("_FogDensity", fogDensity);
            material.SetColor("_FogColor", fogColor);
            material.SetFloat("_FogStart", fogStart);
            material.SetFloat("_FogEnd", fogEnd);

            // 把渲染结果显示到屏幕上
            Graphics.Blit(src, dest, material);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }
}
```

新建 Shader 命名为 Chapter13-FogWithDepthTexture

``` hlsl
Shader "Unity Shaders Book/Chapter 13/Fog With Depth Texture"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _FogDensity ("Fog Density", Range(0, 3)) = 1.0
        _FogColor ("Fog Color", Color) = (1, 1, 1, 1)
        _FogStart ("Fog Start", Float) = 0.0
        _FogEnd ("Fog End", Float) = 1.0
    }
    SubShader
    {
        CGINCLUDE

        #include "UnityCG.cginc"
        
        float4x4 _FrustumCornersRay;

        sampler2D _MainTex;
        half4 _MainTex_TexelSize;
        sampler2D _CameraDepthTexture;
        half _FogDensity;
        fixed4 _FogColor;
        float _FogStart;
        float _FogEnd;

        struct v2f{
            float4 vertex : SV_POSITION;
            half2 uv : TEXCOORD0;
            half2 uv_depth : TEXCOORD1;
            // 存储插值后的像素向量
            float4 interpolatedRay : TEXCOORD2;
        };

        v2f vert(appdata_img v)
        {
            v2f o;
            o.vertex = UnityObjectToClipPos(v.vertex);
            o.uv = v.texcoord;
            o.uv_depth = v.texcoord;

            // 对深度纹理采样坐标进行平台差异化处理
            #if UNITY_UV_STARTS_AT_TOP
                if (_MainTex_TexelSize.y < 0)
                    o.uv_depth.y = 1 - o.uv_depth.y;
            #endif
            
            // 一般使用if会造成比较大的性能问题，但本案例中用到的模型是一个四边形网格，只包含4个顶点，所以影响不大
            int index = 0;
            if (v.texcoord.x < 0.5 && v.texcoord.y < 0.5)
                index = 0;
            else if (v.texcoord.x > 0.5 && v.texcoord.y < 0.5)
                index = 1;
            else if (v.texcoord.x > 0.5 && v.texcoord.y > 0.5)
                index = 2;
            else
                index = 3;

            #if UNITY_UV_STARTS_AT_TOP
                if (_MainTex_TexelSize.y < 0)
                    index = 3 - index;
            #endif
            
            // 使用索引值来获取 _FrustumCornersRay 中对应的行作为该顶点的 interpolatedRay
            o.interpolatedRay = _FrustumCornersRay[index];

            return o;
        }

        fixed4 frag(v2f i) : SV_Target
        {
            // 首先使用 SAMPLE_DEPTH_TEXTURE 对深度纹理进行采样
            // 再使用 LinearEyeDepth 得到视角空间下的线性深度值
            float linearDepth = LinearEyeDepth(SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv_depth));
            float3 worldPos = _WorldSpaceCameraPos + linearDepth * i.interpolatedRay.xyz;

            // 计算当前像素高度 worldPos.y 对应的雾效系数
            float fogDensity = (_FogEnd - worldPos.y) / (_FogEnd - _FogStart);
            // 雾效系数与 _FogDensity 相乘后截取到 [0, 1] 范围内
            fogDensity = saturate(fogDensity * _FogDensity);

            fixed4 finalColor = tex2D(_MainTex, i.uv);
            finalColor.rgb = lerp(finalColor.rgb, _FogColor.rgb, fogDensity);

            return finalColor;
        }
        ENDCG

        Pass
        {
            ZTest Always Cull Off ZWrite Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            ENDCG
        }
    }
    FallBack Off
}
```


## 13.4 再谈边缘检测

在12章中学习了用 Sobel 算子对屏幕图像进行边缘检测，实现描边的效果，这种方法会产生很多我们不希望得到的边缘线（如：物体的纹理、阴影等位置也被描上黑边）。

本节将学习在深度和法线纹理上进行边缘检测，这些图像不会受纹理和光照的影响，而仅仅保存了当前渲染物体的模型信息，通过这样的方式检测出来的边缘更加可靠。

![图13.9 在深度和法线纹理上进行更健壮的边缘检测。左图：在原图上描边的效果。右图：只显示描边的效果](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.9.jpg)

本节将使用 Roberts 算子来进行边缘检测。
![图13.10 Roberts 算子](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.10.jpg)

Roberts 算子的本质就是计算左上角和右下角的差值，乘以右上角和左下角的差值，作为评估边缘的依据。

在下面的实现中，我们也会按这样的方式，取对角方向的深度或法线值，比较它们之间的差值，如果超过某个阈值(可由参数控制)，就认为它们之间存在一条边。

新建脚本 EdgeDetectNormalsAndDepth.cs，继承 12.1 节中的基类：

``` csharp
using UnityEngine;

public class EdgeDetectNormalsAndDepth : PostEffectsBase
{
    public Shader edgeDetectShader;
    private Material edgeDetectMaterial;
    public Material material
    {
        get
        {
            edgeDetectMaterial = CheckShaderAndCreateMaterial(edgeDetectShader, edgeDetectMaterial);
            return edgeDetectMaterial;
        }
    }

    [Range(0.0f, 1.0f)]
    public float edgeOnly = 0.0f;

    public Color edgeColor = Color.black;

    public Color backgroundColor = Color.white;

    // 采样距离，值越大描边越宽
    public float sampleDistance = 1.0f;

    // 影响当邻域的深度值/法线相差多少时，会被认为是边缘
    public float sensitivityDepth = 1.0f;
    public float sensitivityNormals = 1.0f;

    void OnEnable()
    {
        // 需要获取相机的深度和法线纹理
        GetComponent<Camera>().depthTextureMode |= DepthTextureMode.DepthNormals;
    }

    // [ImageEffectOpaque] 表示在不透明Pass执行完成后立即调用该函数，不对透明物体产生影响。
    [ImageEffectOpaque]
    void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (material!= null){
            material.SetFloat("_EdgeOnly", edgeOnly);
            material.SetColor("_EdgeColor", edgeColor);
            material.SetColor("_BackgroundColor", backgroundColor);
            material.SetFloat("_SampleDistance", sampleDistance);
            material.SetVector("_Sensitivity", new Vector4(sensitivityNormals,sensitivityDepth,0,0));

            Graphics.Blit(src, dest, material);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }
}

```

新建 Shader 命名为 Chapter13-EdgeDetectNormalsAndDepth ：

``` hlsl
Shader "Unity Shaders Book/Chapter 13/Edge Detect Normals And Depth"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _EdgeOnly ("Edge Only", Range(0,1)) = 0
        _EdgeColor ("Edge Color", Color) = (0,0,0,1)
        _BackgroundColor ("Background Color", Color) = (1,1,1,1)
        _SampleDistance ("Sample Distance", Float) = 1.0
        _Sensitivity ("Sensitivity", Vector) = (1,1,1,1)
    }
    SubShader
    {
        CGINCLUDE

        #include "UnityCG.cginc"

        sampler2D _MainTex;
        half4 _MainTex_TexelSize;
        fixed _EdgeOnly;
        fixed4 _EdgeColor;
        fixed4 _BackgroundColor;
        float _SampleDistance;
        half4 _Sensitivity;
        sampler2D _CameraDepthNormalsTexture;

        struct v2f
        {
            float4 pos : SV_POSITION;
            half2 uv[5] : TEXCOORD0;
        };

        v2f vert(appdata_img v)
        {
            v2f o;
            o.pos = UnityObjectToClipPos(v.vertex);

            half2 uv = v.texcoord;
            o.uv[0] = uv;

            #if UNITY_UV_STARTS_AT_TOP
            if (_MainTex_TexelSize.y < 0)
                uv.y = 1 - uv.y;
            #endif

            o.uv[1] = uv + _MainTex_TexelSize.xy * half2(1, 1) * _SampleDistance;
            o.uv[2] = uv + _MainTex_TexelSize.xy * half2(-1, -1) * _SampleDistance;
            o.uv[3] = uv + _MainTex_TexelSize.xy * half2(-1, 1) * _SampleDistance;
            o.uv[4] = uv + _MainTex_TexelSize.xy * half2(1, -1) * _SampleDistance;

            return o;
        }

        half CheckSame(half4 center, half4 sample)
        {
            // 对参数进行处理，得到两个采样点的法线和深度值。
            half2 centerNormal = center.xy;
            float centerDepth = DecodeFloatRG(center.zw);
            half2 sampleNormal = sample.xy;
            float sampleDepth = DecodeFloatRG(sample.zw);

            half2 diffNormal = abs(centerNormal - sampleNormal) * _Sensitivity.x;
            int isSameNormal = (diffNormal.x + diffNormal.y) < 0.1;

            float diffDepth = abs(centerDepth - sampleDepth) * _Sensitivity.y;
            int isSameDepth = diffDepth < 0.1 * centerDepth;

            return isSameNormal * isSameDepth ? 1.0 : 0.0;
        }

        half4 fragRobertsCrossDepthAndNormal(v2f i) : SV_Target
        {
            half4 sample1 = tex2D(_CameraDepthNormalsTexture, i.uv[1]);
            half4 sample2 = tex2D(_CameraDepthNormalsTexture, i.uv[2]);
            half4 sample3 = tex2D(_CameraDepthNormalsTexture, i.uv[3]);
            half4 sample4 = tex2D(_CameraDepthNormalsTexture, i.uv[4]);

            half edge = 1.0;

            edge *= CheckSame(sample1, sample2);
            edge *= CheckSame(sample3, sample4);

            fixed4 withEdgeColor = lerp(_EdgeColor, tex2D(_MainTex, i.uv[0]), edge);
            fixed4 onlyEdgeColor = lerp(_EdgeColor, _BackgroundColor, edge);

            return lerp(withEdgeColor, onlyEdgeColor, _EdgeOnly);
        }
        ENDCG

        Pass
        {
            ZTest Always Cull Off ZWrite Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment fragRobertsCrossDepthAndNormal
            ENDCG
        }
    }
    FallBack Off
}
```


## 13.5 扩展阅读

- [Unity 官方手册 Image Effects](https://docs.unity3d.com/510/Documentation/Manual/comp-ImageEffects.html)
- [深度纹理](https://docs.unity3d.com/Manual/SL-DepthTextures.html)

