高级篇涵盖了一些 Shader 的高级用法，例如，如何实现屏幕特效、利用法线和深度缓冲，以及非真实感渲染等，同时，我们还会介绍一些针对移动平台的优化技巧。


# 第12章 屏幕后处理效果

**屏幕后处理效果（screen post-processing effects）** 是游戏中实现屏幕特效的常见方法。

可参考文档：
* [Unity Image Effects](https://docs.unity3d.com/510/Documentation/Manual/comp-ImageEffects.html)
* [GPU Gems 3](https://developer.nvidia.com/gpugems/gpugems3/contributors)



## 12.1 建立一个基本的屏幕后处理脚本系统

屏幕后处理通常指在渲染完整个场景得到屏幕图像后，再对这个图像进行一系列操作，实现各种屏幕特效。如景深（Depth of Field）、运动模糊（Motion Blur）等。

**OnRenderImage** 函数是 Unity 内置的渲染管线中的一个函数，用于得到渲染后的屏幕图像，即抓取屏幕。它的函数声明如下：
``` csharp
// Unity 会把当前渲染得到的图像存储在第一个参数对应的源渲染纹理中，
// 通过函数中的一系列操作后，再把目标渲染纹理显示到屏幕上
MonoBehaviour.OnRenderImage(RenderTexture src, RenderTexture dest)
```

**Graphics.Blit** 函数可以实现图像的拷贝，并对图像进行一些操作。它有3种函数声明：
``` csharp
// src: 源纹理
// dest: 目标渲染纹理，如果为 null，则渲染到屏幕上
public static void Blit(Texture src, RenderTexture dest)

// mat: 材质，用于对图像进行操作
// pass: 默认为 -1，将会调用 Shader 内的所有 Pass。否则，只会调用给定索引的 Pass
public static void Blit(Texture src, RnederTexture dest, Material mat, int pass = -1)

public static void Blit(Texture src, Material mat, int pass = -1)
```

默认情况下，OnRenderImage 函数会在所有不透明和透明的 Pass 执行完毕后被调用，以便对场景中所有游戏对象都产生影响。但有时，会希望在不透明的 Pass 执行完毕后立即调用 OnRenderImage 函数，从而不对透明物体产生任何影响。可在 OnRenderImage 函数前添加 **ImageEffectOpaque** 属性实现这样的目的。


实现屏幕后处理效果过程：
1. 在摄像机中添加一个用于屏幕后处理的脚本。在脚本中，我们会实现 OnRenderImage 函数来获取当前屏幕的渲染纹理。
2. 调用 Graphics.Blit 函数使用特定的 Shader 对当前图像进行处理，再把返回的渲染纹理显示到屏幕上。
3. 对于复杂的屏幕特效，可能需要多次调用 Graphics.Blit 函数来对上一步输出结果进行下一步处理。


在进行屏幕后处理之前，我们需要检查一系列条件是否满足（如平台是否支持渲染纹理和屏幕特效，是否支持当前Shader等）。为此我们创建了一个用于屏幕后处理效果的基类 PostEffectsBase.cs。

``` csharp
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

// 编辑器状态下运行
[ExecuteInEditMode]
// 需要在相机上添加组件
[RequireComponent(typeof(Camera))]
public class PostEffectsBase : MonoBehaviour
{
    private void Start()
    {
        CheckResources();
    }


    /// <summary>
    /// 检查各种资源和条件是否满足
    /// </summary>
    protected void CheckResources()
    {
        bool isSupported = CheckSupport();

        if (!isSupported)
        {
            NotSupported();
        }
    }

    /// <summary>
    /// 检查平台是否支持
    /// </summary>
    protected bool CheckSupport()
    {
        if(!SystemInfo.supportsImageEffects || !SystemInfo.supportsRenderTextures)
        {
			Debug.LogWarning("This platform does not support image effects or render textures.");
			return false;
		}
		
		return true;
    }

    /// <summary>
    /// 平台不支持时的处理
    /// </summary>
    protected void NotSupported()
    {
        enabled = false;
    }

    /// <summary>
    /// 每个屏幕后处理效果通常需要指定一个Shader来创建一个用于处理渲染纹理的材质
    /// </summary>
    /// <param name="shader">该特效需要使用的Shader</param>
    /// <param name="material">后处理的材质</param>
    /// <returns>Shader可用的话则返回该Shader对应的材质，否则返回null</returns>
    protected Material CheckShaderAndCreateMaterial(Shader shader, Material material)
    {
        if (shader == null)
            return null;

        if (shader.isSupported && material && material.shader == shader)
            return material;
        
        if (!shader.isSupported)
            return null;
        else{
            material = new Material(shader);
            material.hideFlags = HideFlags.DontSave;
            if(material)
                return material;
        }
        return null;
    }
}
```



## 12.2 调整屏幕的亮度、饱和度和对比度

新建 C# 脚本 BrightnessSaturationAndContrast.cs，并继承 12.1 节中的基类：

``` csharp
using UnityEngine;

public class BrightnessSaturationAndContrast : PostEffectsBase
{
    public Shader briSatConShader;
    private Material briSatConMaterial;
    public Material material
    {
        get
        {
            briSatConMaterial = CheckShaderAndCreateMaterial(briSatConShader, briSatConMaterial);
            return briSatConMaterial;
        }
    }

    // 提供调整亮度、饱和度、对比度的参数
    [Range(0.0f, 3.0f)]
    public float brightness = 1.0f;

    [Range(0.0f, 3.0f)]
    public float saturation = 1.0f;

    [Range(0.0f, 3.0f)]
    public float contrast = 1.0f;


    private void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        // 检查材质是否可用，如果可用，就把参数传递给材质，再调用 Graphics.Blit 进行处理
        // 否则直接把原图显示到屏幕上
        if (material != null)
        {
            material.SetFloat("_Brightness", brightness);
            material.SetFloat("_Saturation", saturation);
            material.SetFloat("_Contrast", contrast);
            Graphics.Blit(src, dest, material);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }
}

```

新建 Shader 命名为 Chapter12-BrightnessSaturationAndContrast ：

``` hlsl
Shader "Unity Shaders Book/Chapter 12/Brightness Saturation And Contrast"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _Brightness ("Brightness", Float) = 1
        _Saturation ("Saturation", Float) = 1
        _Contrast ("Contrast", Float) = 1
    }
    SubShader
    {
        // 定义用于屏幕后处理的 Pass
        Pass
        {
            // 屏幕后处理 Shader 的“标配”
            ZTest Always  Cull Off  ZWrite Off


            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            
            #include "UnityCG.cginc"


            struct v2f
            {
                float2 uv : TEXCOORD0;
                float4 vertex : SV_POSITION;
            };

            sampler2D _MainTex;
            half _Brightness;
            half _Saturation;
            half _Contrast;

            v2f vert (appdata_img v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                o.uv = v.texcoord;
                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                fixed4 renderTex = tex2D(_MainTex, i.uv);

                // 亮度
                fixed3 finalColor = renderTex.rgb * _Brightness;

                // 计算饱和度
                // 饱和度为0时，颜色为黑白
                fixed luminance = dot(finalColor, fixed3(0.2125, 0.7154, 0.0721));
                fixed3 luminanceColor = fixed3(luminance, luminance, luminance);
                finalColor = lerp(luminanceColor, finalColor, _Saturation);

                // 计算对比度
                // 对比度为0时，颜色为灰色
                fixed3 avgColor = fixed3(0.5, 0.5, 0.5);
                finalColor = lerp(avgColor, finalColor, _Contrast);

                return fixed4(finalColor, renderTex.a);
            }
            ENDCG
        }
    }
    // 关闭 Fallback
    FallBack Off
}
```



## 12.3 边缘检测

边缘检测原理是利用一些边缘检测算子对图像进行 **卷积（convolution）** 操作。

### 12.3.1 什么是卷积

在图像处理中，卷积操作指的就是使用一个 **卷积核（kernel）** 对一张图像中的每个像素进行系列操作。卷积核通常是一个四方形网格结构（例如2x2、3x3的方形区域），该区域内每个方格都有一个权重值。
当对图像中的某个像素进行卷积时，我们会把卷积核的中心放置于该像素上，如图12.4所示，翻转核之后再依次计算核中每个元素和其覆盖的图像像素值的乘积并求和，得到的结果就是该位置的新像素值。

![图12.4 卷积核与卷积](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.4.jpg)

卷积操作可以实现很多常见的图像处理效果，例如图像模糊、边缘检测等。例如：均值模糊可以使用3x3的卷积核，核内每个元素的值均为1/9。


### 12.3.2 常见的边缘检测算子

如果相邻像素之间存在差别明显的颜色、亮度、纹理等属性，可以认为他们之间有一条边界，这种相邻像素之间的差异用 **梯度（gradient）** 来表示。边缘处的梯度绝对值会比较大，基于这样的理解，有几种不同的边缘检测算子被先后提出来。

![图12.5 3种常见的边缘检测算子](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.5.jpg)

它们都包含了两个方向的卷积核，分别用于检测水平方向和竖直方向上的边缘信息。在进行边缘检测时，我们需要对每个像素分别进行一次卷积计算，得到两个方向上的梯度值 Gx 和 Gy，而整体的梯度可按下面的公式计算而得：

$$
G = \sqrt{G_x^2 + G_y^2}
$$

由于上述计算包含了开根号操作，出于性能考虑，我们有时会使用绝对值操作来代替开根号操作：

$$
G = |G_x| + |G_y|
$$

当得到梯度 G 后，我们就可以根据此来判断哪些像素对应了边缘（梯度值越大，越有可能是边缘点）。


### 12.3.3 实现

新建脚本 EdgeDetection.cs，并继承 12.1 节中的基类：

``` csharp
using UnityEngine;

public class EdgeDetection : PostEffectsBase
{
    public Shader edgeDetectShader;

    private Material edgeDetectMaterial;
    public Material material{
        get{
            edgeDetectMaterial = CheckShaderAndCreateMaterial(edgeDetectShader, edgeDetectMaterial);
            return edgeDetectMaterial;
        }
    }

    // 提供用于调整边缘线强度、描边颜色以及背景颜色的参数
    // 当 edgesOnly 为 1.0 时，仅显示边缘线，不显示原渲染图像，否则边缘线会叠加到原渲染图像上
    [Range(0.0f, 1.0f)]
    public float edgeOnly = 0.0f;
    public Color edgeColor = Color.black;
    public Color backgroundColor = Color.white;

    private void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (material!= null){
            material.SetFloat("_EdgeOnly", edgeOnly);
            material.SetColor("_EdgeColor", edgeColor);
            material.SetColor("_BackgroundColor", backgroundColor);
            Graphics.Blit(src, dest, material);
        }
        else{
            Graphics.Blit(src, dest);
        }
    }
}
```

新建 Shader 命名为 Chapter12-EdgeDetection：

``` hlsl
Shader "Unity Shaders Book/Chapter 12/Edge Detection"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _EdgeOnly ("Edge Only", Range(0, 1)) = 0
        _EdgeColor ("Edge Color", Color) = (0, 0, 0, 1)
        _BackgroundColor ("Background Color", Color) = (1, 1, 1, 1)
    }
    SubShader
    {
        // 定义用于屏幕后处理的 Pass
        Pass
        {
            // 屏幕后处理 Shader 的“标配”
            ZTest Always  Cull Off  ZWrite Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            
            #include "UnityCG.cginc"

            sampler2D _MainTex;
            // 主纹理的纹素大小（例如：一张512 * 512的纹理，纹素大小为1/512）
            // 利用纹素，做相邻区域内纹理采样时，计算各相邻区域的纹理坐标
            half4 _MainTex_TexelSize;
            fixed _EdgeOnly;
            fixed4 _EdgeColor;
            fixed4 _BackgroundColor;

            struct v2f
            {
                float4 vertex : SV_POSITION;
                // 定义一个维数为9的数组，对应了Sobel算子采样时需要的9个邻域纹理坐标
                half2 uv[9] : TEXCOORD0;
            };

            v2f vert (appdata_img v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);

                // 把计算采样纹理坐标的代码从片元着色器转移到顶点着色器中，可以减少运算，提高性能
                half2 uv = v.texcoord;
                o.uv[0] = uv + half2(-1, -1) * _MainTex_TexelSize.xy;
                o.uv[1] = uv + half2(0, -1) * _MainTex_TexelSize.xy;
                o.uv[2] = uv + half2(1, -1) * _MainTex_TexelSize.xy;
                o.uv[3] = uv + half2(-1, 0) * _MainTex_TexelSize.xy;
                o.uv[4] = uv + half2(0, 0) * _MainTex_TexelSize.xy;
                o.uv[5] = uv + half2(1, 0) * _MainTex_TexelSize.xy;
                o.uv[6] = uv + half2(-1, 1) * _MainTex_TexelSize.xy;
                o.uv[7] = uv + half2(0, 1) * _MainTex_TexelSize.xy;
                o.uv[8] = uv + half2(1, 1) * _MainTex_TexelSize.xy;

                return o;
            }

            // 计算明度
            fixed luminance(fixed3 color){
                return 0.2125 * color.r + 0.7154 * color.g + 0.0721 * color.b;
            }

            half Sobel(v2f i){
                // Sobel算子
                const half Gx[9] = { -1, -2, -1, 
                                     0, 0, 0, 
                                     1, 2, 1 };
                const half Gy[9] = { -1, 0, 1, 
                                     -2, 0, 2,
                                     -1, 0, 1 };
                
                half texColor;
                half edgeX = 0;
                half edgeY = 0;

                // 依次对9个像素进行采样
                for(int it = 0; it < 9; it++){
                    // 计算亮度值，再与卷积核Gx和Gy中对应的权重相乘后，叠加到各自的梯度值上
                    texColor = luminance(tex2D(_MainTex, i.uv[it]).rgb);
                    edgeX += Gx[it] * texColor;
                    edgeY += Gy[it] * texColor;
                }

                // 用1减去两个方向的梯度值的绝对值，得到edge，值越小，表明越可能是边缘点
                half edge = 1 - abs(edgeX) - abs(edgeY);
                return edge;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                // 使用 Sobel 函数计算当前像素的梯度值
                half edge = Sobel(i);

                // 根据梯度值分别计算背景为原图和纯色下的颜色值
                fixed4 withEdgeColor = lerp(_EdgeColor, tex2D(_MainTex, i.uv[4]), edge);
                fixed4 onlyEdgeColor = lerp(_EdgeColor, _BackgroundColor, edge);

                return lerp(withEdgeColor, onlyEdgeColor, _EdgeOnly);
            }
            ENDCG
        }
    }
    // 关闭 Fallback
    FallBack Off
}
```

效果：
<center>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.3.jpg" alt="图12.3 左图：12.2节得到的结果。右图：进行边缘检测得到的结果"  width=360/>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.6.jpg" alt="图12.6 只显示边缘的屏幕效果"  width=240 />
</center>


## 12.4 高斯模糊

模糊的实现有很多种方法，例如：
* 均值模糊：卷积核中的各个元素值都相等，且相加等于1，也就是其邻域内各个像素值的平均值。
* 中值模糊：邻域内对所有像素排序后的中值替换掉原颜色。

一个更高级的模糊方法是高斯模糊。效果：

![图12.7 左图：原效果，右图：高斯模糊效果](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.7.jpg)

### 12.4.1 高斯滤波

高斯模糊同样利用了卷积计算，它使用的卷积核名为高斯核。高斯核是一个正方形大小的滤波核，其中每个元素都是基于下面的高斯方程：

$$
G(x,y) = \frac{1}{2\pi\sigma^2}e^{-\frac{(x^2+y^2)}{2\sigma^2}}
$$

* $\sigma$ ：标准方差（一般为1），它控制着高斯核的模糊程度。
* $x$ 和 $y$ ：当前位置到卷积核中心的整数距离。

要构建一个高斯核，我们只需要计算高斯核中各个位置对应的高斯值。为了保证滤波后的图像不会变暗，我们需要对高斯核中的权重进行归一化，即让每个权重除以所有权重的和，这样可以保证所有权重的和为1。因此，高斯函数中 $e$ 前面的系数实际不会对结果有任何影响。

高斯方程很好地模拟了邻域每个像素对当前处理像素的影响程度——距离越近，影响越大。高斯核的维数越高，模糊程度越大。

使用一个 N x N 的高斯核对图像进行卷积滤波，需要进行 N x N x W x H（W、H分别是图像的宽和高）次纹理采样，N 不断增大，采样次数会变得很大。
我们可以把这个二维高斯函数可以拆分成两个一维函数。使用两个一维的高斯核先后对图像进行滤波，得到的结果和直接使用二位高斯核是一样的，但采样次数只需要 2 x N x W x H。

![图12.8 一个5×5大小的高斯核。左图显示了标准方差为1的高斯核的权重分布。我们可以把这个二维高斯核拆分成两个一维的高斯核（右图）](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.8.jpg)


在本节,我们将会使用上述 5x5 的高斯核对原图像进行高斯模糊。我们将先后调用两个 Pass：
* 第一个 Pass 将会使用竖直方向的一维高斯核对图像进行滤波；
* 第二个 Pass 再使用水平方向的一维高斯核对图像进行滤波，得到最终的目标图像。

在实现中，我们还将利用图像缩放来进一步提高性能，并通过调整高斯滤波的应用次数来控制模糊程度（次数越多，图像越模糊）。



### 12.4.2 实现

新建脚本 GaussianBlur.cs，并继承 12.1 节中的基类：

``` csharp
using UnityEngine;

public class GaussianBlur : PostEffectsBase
{
    public Shader gaussianBlurShader;

    private Material gaussianBlurMaterial;
    public Material material{
        get{
            gaussianBlurMaterial = CheckShaderAndCreateMaterial(gaussianBlurShader, gaussianBlurMaterial);
            return gaussianBlurMaterial;
        }
    }

    // 提供调整高斯模糊迭代次数、模糊范围、和缩放系数的参数
    [Range(0, 4)]
    public int iterations = 3;

    [Range(0.2f, 3.0f)]
    public float blurSpread = 0.6f;

    // 降采样，值越大，性能越好，但过大可能会造成像素化
    [Range(1, 8)]
    public int downsample = 2;

    // // 版本1
    // public void OnRenderImage(RenderTexture src, RenderTexture dest)
    // {
    //     if (material != null)
    //     {
    //         int rtW = src.width;
    //         int rtH = src.height;

    //         // 分配一块缓冲区，因为高斯模糊需要调用两个 Pass，
    //         // 我们需要使用一块中间缓存存储第一个Pass执行完毕后得到的模糊结果
    //         RenderTexture buffer = RenderTexture.GetTemporary(rtW, rtH, 0);

    //         // 使用Shader中的第一个Pass对src进行处理，并将结果存储在buffer中
    //         Graphics.Blit(src, buffer, material, 0);
    //         // 使用Shader中的第二个Pass对buffer进行处理，返回最终的屏幕图像
    //         Graphics.Blit(buffer, dest, material, 1);

    //         // 释放中间缓存
    //         RenderTexture.ReleaseTemporary(buffer);
    //     }
    //     else
    //     {
    //         Graphics.Blit(src, dest);
    //     }
    // }

    // // 版本2 利用缩放对图像进行降采样，从而减少需要处理的像素数量，提高性能
    // public void OnRenderImage(RenderTexture src, RenderTexture dest)
    // {
    //     if (material != null)
    //     {
    //         // 利用缩放对图像进行降采样，从而减少需要处理的像素数量，提高性能
    //         int rtW = src.width / downsample;
    //         int rtH = src.height / downsample;

    //         // 分配一块缓冲区，因为高斯模糊需要调用两个 Pass，
    //         // 我们需要使用一块中间缓存存储第一个Pass执行完毕后得到的模糊结果
    //         RenderTexture buffer = RenderTexture.GetTemporary(rtW, rtH, 0);
    //         // 双线性过滤，防止降采样后的纹理像素看上去不连贯，丢失严重
    //         buffer.filterMode = FilterMode.Bilinear;

    //         // 使用Shader中的第一个Pass对src进行处理，并将结果存储在buffer中
    //         Graphics.Blit(src, buffer, material, 0);
    //         // 使用Shader中的第二个Pass对buffer进行处理，返回最终的屏幕图像
    //         Graphics.Blit(buffer, dest, material, 1);

    //         // 释放中间缓存
    //         RenderTexture.ReleaseTemporary(buffer);
    //     }
    //     else
    //     {
    //         Graphics.Blit(src, dest);
    //     }
    // }

    // 版本3 考虑了高斯模糊的迭代次数
    public void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (material != null)
        {
            // 利用缩放对图像进行降采样，从而减少需要处理的像素数量，提高性能
            int rtW = src.width / downsample;
            int rtH = src.height / downsample;

            // 分配一块缓冲区，因为高斯模糊需要调用两个 Pass，
            // 我们需要使用一块中间缓存存储第一个Pass执行完毕后得到的模糊结果
            RenderTexture buffer = RenderTexture.GetTemporary(rtW, rtH, 0);
            // 双线性过滤，防止降采样后的纹理像素看上去不连贯，丢失严重
            buffer.filterMode = FilterMode.Bilinear;

            Graphics.Blit(src, buffer);

            for (int i = 0; i < iterations; i++)
            {
                material.SetFloat("_BlurSize", 1.0f + i * blurSpread);

                RenderTexture buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0);

                // 第一个Pass，纵向模糊
                Graphics.Blit(buffer, buffer1, material, 0);

                // 释放buffer，将结果值buffer1存储到buffer中，重新分配buffer1
                RenderTexture.ReleaseTemporary(buffer);
                buffer = buffer1;
                buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0);

                // 第二个Pass，横向模糊
                Graphics.Blit(buffer, buffer1, material, 1);

                RenderTexture.ReleaseTemporary(buffer);
                buffer = buffer1;
            }

            // 最后一步，将模糊后的图像输出到屏幕上
            Graphics.Blit(buffer, dest);
            // 释放中间缓存
            RenderTexture.ReleaseTemporary(buffer);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }
}

```

新建 Shader 命名为 Chapter12-GaussianBlur：

``` hlsl
Shader "Unity Shaders Book/Chapter 12/Gaussian Blur"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _BlurSize ("Blur Size", Float) = 1
    }
    SubShader
    {
        // 定义可复用的代码块，可在不同Pass中调用
        CGINCLUDE

        #include "UnityCG.cginc"

        sampler2D _MainTex;
        // 主纹理的纹素大小（例如：一张512 * 512的纹理，纹素大小为1/512）
        // 利用纹素，做相邻区域内纹理采样时，计算各相邻区域的纹理坐标
        half4 _MainTex_TexelSize;
        float _BlurSize;

        struct v2f
        {
            float4 vertex : SV_POSITION;
            half2 uv[5] : TEXCOORD0;
        };

        v2f vertBlurVertical (appdata_img v)
        {
            v2f o;
            o.vertex = UnityObjectToClipPos(v.vertex);

            // 把计算采样纹理坐标的代码从片元着色器转移到顶点着色器中，可以减少运算，提高性能
            // 构建一个纵向高斯核
            half2 uv = v.texcoord;
            o.uv[0] = uv;
            // _BlurSize控制邻域像素之间的采样距离，_BlurSize值越大，模糊程度越高，但采样数不会受到影响
            // 但过大的_BlurSize值会造成虚影
            o.uv[1] = uv + float2(0.0, _MainTex_TexelSize.y * 1.0) * _BlurSize;
            o.uv[2] = uv - float2(0.0, _MainTex_TexelSize.y * 1.0) * _BlurSize;
            o.uv[3] = uv + float2(0.0, _MainTex_TexelSize.y * 2.0) * _BlurSize;
            o.uv[4] = uv - float2(0.0, _MainTex_TexelSize.y * 2.0) * _BlurSize;

            return o;
        }

        v2f vertBlurHorizontal (appdata_img v)
        {
            v2f o;
            o.vertex = UnityObjectToClipPos(v.vertex);

            // 把计算采样纹理坐标的代码从片元着色器转移到顶点着色器中，可以减少运算，提高性能
            // 构建一个横向高斯核
            half2 uv = v.texcoord;
            o.uv[0] = uv;
            // _BlurSize控制邻域像素之间的采样距离，_BlurSize值越大，模糊程度越高，但采样数不会受到影响
            // 但过大的_BlurSize值会造成虚影
            o.uv[1] = uv + float2(_MainTex_TexelSize.x * 1.0, 0.0) * _BlurSize;
            o.uv[2] = uv - float2(_MainTex_TexelSize.x * 1.0, 0.0) * _BlurSize;
            o.uv[3] = uv + float2(_MainTex_TexelSize.x * 2.0, 0.0) * _BlurSize;
            o.uv[4] = uv - float2(_MainTex_TexelSize.x * 2.0, 0.0) * _BlurSize;

            return o;
        }

        fixed4 fragBlur (v2f i) : SV_Target
        {
            // 由于对称性，只需记录3个高斯权重
            float weight[3] = {0.4026, 0.2442, 0.0545};

            fixed3 sum = tex2D(_MainTex, i.uv[0]).rgb * weight[0];

            for (int it = 1; it < 3; it++){
                sum += tex2D(_MainTex, i.uv[it*2-1]).rgb * weight[it];
                sum += tex2D(_MainTex, i.uv[it*2]).rgb * weight[it];
            }

            return fixed4(sum, 1.0);
        }

        ENDCG

        // 屏幕后处理 Shader 的“标配”
        ZTest Always  Cull Off  ZWrite Off

        Pass
        {
            NAME "GAUSSIAN_BLUR_VERTICAL"

            CGPROGRAM
            #pragma vertex vertBlurVertical
            #pragma fragment fragBlur
            
            ENDCG
        }
        Pass
        {
            NAME "GAUSSIAN_BLUR_HORIZONTAL"

            CGPROGRAM
            #pragma vertex vertBlurHorizontal
            #pragma fragment fragBlur
            
            ENDCG
        }
    }
    // 关闭 Fallback
    FallBack Off
}
```


## 12.5 Bloom 效果

Bloom 效果是让画面中较亮的区域“扩散”到周围的区域中，造成一种朦胧的效果。

![图12.10 左边为原图，右边为 Bloom 效果](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/12.10.jpg)

Bloom 的实现非常简单：
1. 首先根据一个阈值提取出图像中的较亮区域，把它们存储在一张渲染纹理中；
2. 利用高斯模糊对这张渲染纹理进行模糊处理，模拟光线扩散的效果；
3. 最后将其和原图像进行混合，得到最终的效果。

新建脚本 Bloom.cs：

``` csharp
using UnityEngine;

public class Bloom : PostEffectsBase
{
    public Shader bloomShader;
    private Material bloomMaterial;
    public Material material
    {
        get
        {
            bloomMaterial = CheckShaderAndCreateMaterial(bloomShader, bloomMaterial);
            return bloomMaterial;
        }
    }

    // 由于 Bloom 效果是建立在高斯模糊的基础上，因此参数几乎与高斯模糊相同。

    // 提供调整高斯模糊迭代次数、模糊范围、和缩放系数的参数
    [Range(0, 4)]
    public int iterations = 3;

    [Range(0.2f, 3.0f)]
    public float blurSpread = 0.6f;

    [Range(1, 8)]
    public int downsample = 2;

    // 控制提取较亮区域时使用的阈值
    [Range(0.0f, 4.0f)]
    public float luminanceThreshold = 0.6f;


    public void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (material != null)
        {
            material.SetFloat("_LuminanceThreshold", luminanceThreshold);
            // 利用缩放对图像进行降采样，从而减少需要处理的像素数量，提高性能
            int rtW = src.width / downsample;
            int rtH = src.height / downsample;

            // 分配一块缓冲区，因为高斯模糊需要调用两个 Pass，
            // 我们需要使用一块中间缓存存储第一个Pass执行完毕后得到的模糊结果
            RenderTexture buffer = RenderTexture.GetTemporary(rtW, rtH, 0);
            // 双线性过滤，防止降采样后的纹理像素看上去不连贯，丢失严重
            buffer.filterMode = FilterMode.Bilinear;

            // 第一个Pass，提取图像中较亮区域
            Graphics.Blit(src, buffer, material, 0);

            for (int i = 0; i < iterations; i++)
            {
                material.SetFloat("_BlurSize", 1.0f + i * blurSpread);

                RenderTexture buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0);

                Graphics.Blit(buffer, buffer1, material, 1);

                RenderTexture.ReleaseTemporary(buffer);
                buffer = buffer1;
                buffer1 = RenderTexture.GetTemporary(rtW, rtH, 0);

                Graphics.Blit(buffer, buffer1, material, 2);

                RenderTexture.ReleaseTemporary(buffer);
                buffer = buffer1;
            }

            // 将 buffer 传递给材质
            material.SetTexture("_Bloom", buffer);
            // 使用 Shader 中的第四个 Pass 来进行最后的混合，将结果存储在目标渲染纹理中
            Graphics.Blit(src, dest, material, 3);

            // 释放临时缓存
            RenderTexture.ReleaseTemporary(buffer);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }
}

```

新建 Shader 命名为 Chapter12-Bloom：

``` hlsl
Shader "Unity Shaders Book/Chapter 12/Bloom"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _Bloom ("Bloom (RGB)", 2D) = "black" {}
        _LuminanceThreshold ("Luminance Threshold", Float) = 0.5
        _BlurSize ("Blur Size", Float) = 1
    }
    SubShader
    {
        // 定义可复用的代码块，可在不同Pass中调用
        CGINCLUDE

        #include "UnityCG.cginc"

        sampler2D _MainTex;
        // 主纹理的纹素大小（例如：一张512 * 512的纹理，纹素大小为1/512）
        // 利用纹素，做相邻区域内纹理采样时，计算各相邻区域的纹理坐标
        half4 _MainTex_TexelSize;
        sampler2D _Bloom;
        float _LuminanceThreshold;
        float _BlurSize;

        struct v2f
        {
            float4 vertex : SV_POSITION;
            half2 uv : TEXCOORD0;
        };

        // 定义提取较亮区域需要用的顶点片元着色器
        v2f vertExtractBright (appdata_img v)
        {
            v2f o;
            o.vertex = UnityObjectToClipPos(v.vertex);
            o.uv = v.texcoord;

            return o;
        }
        
        fixed luminance(fixed3 color)
        {
            return 0.2125 * color.r + 0.7154 * color.g + 0.0721 * color.b;
        }

        fixed4 fragExtractBright (v2f i) : SV_Target
        {
            fixed4 color = tex2D(_MainTex, i.uv);
            // 将采样得到的亮度值减去阈值，并把结果截取到0-1之间
            fixed val = clamp(luminance(color.rgb) - _LuminanceThreshold, 0.0, 1.0);

            // 把该值和原像素相乘，得到提取后的亮部区域
            return color * val;
        }


        // 定义混合亮部图像和原图像的顶点片元着色器
        struct v2fBloom
        {
            float4 vertex : SV_POSITION;
            half4 uv : TEXCOORD0;
        };

        v2fBloom vertBloom (appdata_img v)
        {
            v2fBloom o;
            o.vertex = UnityObjectToClipPos(v.vertex);
            o.uv.xy = v.texcoord;
            o.uv.zw = v.texcoord;

            #if UNITY_UV_STARTS_AT_TOP
            if (_MainTex_TexelSize.y < 0.0)
                o.uv.w = 1.0 - o.uv.w;
            #endif

            return o;
        }

        fixed4 fragBloom (v2fBloom i) : SV_Target
        {
            return tex2D(_MainTex, i.uv.xy) + tex2D(_Bloom, i.uv.zw);
        }


        ENDCG

        // 屏幕后处理 Shader 的“标配”
        ZTest Always  Cull Off  ZWrite Off

        Pass
        {
            CGPROGRAM
            #pragma vertex vertExtractBright
            #pragma fragment fragExtractBright
            
            ENDCG
        }

        // 通过 UsePass 语义指明上一节中高斯模糊定义的两个 Pass
        UsePass "Unity Shaders Book/Chapter 12/Gaussian Blur/GAUSSIAN_BLUR_VERTICAL"
        UsePass "Unity Shaders Book/Chapter 12/Gaussian Blur/GAUSSIAN_BLUR_HORIZONTAL"
        
        Pass
        {
            CGPROGRAM
            #pragma vertex vertBloom
            #pragma fragment fragBloom
            
            ENDCG
        }
    }
    // 关闭 Fallback
    FallBack Off
}
```


## 12.6 运动模糊

运动模糊是真实世界中相机的一种效果，如果在摄像机曝光时，拍摄场景发生了变化，就会产生模糊的画面。在计算机图像中，由于不存在曝光现象，渲染出来的图像往往都是棱角分明，缺少运动模糊。

运动模糊实现有多种方法：
* **累计缓存（Accumulation Buffer）** ：当物体快速移动产生多张图像后，取它们之间的平均值作为最后的运动模糊图像。然而，这种方法性能消耗很大，因为想要获取多张帧图像往往意味着我们需要在同一帧里渲染多次场景。

* **速度缓存（Velocity Buffer）** ：这个缓存中存储了各个像素当前的运动速度，然后利用该值来决定模糊的方向和大小。


我们将使用类似第一种方法，不需要在一帧中把场景渲染多次，但需要保存之前的渲染结果，不断把当前的渲染图像叠加到之前的渲染图像中，从而产生一种运动轨迹的视觉效果。这种方法性能比累计缓存好，但模糊效果可能会略有影响。

新建脚本 MotionBlur.cs：

``` csharp
using UnityEngine;

public class MotionBlur : PostEffectsBase
{
    public Shader motionBlurShader;
    private Material motionBlurMaterial;
    public Material material
    {
        get
        {
            motionBlurMaterial = CheckShaderAndCreateMaterial(motionBlurShader, motionBlurMaterial);
            return motionBlurMaterial;
        }
    }

    // 定义运动模糊在混合图像时使用的模糊参数
    [Range(0.0f, 0.9f)]
    public float blurAmount = 0.5f;

    // 定义一个 RenderTexture 用于存储之前图像叠加的结果
    private RenderTexture accumulationTexture;

    // 当脚本不运行时，立即销毁 accumulationTexture
    // 因为我们希望下一次开始应用运动模糊时重新叠加图像
    void OnDisable()
    {
        DestroyImmediate(accumulationTexture);
    }

    private void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (material != null)
        {
            // 如果 accumulationTexture 为空，或者大小与 src 不匹配，则创建一个适合当前分辨率的 accumulationTexture
            if (accumulationTexture == null || accumulationTexture.width != src.width || accumulationTexture.height != src.height)
            {
                DestroyImmediate(accumulationTexture);
                accumulationTexture = new RenderTexture(src.width, src.height, 0);
                accumulationTexture.hideFlags = HideFlags.HideAndDontSave;
                Graphics.Blit(src, accumulationTexture);
            }

            // MarkRestoreExpected 函数表明我们需要进行一个渲染纹理的恢复操作
            // 恢复操作：发生在渲染到纹理而该纹理又没有提前清空或销毁的情况
            accumulationTexture.MarkRestoreExpected();

            material.SetFloat("_BlurAmount", 1.0f - blurAmount);

            // 把当前屏幕图像叠加到 accumulationTexture 中
            Graphics.Blit(src, accumulationTexture, material);
            Graphics.Blit(accumulationTexture, dest);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }

}
```

新建 Shader 命名为 Chapter12-MotionBlur：

``` hlsl
Shader "Unity Shaders Book/Chapter 12/Motion Blur"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _BlurAmount ("Blur Amount", Float) = 1.0
    }
    SubShader
    {
        CGINCLUDE

        #include "UnityCG.cginc"

        sampler2D _MainTex;
        fixed _BlurAmount;

        struct v2f
        {
            float2 uv : TEXCOORD0;
            float4 vertex : SV_POSITION;
        };

        v2f vert (appdata_img v)
        {
            v2f o;
            o.vertex = UnityObjectToClipPos(v.vertex);
            o.uv = v.texcoord;
            return o;
        }

        // 对当前图像进行采样，将其A通道的值设为_BlurAmount，方便后面混合时控制透明通道
        fixed4 fragRGB (v2f i) : SV_Target
        {
            return fixed4(tex2D(_MainTex, i.uv).rgb, _BlurAmount);
        }

        // 直接返回采样结果
        half4 fragA (v2f i) : SV_Target
        {
            return tex2D(_MainTex, i.uv);
        }

        ENDCG

        // 后处理“标配”
        ZTest Always Cull Off ZWrite Off

        // 第一个Pass，更新渲染纹理的RGB通道，
        // 通过_BlurAmount设置A通道来混合图像，但不会把A通道写入渲染纹理
        Pass
        {
            Blend SrcAlpha OneMinusSrcAlpha
            ColorMask RGB

            CGPROGRAM

            #pragma vertex vert
            #pragma fragment fragRGB

            ENDCG
        }

        // 用当前图像的透明通道作为混合后的图像的透明通道
        Pass
        {
            Blend One Zero
            ColorMask A
            
            CGPROGRAM

            #pragma vertex vert
            #pragma fragment fragA

            ENDCG
        }
    }
    FallBack Off
}
```



----------------------------------------------------------------



# 第13章 使用深度和法线纹理

很多时候我们不仅需要当前屏幕的颜色信息，还希望得到深度和法线信息。例如，在进行边缘检测时，直接利用颜色信息会使检测到的边缘信息受物体纹理和光照等外部因素的影响，得到很多我们不需要的边缘点。一种更好的方法是，我们可以在深度纹理和法线纹理上进行边缘检测，这些图像不会受纹理和光照影响，而仅仅保存了当前渲染物体的模型信息，通过这样的方式检测出来的边缘更加可靠。


## 13.1 获取深度和法线纹理

### 13.1.1 背后的原理

深度纹理实际是一张渲染纹理，里面存储的是高精度的深度值，数值的范围是[0, 1]，且是非线性分布的。

这些深度值来自于顶点变换后得到的归一化的设备坐标（Normalized Device Coordinates，NDC），NDC 每个坐标的范围是[-1, 1]。

为什么是非线性分布的？
顶点从模型空间变换到其次裁剪坐标系下，是通过在顶点着色器中乘以 MVP 矩阵得到的，在变换的最后一步，如果使用了透视投影类型的相机，这个投影矩阵就是非线性的。

Unity 中透视投影对顶点的变换过程：

![图13.1 在透视投影中，投影矩阵首先对顶点进行了缩放。在经过齐次除法后，透视投影的裁剪空间会变换到一个立方体。图中标注了4个关键点经过投影矩阵变换后的结果](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.1.jpg)

> 注意：投影过程建立在Unity对坐标系的假定上（观察空间为右手坐标系），使用列矩阵在矩阵右侧进行相乘，且得到NDC后z分量范围在[-1, 1]之间；在类似DirectX这样的图形接口中，变换后z分量在[0, 1]之间。

使用正交摄像机时投影变换的过程：

![图13.2 在正交投影中，投影矩阵对顶点进行了缩放。在经过齐次除法后，正交投影的裁剪空间会变换到一个立方体。图中标注了4个关键点经过投影矩阵变换后的结果](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.2.jpg)


由于 NDC 中 z 分量的范围在 [-1, 1]，为了让这些值能够存储在一张图像中，我们需要将其映射到 [0, 1] 之间： 
$d = 0.5 * z_{ndc} + 0.5$，其中 d 对应深度纹理中的像素值， $z_{ndc}$ 是 NDC 中的 z 分量。

Unity 是怎么得到这样一张深度纹理的呢？
* 在 Unity 中，深度纹理可以直接来自于真正的深度缓存，也可以是由一个单独的Pass渲染而得，这取决于使用的渲染路径和硬件。
* 通常来讲当使用延迟渲染路径(包括遗留的延迟渲染路径)时，深度纹理理所当然可以访问到，因为延迟渲染会把这些信息渲染到 G-bufer 中。
* 当无法直接获取深度缓存时，深度和法线纹理是通过一个单独的 Pass 渲染而得的。具体实现是：
    * Unity会使用着色器替换（Shader Replacement）技术选择那些渲染类型（RenderType）为 Opaque 的物体
    * 判断它们使用的渲染队列是否小于等于2500（内置的Background、Geometry和AlphaTest 渲染队列均在此范围内）
    * 如果满足条件，就把它渲染到深度和法线纹理中。
    * 因此，要想让物体能够出现在深度和法线纹理中，就必须在 Shader 中设置正确的 RenderType 标签

在Unity中，我们可以选择让一个摄像机生成一张深度纹理或是一张深度+法线纹理。
* 只需要一张单独的深度纹理时
    * Unity会直接获取深度缓存或是按之前讲到的着色器替换技术，选取需要的不透明物体，并使用它投射阴影时使用的Pass来得到深度纹理。
    * 如果 Shader 中不包含这样一个 Pass，那么这个物体就不会出现在深度纹理中。
    * 深度纹理的精度通常是24位或16位，这取决于使用的深度缓存的精度。
* 如果选择生成一张深度+法线纹理时
    * Unity会创建一张和屏幕分辨率相同、精度为32位（每个通道为8位）的纹理，其中观察空间下的法线信息会被编码进纹理的 R 和 G 通道，而深度信息会被编码进 B 和 A 通道。
    * 法线信息的获取在延迟渲染中是可以非常容易就得到的，Unity 只需要合并深度和法线缓存即可。
    * 而在前向渲染中，默认情况下是不会创建法线缓存的，因此 Unity 底层使用了一个单独的 Pass 把整个场景再次渲染一遍来完成。这个 Pass 在 buildin_shaders-xxx/DefaultResources/Camera-DepthNormalTexture.shader 文件中可以找到。



### 13.1.2 如何获取

在脚本中设置摄像机的 depthTextureMode，然后再在 Shader 中直接访问特定的纹理属性即可。

``` csharp
// 获取深度纹理
camera.depthTextureMode = DepthTextureMode.Depth;
// 获取深度+法线纹理
camera.depthTextureMode = DepthTextureMode.DepthNormals;
// 同时产生一张深度和深度+法线纹理
camera.depthTextureMode |= DepthTextureMode.Depth;
camera.depthTextureMode |= DepthTextureMode.DepthNormals;
```

在 Shader 中，我们可以通过 _CameraDepthTexture 访问深度纹理，或者通过 _CameraDepthNormalsTexture 访问深度+法线纹理。

当在 Shader 中访问到深度纹理后，我们就可以使用当前像素的纹理坐标对它进行采样。绝大多数情况可以直接通过 tex2D() 函数来采样，但在某些平台（如 PS3 和 PS2）上，需要进行一些特殊处理。Unity 提供了统一的宏 SAMPLE_DEPTH_TEXTURE 用来处理平台差异造成的问题：
```
// i.uv 是一个 float2 类型的变量，对应了当前像素的纹理坐标
float d = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv);
```

类似的宏还有 SAMPLE_DEPTH_TEXTURE_PROJ、SAMPLE_DEPTH_TEXTURE_LOD，可以在 Unity 内置的 HLSLSupport.cginc 文件中找到。

SAMPLE_DEPTH_TEXTURE_PROJ 接受两个参数：深度纹理和一个float3或float4类型的纹理坐标，它的内部使用了 tex2Dproj 这样的函数进行投影纹理采样，纹理坐标的前两个分量会除以最后一个分量，再进行纹理采样。如果提供了第四个分量，还会进行一次比较，通常用于阴影的实现中。 SAMPLE_DEPTH_TEXTURE_PROJ 的第二个参数通常有顶点着色器输出差值得到的屏幕坐标：

```
// i.scr 是顶点着色器中通过调用ComputeScreenPos(o.pos)得到的屏幕坐标
float d = SAMPLE_DEPTH_TEXTURE_PROJ(_CameraDepthTexture, UNITY_PROJ_COORD(i.scrPos));
```

通过纹理采样得到的深度值往往是非线性的，在我们的计算过程中通常是需要线性的深度值，也就是说，我们需要把投影后的深度值变换到线性空间下，例如视角空间下的深度值。我们可以通过倒推顶点变换的过程来进行这个转换。

下面我们以透视投影为例，推导如何由深度纹理中的深度信息计算得到视角空间下的深度值。

由4.6.7节可知，当使用透视投影的矩阵 $P_{clip}$ 对视角空间下 一个顶点进行变换后，裁剪空间下顶点的 z 和 w 分量为：

$$
z_{clip} = z_{view} \frac{Far + Near}{Far - Near} - \frac{2 * Near * Far}{Far - Near}
$$

$$
w_{clip} = -z_{view} 
$$

其中，Far 和 Near 分别是远近裁剪平面的距离。
然后，通过齐次除法可以得到 NDC 下的 z 分量：

$$
z_{ndc} = \frac{z_{clip}}{w_{clip}} 
= \frac{Far + Near}{Far - Near} + \frac{2 * Near * Far}{(Far - Near) * z_{view}}
$$

在 13.1.1 节中我们知道，深度纹理中的深度值是通过下面的公式由 NDC 计算而得的：

$$
d = 0.5 * z_{ndc} + 0.5
$$

由上面这些式子，我们可以推导出用 d 表示而得的 $z_{view}$ 的表达式：

$$
z_{view} = \frac{1}{\frac{Far - Near}{Near * Far}d - \frac{1}{Near}}
$$

由于在 Unity 中使用的视角空间中，摄像机正向对应的 z 值均为负值，因此为了得到深度值的正数表示，我们需要对上面的结果取反：

$$
z'_{view} = \frac{1}{\frac{Far - Near}{Near * Far}d + \frac{1}{Near}}
$$

它的取值范围就是视锥体深度范围 [Near, Far]。如果我们想得到范围在 [0, 1] 之间的深度值，只需要把上面的结果除以 Far 即可。这样，0 表示该点与摄像机位于同一位置，1 表示该点位于视锥体的远裁剪平面上。

$$
z_{01} = \frac{1}{\frac{Far - Near}{Near}d + \frac{Far}{Near}}
$$

Unity 提供了两个辅助函数为我们进行上述计算过程 —— LinearEyeDepth 和 Linear01Depth。
* LinearEyeDepth：把深度纹理的采样结果转换到视角空间下的深度值，也就是 $z'_{view}$ 。
* Linear01Depth：返回一个范围在 [0, 1] 之间的线性深度值，也就是 $z_{01}$ 。

这两个函数内部使用了内置的 _ZBufferParams 变量来得到远近裁剪平面的距离。

如果需要获取深度+法线纹理，可直接使用 tex2D 函数对 _CameraDepthNormalsTexture 进行采样，得到里面存储的深度和法线信息。
Unity 提供了辅助函数 DecodeDepthNormal 来对这个采样结果进行解码，从而得到深度值和法线方向，其在UnityCG.cginc里的定义：

```
// enc: 对深度+法线纹理的采样结果，是Unity对其编码后的结果
// xy分量存储视角空间下的法线信息，zw分量是深度信息
// depth：解码后的深度值，范围在[0, 1]，为线性深度值
// normal：解码后的视角空间下的法线方向
inline void DecodeDepthNormal(float4 enc, out float depth, out float3 normal)
{
    depth = DecodeFloatRG(enc.zw);
    normal = DecodeViewNormalStereo(enc);
}
```


### 13.1.3 查看深度和法线纹理

使用 **帧调试器（Frame Debugger）** 可以查看深度和法线纹理。

![图13.3 使用 Frame Debugger 查看深度纹理（左）和深度 + 法线纹理（右）](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.3.jpg)

使用帧调试器查看到的深度纹理是非线性空间的深度值，而深度+法线纹理都是由 Unity 编码后的结果。有时，显示出线性空间下的深度信息或解码后的法线方向会更加有用。此时，我们可以自行在片元着色器中输出转换或解码后的深度和法线值：

```
// 输出线性深度值
float depth = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv);
float linearDepth = Linear01Depth(depth);
return fixed4(linearDepth, linearDepth, linearDepth, 1.0);

// 输出法线方向
fixed3 normal = DecodeViewNormalStereo(tex2D(_CameraDepthNormalsTexture, i.uv).xy);
return fixed4(normal * 0.5 + 0.5, 1.0);
```

查看深度纹理时，如果画面几乎是全黑或全白时，可以将相机的远裁剪平面的距离（Unity默认为1000）调小，使之刚好覆盖场景的所在区域即可。因为若裁剪平面的距离过大，会导致距离相机较近的物体会被映射到非常小的深度值，导致看起来全黑（场景为封闭区域比较常见）；相反若场景为开放区域，物体距离相机较远，则会导致画面几乎全白。

![图13.4 左图：线性空间下的深度纹理。右图：解码后并且被映射到[0, 1]范围内的视角空间下的法线纹理](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.4.jpg)



## 13.2 再谈运动模糊

在12.6节中，我们通过混合多张屏幕图像来模拟运动模糊的效果。但是，另一种应用更加广泛的技术则是使用速度映射图。

速度映射图中存储了每个像素的速度，然后使用这个速度来决定模糊的方向和大小。速度缓冲的生成有多种方法：
* 一种方法是把场景中所有物体的速度渲染到一张纹理中。
   * 缺点：需要修改场景中所有物体的 Shader 代码，使其添加计算速度的代码并输出到一个渲染纹理中。
* [《GPU Gems3》第27章](https://developer.nvidia.com/gpugems/gpugems3/part-iv-image-effects/chapter-27-motion-blur-post-processing-effect) 中介绍了一种生成速度映射图的方法：
   1. 利用深度纹理在片元着色器中为每个像素计算其在世界空间下的位置，这是通过使用当前的视角\*投影矩阵的逆矩阵对 NDC 下的顶点坐标进行变换得到的。
   2. 当得到世界空间中的顶点坐标后，我们使用前一帧的视角\*投影矩阵对其进行变换，得到该位置在前一帧中的 NDC 坐标。
   3. 然后，我们计算前一帧和当前帧的位置差，生成该像素的速度。
   * 优点：可以在一个屏幕后处理步骤中完成整个效果的模拟；
   * 缺点：需要在片元着色器中进行两次矩阵乘法的操作，对性能有所影响。


新建脚本 MotionBlurWithDepthTexture.cs，继承 12.1 节中的基类：

``` csharp
using UnityEngine;

public class MotionBlurWithDepthTexture : PostEffectsBase
{
    public Shader motionBlurShader;
    private Material motionBlurMaterial;
    public Material material
    {
        get{
            motionBlurMaterial = CheckShaderAndCreateMaterial(motionBlurShader, motionBlurMaterial);
            return motionBlurMaterial;
        }
    }

    // 定义运动模糊时模糊图像使用的大小
    [Range(0.0f, 1.0f)]
    public float blurSize = 0.5f;

    // 需要摄像机的视角和投影矩阵
    private Camera myCamera;
    public Camera camera{
        get{
            if(myCamera == null)
                myCamera = GetComponent<Camera>();
            return myCamera;
        }
    }

    // 定义一个变量来保存上一帧摄像机的视角*投影矩阵
    private Matrix4x4 previousViewProjectionMatrix;

    void OnEnable(){
        // 需要获取相机的深度纹理
        camera.depthTextureMode |= DepthTextureMode.Depth;
    }

    void OnRenderImage(RenderTexture src, RenderTexture dest){
        if(material != null){
            material.SetFloat("_BlurSize", blurSize);

            // 设置上一帧的视角*投影矩阵
            material.SetMatrix("_PreviousViewProjectionMatrix", previousViewProjectionMatrix);
            // 设置当前帧的视角*投影矩阵的逆矩阵
            Matrix4x4 currentViewProjectionMatrix = camera.projectionMatrix * camera.worldToCameraMatrix;
            material.SetMatrix("_CurrentViewProjectionInverseMatrix", currentViewProjectionMatrix.inverse);
            previousViewProjectionMatrix = currentViewProjectionMatrix;

            Graphics.Blit(src, dest, material);
        }
        else{
            Graphics.Blit(src, dest);
        }
    }

}

```

新建 Shader 命名为 Chapter13-MotionBlurWithDepthTexture：

``` hlsl
Shader "Unity Shaders Book/Chapter 13/Motion Blur With Depth Texture"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _BlurSize ("Blur Size", Float) = 1.0
    }
    SubShader
    {
        CGINCLUDE
        
        #include "UnityCG.cginc"

        sampler2D _MainTex;
        // 主纹理的纹素大小，用于对深度纹理的采样坐标进行平台差异化处理
        half4 _MainTex_TexelSize;
        // Unity 传递的深度纹理
        sampler2D _CameraDepthTexture;
        // 脚本传递而来的矩阵
        float4x4 _CurrentViewProjectionInverseMatrix;
        float4x4 _PreviousViewProjectionMatrix;
        half _BlurSize;

        struct v2f
        {
            float4 vertex : SV_POSITION;
            half2 uv : TEXCOORD0;
            half2 uv_depth : TEXCOORD1;
        };

        v2f vert (appdata_img v)
        {
            v2f o;
            o.vertex = UnityObjectToClipPos(v.vertex);
            o.uv = v.texcoord.xy;
            o.uv_depth = v.texcoord.xy;

            // 处理平台差异导致的图像翻转问题，以便在类似 DirectX 的平台上开启了抗锯齿仍可得到正确的结果
            #if UNITY_UV_STARTS_AT_TOP
                if (_MainTex_TexelSize.y < 0)
                    o.uv_depth.y = 1 - o.uv_depth.y;
            #endif

            return o;
        }
        
        fixed4 frag (v2f i) : SV_Target
        {
            // 使用内置宏SAMPLE_DEPTH_TEXTURE和纹理坐标对深度纹理进行采样，得到深度值
            float d = SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv_depth);
            // 构建像素的NDC坐标H，需要把深度值d重新映射回NDC，用原映射的反函数 d * 2 - 1 即可
            float4 H = float4(i.uv.x * 2 - 1, i.uv.y * 2 - 1, d * 2 - 1, 1);
            // 通过视角*投影矩阵的逆矩阵，得到视角空间下的坐标
            float4 D = mul(_CurrentViewProjectionInverseMatrix, H);
            // 除以w分量，将其转为世界坐标
            float4 worldPos = D / D.w;

            // 当前帧视角空间下的坐标
            float4 currentPos = H;
            // 将世界坐标通过前一帧的视角*投影矩阵进行变换，得到前一帧在NDC下的坐标
            float4 previousPos = mul(_PreviousViewProjectionMatrix, worldPos);
            previousPos /= previousPos.w;

            // 计算当前帧和前一帧的屏幕空间下的位置差，得到该像素的速度
            float2 velocity = (currentPos.xy - previousPos.xy) / 2.0f;

            float2 uv = i.uv;
            float4 color = tex2D(_MainTex, uv);
            // 使用速度值对它的邻域像素进行采样，相加后取平均值得到模糊的效果。
            // _BlurSize 控制采样距离
            uv += velocity * _BlurSize;
            for(int it = 1; it < 3; it++, uv += velocity * _BlurSize)
            {
                float4 currentColor = tex2D(_MainTex, uv);
                color += currentColor;
            }
            
            color /= 3.0f;

            return color;
        }

        ENDCG

        Pass
        {
            // 屏幕后处理标配
            ZTest Always Cull Off ZWrite Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            ENDCG
        }
    }
    FallBack Off
}
```


## 13.3 全局雾效

**雾效（Fog）** 是游戏中常用的一种效果。

**Unity 内置的雾效** 可以产生基于距离的线性或指数雾效。
* 实现：在 Shader 中添加 `#pragma multi_compile_fog` 指令，并在顶点着色器输出结构体中声明 `UNITY_FOG_COORDS(x)`，在顶点着色器中使用 `UNITY_TRANSFER_FOG(o,o.vertex)` 指令来传输雾效坐标。在片元着色器中，使用 `UNITY_APPLY_FOG(i.fogCoord, color)` 指令来应用雾效。
* 缺点：不仅需要为场景中所有物体添加相关渲染代码，而且能够实现的效果非常有限。

**基于屏幕后处理的全局雾效** ：
基于屏幕后处理的全局雾效的关键是，根据深度纹理来重建每个像素在世界空间下的位置。
* 首先对图像空间下的视锥体射线（冲摄像机出发，指向图像上的某点的射线）进行插值，这条射线存储了该像素在世界空间下到摄像机的方向信息。
* 然后，我们把该射线和线性化后的视角空间下的深度值相乘，再加上摄像机的世界位置，就可以得到该像素在世界空间下的位置。
* 优点：自由度高。

### 13.3.1 重建世界坐标

我们只需要知道摄像机在世界空间下的位置，以及世界空间下的该像素相对于摄像机的偏移量，把它们相加就可以得到该像素的世界坐标：
``` hlsl
// _WorldSpaceCameraPos 是摄像机的世界坐标
// linearDepth * interpolatedRay 可以计算得到该像素相对于摄像机的偏移量
// lineardDepth 是由深度纹理得到的线性深度值
// interpolatedRay 是由顶点着色器输出并插值后得到的射线
float4 worldPos = _WorldSpaceCameraPos + lineardDepth * interpolatedRay;
```

interpolatedRay 求法：

interpolatedRay 来源于对近裁剪平面的4个角的某个特定向量的插值，这四个向量包含了它们到摄像机的方向和距离信息，我们可以利用摄像机的近裁剪平面距离、FOV、纵横比计算而得。

![图13.6 计算interpolatedRay](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.6.jpg)

为了方便计算，我们可以先计算 toTop 和 toRight 两个向量：

$$
halfHeight = Near * tan(\frac{FOV}{2})
$$

$$
toTop = camera.up * halfHeight
$$

$$
toRight = camera.right * halfHeight * aspect
$$

其中，Near 是近裁剪平面的距离，FOV 是竖直方向的视角范围，camera.up 和 camera.right 分别对应了摄像机的正上方和正右方。

得到这两个辅助向量后，我们就可以计算4个角相对于摄像机的方向了：

$$
TL = camera.forward * Near + toTop - toRight
$$

$$
TR = camera.forward * Near + toTop + toRight
$$

$$
BL = camera.forward * Near - toTop - toRight
$$

$$
BR = camera.forward * Near - toTop + toRight
$$

上面求得的4个向量不仅包含了方向信息，它们的模对应了4个点到摄像机的空间距离。由于我们得到的线性深度值并非是点到摄像机的欧式距离，而是在 z 方向上的距离，因此，我们不能直接使用深度值和4个角的单位方向的乘积来计算它们到摄像机的偏移量，如图13.7所示。

![图13.7 采样得到的深度值并非是点到摄像机的欧式距离](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.7.jpg)

想要把深度值转换成到摄像机的欧式距离也很简单，我们以 TL 点为例，根据相似三角形原理，TL 所在的射线上，像素的深度值和它到摄像机的实际距离的比等于近裁剪平面的距离和 TL 向量的模的比，即：

$$
\frac{depth}{dist} = \frac{Near}{|TL|}
$$

由此可得，我们需要的 TL 距离摄像机的欧式距离 dist：

$$
dist = \frac{|TL|}{Near} * depth
$$

由于4个点相互对称，因此其他3个向量的模和 TL 相等，即我们可以使用同一个因子和单位向量相乘，得到它们对应的向量值：

$$
scale = \frac{|TL|}{|Near|}
$$

$$
Ray_{TL} = \frac{TL}{|TL|} * scale, Ray_{TR} = \frac{TR}{|TR|} * scale, \\ 
Ray_{BL} = \frac{BL}{|BL|} * scale, Ray_{BR} = \frac{BR}{|BR|} * scale
$$

屏幕后处理的是使用特定的材质去渲染一个刚好填充整个屏幕的四边形面片。这个四边形面片的4个顶点就对应了近裁剪平面的4个角。因此，我们可以把上面的计算结果传递给顶点着色器，顶点着色器根据当前的位置选择它所对应的向量，然后再将其输出，经插值后传递给片元着色器得到 interpolatedRay，我们就可以直接利用本节一开始提到的公式重建该像素在世界空间下的位置了。


### 13.3.2 雾的计算

在简单的雾效实现中，我们需要计算一个雾效系数 f，作为混合原始颜色和雾的颜色的混合系数：
`float3 afterFog = f * fogColor + (1 - f) * origColor;`

这个雾效系数 f 有很多计算方法。在 Unity 内置的雾效实现中，支持三种雾的计算方式：
* 线性（Linear）： $f = \frac{d_{max} - \lvert z \rvert}{d_{max} - d_{min}}$ ，d_min 和 d_max 分别表示受雾影响的最小距离和最大距离。
* 指数（Exponential）： $f = e ^ {- d \cdot \lvert z \rvert}$ ，d 是控制雾的浓度的参数。
* 指数的平方（Exponential Squared）： $f = e ^ {- (d \cdot \lvert z \rvert) ^ 2}$ ，d 是控制雾的浓度的参数。

本节将使用类似线性雾的计算方式，计算基于高度的雾效。具体方法是，当给定一点在世界空间下的高度 y 后，f 的计算公式为：
$ f = \frac{H_{end} - y}{H_{end} - H_{start}}$ ，其中 $H_{start}$ 和 $H_{end}$ 分别表示受雾影响的起始高度和终止高度。


### 13.3.3 实现

新建脚本 FogWithDepthTexture.cs，继承 12.1 节中的基类：

``` csharp
using UnityEngine;

public class FogWithDepthTexture : PostEffectsBase
{
    public Shader fogShader;
    private Material fogMaterial;
    public Material material
    {
        get{
            fogMaterial = CheckShaderAndCreateMaterial(fogShader, fogMaterial);
            return fogMaterial;
        }
    }

    // 需要相机相关参数，如近裁剪平面的距离、FOV等，还有相机世界空间下的前方、上方、和右方等方向
    private Camera myCamera;
    public Camera camera
    {
        get{
            if(myCamera == null)
                myCamera = GetComponent<Camera>();
            return myCamera;
        }
    }

    private Transform myCameraTransform;
    public Transform cameraTransform
    {
        get{
            if(myCameraTransform == null)
                myCameraTransform = camera.transform;
            return myCameraTransform;
        }
    }

    // 控制雾的浓度
    [Range(0.0f, 3.0f)]
    public float fogDensity = 1.0f;
    // 控制雾的颜色
    public Color fogColor = Color.white;

    // 雾的起止高度
    public float fogStart = 0.0f;
    public float fogEnd = 2.0f;

    void OnEnable()
    {
        // 需要获取相机的深度纹理
        camera.depthTextureMode |= DepthTextureMode.Depth;
    }
    
    void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if(material != null)
        {
            // 存储近裁剪平面的四个角对应的向量
            Matrix4x4 frustumCorners = Matrix4x4.identity;

            // 获取需要的参数
            float fov = camera.fieldOfView;
            float near = camera.nearClipPlane;
            float far = camera.farClipPlane;
            float aspect = camera.aspect;

            // 用之前学的公式计算近裁剪平面的四个角对应的向量
            float halfHeight = near * Mathf.Tan(fov * 0.5f * Mathf.Deg2Rad);
            Vector3 toRight = cameraTransform.right * halfHeight * aspect;
            Vector3 toTop = cameraTransform.up * halfHeight;

            Vector3 topLeft = cameraTransform.forward * near + toTop - toRight;
            float scale = topLeft.magnitude / near;

            topLeft.Normalize();
            topLeft *= scale;

            Vector3 topRight = cameraTransform.forward * near + toTop + toRight;
            topRight.Normalize();
            topRight *= scale;

            Vector3 bottomRight = cameraTransform.forward * near - toTop + toRight;
            bottomRight.Normalize();
            bottomRight *= scale;

            Vector3 bottomLeft = cameraTransform.forward * near - toTop - toRight;
            bottomLeft.Normalize();
            bottomLeft *= scale;

            // 存储四个角对应的向量到矩阵中
            frustumCorners.SetRow(0, bottomLeft);
            frustumCorners.SetRow(1, bottomRight);
            frustumCorners.SetRow(2, topRight);
            frustumCorners.SetRow(3, topLeft);

            // 将参数传递给材质
            material.SetMatrix("_FrustumCornersRay", frustumCorners);
            material.SetMatrix("_ViewProjectionInverseMatrix", (camera.projectionMatrix * camera.worldToCameraMatrix).inverse);

            material.SetFloat("_FogDensity", fogDensity);
            material.SetColor("_FogColor", fogColor);
            material.SetFloat("_FogStart", fogStart);
            material.SetFloat("_FogEnd", fogEnd);

            // 把渲染结果显示到屏幕上
            Graphics.Blit(src, dest, material);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }
}
```

新建 Shader 命名为 Chapter13-FogWithDepthTexture

``` hlsl
Shader "Unity Shaders Book/Chapter 13/Fog With Depth Texture"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _FogDensity ("Fog Density", Range(0, 3)) = 1.0
        _FogColor ("Fog Color", Color) = (1, 1, 1, 1)
        _FogStart ("Fog Start", Float) = 0.0
        _FogEnd ("Fog End", Float) = 1.0
    }
    SubShader
    {
        CGINCLUDE

        #include "UnityCG.cginc"
        
        float4x4 _FrustumCornersRay;

        sampler2D _MainTex;
        half4 _MainTex_TexelSize;
        sampler2D _CameraDepthTexture;
        half _FogDensity;
        fixed4 _FogColor;
        float _FogStart;
        float _FogEnd;

        struct v2f{
            float4 vertex : SV_POSITION;
            half2 uv : TEXCOORD0;
            half2 uv_depth : TEXCOORD1;
            // 存储插值后的像素向量
            float4 interpolatedRay : TEXCOORD2;
        };

        v2f vert(appdata_img v)
        {
            v2f o;
            o.vertex = UnityObjectToClipPos(v.vertex);
            o.uv = v.texcoord;
            o.uv_depth = v.texcoord;

            // 对深度纹理采样坐标进行平台差异化处理
            #if UNITY_UV_STARTS_AT_TOP
                if (_MainTex_TexelSize.y < 0)
                    o.uv_depth.y = 1 - o.uv_depth.y;
            #endif
            
            // 一般使用if会造成比较大的性能问题，但本案例中用到的模型是一个四边形网格，只包含4个顶点，所以影响不大
            int index = 0;
            if (v.texcoord.x < 0.5 && v.texcoord.y < 0.5)
                index = 0;
            else if (v.texcoord.x > 0.5 && v.texcoord.y < 0.5)
                index = 1;
            else if (v.texcoord.x > 0.5 && v.texcoord.y > 0.5)
                index = 2;
            else
                index = 3;

            #if UNITY_UV_STARTS_AT_TOP
                if (_MainTex_TexelSize.y < 0)
                    index = 3 - index;
            #endif
            
            // 使用索引值来获取 _FrustumCornersRay 中对应的行作为该顶点的 interpolatedRay
            o.interpolatedRay = _FrustumCornersRay[index];

            return o;
        }

        fixed4 frag(v2f i) : SV_Target
        {
            // 首先使用 SAMPLE_DEPTH_TEXTURE 对深度纹理进行采样
            // 再使用 LinearEyeDepth 得到视角空间下的线性深度值
            float linearDepth = LinearEyeDepth(SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv_depth));
            float3 worldPos = _WorldSpaceCameraPos + linearDepth * i.interpolatedRay.xyz;

            // 计算当前像素高度 worldPos.y 对应的雾效系数
            float fogDensity = (_FogEnd - worldPos.y) / (_FogEnd - _FogStart);
            // 雾效系数与 _FogDensity 相乘后截取到 [0, 1] 范围内
            fogDensity = saturate(fogDensity * _FogDensity);

            fixed4 finalColor = tex2D(_MainTex, i.uv);
            finalColor.rgb = lerp(finalColor.rgb, _FogColor.rgb, fogDensity);

            return finalColor;
        }
        ENDCG

        Pass
        {
            ZTest Always Cull Off ZWrite Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            ENDCG
        }
    }
    FallBack Off
}
```


## 13.4 再谈边缘检测

在12章中学习了用 Sobel 算子对屏幕图像进行边缘检测，实现描边的效果，这种方法会产生很多我们不希望得到的边缘线（如：物体的纹理、阴影等位置也被描上黑边）。

本节将学习在深度和法线纹理上进行边缘检测，这些图像不会受纹理和光照的影响，而仅仅保存了当前渲染物体的模型信息，通过这样的方式检测出来的边缘更加可靠。

![图13.9 在深度和法线纹理上进行更健壮的边缘检测。左图：在原图上描边的效果。右图：只显示描边的效果](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.9.jpg)

本节将使用 Roberts 算子来进行边缘检测。
![图13.10 Roberts 算子](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/13.10.jpg)

Roberts 算子的本质就是计算左上角和右下角的差值，乘以右上角和左下角的差值，作为评估边缘的依据。

在下面的实现中，我们也会按这样的方式，取对角方向的深度或法线值，比较它们之间的差值，如果超过某个阈值(可由参数控制)，就认为它们之间存在一条边。

新建脚本 EdgeDetectNormalsAndDepth.cs，继承 12.1 节中的基类：

``` csharp
using UnityEngine;

public class EdgeDetectNormalsAndDepth : PostEffectsBase
{
    public Shader edgeDetectShader;
    private Material edgeDetectMaterial;
    public Material material
    {
        get
        {
            edgeDetectMaterial = CheckShaderAndCreateMaterial(edgeDetectShader, edgeDetectMaterial);
            return edgeDetectMaterial;
        }
    }

    [Range(0.0f, 1.0f)]
    public float edgeOnly = 0.0f;

    public Color edgeColor = Color.black;

    public Color backgroundColor = Color.white;

    // 采样距离，值越大描边越宽
    public float sampleDistance = 1.0f;

    // 影响当邻域的深度值/法线相差多少时，会被认为是边缘
    public float sensitivityDepth = 1.0f;
    public float sensitivityNormals = 1.0f;

    void OnEnable()
    {
        // 需要获取相机的深度和法线纹理
        GetComponent<Camera>().depthTextureMode |= DepthTextureMode.DepthNormals;
    }

    // [ImageEffectOpaque] 表示在不透明Pass执行完成后立即调用该函数，不对透明物体产生影响。
    [ImageEffectOpaque]
    void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (material!= null){
            material.SetFloat("_EdgeOnly", edgeOnly);
            material.SetColor("_EdgeColor", edgeColor);
            material.SetColor("_BackgroundColor", backgroundColor);
            material.SetFloat("_SampleDistance", sampleDistance);
            material.SetVector("_Sensitivity", new Vector4(sensitivityNormals,sensitivityDepth,0,0));

            Graphics.Blit(src, dest, material);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }
}

```

新建 Shader 命名为 Chapter13-EdgeDetectNormalsAndDepth ：

``` hlsl
Shader "Unity Shaders Book/Chapter 13/Edge Detect Normals And Depth"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _EdgeOnly ("Edge Only", Range(0,1)) = 0
        _EdgeColor ("Edge Color", Color) = (0,0,0,1)
        _BackgroundColor ("Background Color", Color) = (1,1,1,1)
        _SampleDistance ("Sample Distance", Float) = 1.0
        _Sensitivity ("Sensitivity", Vector) = (1,1,1,1)
    }
    SubShader
    {
        CGINCLUDE

        #include "UnityCG.cginc"

        sampler2D _MainTex;
        half4 _MainTex_TexelSize;
        fixed _EdgeOnly;
        fixed4 _EdgeColor;
        fixed4 _BackgroundColor;
        float _SampleDistance;
        half4 _Sensitivity;
        sampler2D _CameraDepthNormalsTexture;

        struct v2f
        {
            float4 pos : SV_POSITION;
            half2 uv[5] : TEXCOORD0;
        };

        v2f vert(appdata_img v)
        {
            v2f o;
            o.pos = UnityObjectToClipPos(v.vertex);

            half2 uv = v.texcoord;
            o.uv[0] = uv;

            #if UNITY_UV_STARTS_AT_TOP
            if (_MainTex_TexelSize.y < 0)
                uv.y = 1 - uv.y;
            #endif

            o.uv[1] = uv + _MainTex_TexelSize.xy * half2(1, 1) * _SampleDistance;
            o.uv[2] = uv + _MainTex_TexelSize.xy * half2(-1, -1) * _SampleDistance;
            o.uv[3] = uv + _MainTex_TexelSize.xy * half2(-1, 1) * _SampleDistance;
            o.uv[4] = uv + _MainTex_TexelSize.xy * half2(1, -1) * _SampleDistance;

            return o;
        }

        half CheckSame(half4 center, half4 sample)
        {
            // 对参数进行处理，得到两个采样点的法线和深度值。
            half2 centerNormal = center.xy;
            float centerDepth = DecodeFloatRG(center.zw);
            half2 sampleNormal = sample.xy;
            float sampleDepth = DecodeFloatRG(sample.zw);

            half2 diffNormal = abs(centerNormal - sampleNormal) * _Sensitivity.x;
            int isSameNormal = (diffNormal.x + diffNormal.y) < 0.1;

            float diffDepth = abs(centerDepth - sampleDepth) * _Sensitivity.y;
            int isSameDepth = diffDepth < 0.1 * centerDepth;

            return isSameNormal * isSameDepth ? 1.0 : 0.0;
        }

        half4 fragRobertsCrossDepthAndNormal(v2f i) : SV_Target
        {
            half4 sample1 = tex2D(_CameraDepthNormalsTexture, i.uv[1]);
            half4 sample2 = tex2D(_CameraDepthNormalsTexture, i.uv[2]);
            half4 sample3 = tex2D(_CameraDepthNormalsTexture, i.uv[3]);
            half4 sample4 = tex2D(_CameraDepthNormalsTexture, i.uv[4]);

            half edge = 1.0;

            edge *= CheckSame(sample1, sample2);
            edge *= CheckSame(sample3, sample4);

            fixed4 withEdgeColor = lerp(_EdgeColor, tex2D(_MainTex, i.uv[0]), edge);
            fixed4 onlyEdgeColor = lerp(_EdgeColor, _BackgroundColor, edge);

            return lerp(withEdgeColor, onlyEdgeColor, _EdgeOnly);
        }
        ENDCG

        Pass
        {
            ZTest Always Cull Off ZWrite Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment fragRobertsCrossDepthAndNormal
            ENDCG
        }
    }
    FallBack Off
}
```

如果想实现只针对特定物体进行表面的效果，Unity提供的Graphics.DrawMesh或Graphics.DrawMeshNow函数，把需要描边的物体再次渲染一次（在所有不透明物体渲染完毕之后），再使用本节提到的边缘检测算法计算深度或法线纹理中每个像素的梯度值，判断它们是否小于阈值。若是，则在Shader中使用Clip()函数将该像素剔除掉，从而显示出原来的颜色。


## 13.5 扩展阅读

- [Unity 官方手册 Image Effects](https://docs.unity3d.com/510/Documentation/Manual/comp-ImageEffects.html)



-------------------------------------------------------------------------------



# 第14章 非真实感渲染

**非真实感渲染（Non-PhotorealisticRendering，NPR）** 主要目标：使用一些渲染方法使得画面达到和某些特殊的绘画风格相似的效果，例如卡通、水彩风格等。


## 14.1 卡通风格的渲染

卡通风格的游戏画面通常有一些共有的特点，例如物体都被黑色线条描边，以及分明的明暗变化等。

**基于色调的着色技术（tone-based shading）** ：使用一张蛮烦色系数对一张一维纹理进行采样，以控制漫反射的色调。

<center>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/14.1.jpg" alt="图14.1 游戏《大神》（英文名：Okami）的游戏截图" width="420">
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/14.2.jpg" alt="图14.2 卡通风格的渲染效果" width="300">
</center>


### 14.1.1 渲染轮廓线

在实时渲染中，轮廓线渲染是应用非常广泛的一种效果。近20年来，有许多绘制模型轮廓线的方法被先后提出来。在《Real Time Rendering, third edition》一书中，作者把这些方法分成了5种类型：

* 基于观察角度和表面法线的轮廓线渲染。这种方法使用视角方向和表面法线的点乘结果来得到轮廓线的信息。
   * 优点：简单快速，可以在一个 Pass 中就得到渲染结果。
   * 缺点：局限性很大，很多模型渲染出来的描边效果都不尽如人意。

* 过程式几何轮廓线渲染。这种方法的核心是使用两个 Pass 渲染。第一个 Pass 渲染背面的面片，并使用某些技术让它的轮廓可见；第二个Pass再正常渲染正面的面片。
   * 优点：快速有效，并且适用于绝大多数表面平滑的模型。
   * 缺点：不适合类似于立方体这样平整的模型。

* 基于图像处理的轮廓线渲染。我们在第12、13章介绍的边缘检测的方法就属于这个类别。
   * 优点：可以适用于任何种类的模型。
   * 缺点：一些深度和法线变化很小的轮廓无法被检测出来，例如桌子上的纸张。

* 基于轮廓边检测的轮廓线渲染。上面提到的各种方法，一个最大的问题是，无法控制轮廓线的风格渲染。对于一些情况，我们希望可以渲染出独特风格的轮廓线，例如水墨风格等。为此，我们希望可以检测出精确的轮廓边，然后直接渲染它们。
   > 检测一条边是否是轮廓边：检查这条边与相邻的三角面片是否满足 $(n_0 \cdot v > 0) \neq (n_1 \cdot v > 0)$ , 其中， $n_0$ 和 $n_1$ 分别是相邻的两个面片的法向，$v$ 是从视角到该边上任意顶点的方向。本质在于检测两个相邻的三角面片是否是相反方向的。可以在几何着色器（Geometry Shader）的帮助下实现上面的检测过程。
   * 优点：可以控制轮廓线的风格渲染，可以检测出精确的轮廓边。
   * 缺点：实现相对复杂，会有动画连贯性的问题。

* 最后一个种类就是混合了上述的几种渲染方法。例如，首先找到精确的轮廓边，把模型和轮廓边渲染到纹理中，再使用图像处理的方法识别出轮廓线，并在图像空间下进行风格化渲染。

本节将会在Unity中使用过程式几何轮廓渲染的方法进行轮廓描边，在第一个Pass中使用轮廓线颜色渲染整个背面的面片，并在视角空间下把模型顶点沿着法线方向向外扩展一段距离，以此让背面轮廓线可见：
``` hlsl
viewPos = viewPos + viewNormal * _Outline;
```

但对于内凹的模型，可能会发生背面面片遮挡正面面片的情况。为了尽可能防止出现这样的情况，在扩张背面顶点之前，首先对顶点法线的 z 分量进行处理，使其等于一个定值，然后把法线归一化后再对顶点进行扩张，这样好处在于，扩展后的背面更加扁平化，从而降低了遮挡正面面片的可能性：
``` hlsl
viewNormal.z = -0.5;
viewNomal = normalize(viewNormal);
viewPos = viewPos + viewNormal * _Outline;
```


### 14.1.2 添加高光

卡通风格中的高光往往是模型上一块块分界明显的纯色区域。

回顾一下，之前实现 Blinn-Phong 模型过程中，我们使用法线点乘光照方向以及视角方向和的一半，再和另一个参数进行指数操作得到高光反射系数：
``` hlsl
float spec = paw(max(0, dot(normal, halfDir)), _Gloss);
```

对于卡通渲染需要的高光反射光照模型，我们同样需要计算 normal 和 halfDir 的点乘结果，但不同的是，我们把该值和一个阈值进行比较，如果小于该阈值，则高光反射系数为 0，否则返回 1。
``` hlsl
float spec = dot(worldNormal, worldHalfDir);
// step(参考值, 待比较的数值) 如果待比较的数值大于参考值，返回 1，否则返回 0。
spec = step(threshold, spec);
```

但是，这种方式会在高光区域边缘造成锯齿，因为高光区域边缘不是平滑渐变的，而是由 0 突变到 1。要想对其进行抗锯齿处理，我们可以在边界处很小的一块区域内进行平滑处理：
``` hlsl
float spec = dot(worldNormal, worldHalfDir);
// w 是一个很小的值，当 spec - threshold 小于 -w 时，返回 0，大于 w 时，返回 1，否则在 0-1 之间进行插值。
spec = lerp(0, 1, smoothstep(-w, w, spec - threshold));
```

尽管我们可以把 w 设为一个很小的定值，但在本例中，我们选择使用邻域像素之间的近似导数值，这可以通过 CG 的 fwidth 函数来得到。

![图14.3 左图：未对高光区域进行抗锯齿处理。右图：使用fwidth函数对高光区域进行抗锯齿处理](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/14.3.jpg)

可以在 [非真实渲染](https://blog.csdn.net/candycat1992/article/details/47284289) 中找到这种方法在 Unity 中的实现。

> fwidth(c) ==> abs(ddx(c)) + abs(ddy(c)) 
> 
> 当代 GPU 在像素化的时候一般是以2x2像素为基本单位，那么在这个2x2像素块当中，右侧的像素对应的 fragment 的 x 坐标减去左侧的像素对应的 fragment 的 x 坐标就是 ddx；下侧像素对应的 fragment 的坐标 y 减去上侧像素对应的 fragment 的坐标 y 就是 ddy，ddx 和 ddy 代表了相邻两个像素在设备坐标系当中的距离。
> 
> 摘自知乎：https://www.zhihu.com/question/329521044/answer/717906456

从上面的引用可看出，fwidth 就是临近像素级大小的宽度，本案例使用这个方法，就是为了让 w 被设置为一个足够小的值。

卡通渲染中高光往往有更多个性化的需要，更多非真实渲染的效果可前往原书作者的CSDN博客：https://blog.csdn.net/candycat1992/article/details/47284289。



### 14.1.3 实现

新建 Shader 命名为 Chapter14-ToonShading ：

``` hlsl
Shader "Unity Shaders Book/Chapter 14/Toon Shading"
{
    Properties
    {
        _Color ("Color", Color) = (1,1,1,1)
        _MainTex ("Texture", 2D) = "white" {}
        // 漫反射色调的渐变纹理
        _Ramp ("Ramp Texture", 2D) = "white" {}
        // 轮廓线宽度
        _Outline ("Outline Width", Range(0, 1)) = 0.03
        // 轮廓线颜色
        _OutlineColor ("Outline Color", Color) = (0,0,0,1)
        // 高光颜色
        _Specular ("Specular", Color) = (1,1,1,1)
        // 高光反射时使用的阈值
        _SpecularScale ("Specular Scale", Range(0, 0.1)) = 0.01
    }
    SubShader
    {
        // 定义渲染轮廓线需要的 Pass，只渲染背面
        Pass
        {
            // 定义 Pass 的名称，后面使用只需调用它的名字即可
            NAME "OUTLINE"

            Cull Front

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"

            float _Outline;
            fixed4 _OutlineColor;

            struct appdata
            {
                float4 vertex : POSITION;
                float3 normal : NORMAL;
                float2 uv : TEXCOORD0;
            };

            struct v2f
            {
                float4 vertex : SV_POSITION;
            };

            v2f vert (appdata v)
            {
                v2f o;

                // 把顶点和法线变换到视角空间下，为了让描边可以在观察空间达到最好的效果
                float4 pos = mul(UNITY_MATRIX_MV, v.vertex);
                float3 normal = mul((float3x3)UNITY_MATRIX_IT_MV, v.normal);
                // 设置法线2分量，对其归一化后再将顶点沿其方向扩张
                normal.z = -0.5;
                pos = pos + float4(normalize(normal), 0) * _Outline;
                // 把顶点从视角空间变换到裁剪空间
                o.vertex = mul(UNITY_MATRIX_P, pos);

                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                // 只需要用轮廓线颜色渲染整个背面即可
                return float4(_OutlineColor.rgb, 1);
            }
            ENDCG
        }

        // 定义光照模型所在的 Pass，渲染正面
        Pass
        {
            Tags{"LightMode" = "ForwardBase"}

            Cull Back

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #pragma multi_compile_fwdbase

            #include "UnityCG.cginc"
            #include "Lighting.cginc"
            #include "AutoLight.cginc"

            struct a2v
            {
                float4 vertex : POSITION;
                float3 normal : NORMAL;
                float2 uv : TEXCOORD0;
            };

            struct v2f
            {
                float4 pos : SV_POSITION;
                float3 worldNormal : TEXCOORD0;
                float3 worldPos : TEXCOORD1;
                float2 uv : TEXCOORD2;
                SHADOW_COORDS(3)
            };
            
            fixed4 _Color;
            sampler2D _MainTex;
            float4 _MainTex_ST;
            sampler2D _Ramp;
            float _Outline;
            fixed4 _OutlineColor;
            fixed4 _Specular;
            float _SpecularScale;

            v2f vert (a2v v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                o.worldNormal = mul(v.normal, (float3x3)unity_WorldToObject);
                o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz;
                o.uv = TRANSFORM_TEX(v.uv, _MainTex);

                TRANSFER_SHADOW(o);

                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                // 计算光照模型中需要的各个方向矢量
                fixed3 worldNormal = normalize(i.worldNormal);
                fixed3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos));
                fixed3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos));
                fixed3 worldHalfDir = normalize(worldLightDir + worldViewDir);

                fixed4 c = tex2D(_MainTex, i.uv);
                fixed3 albedo = c.rgb * _Color.rgb;

                fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo;

                // 计算当前世界坐标下的阴影值
                UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos);

                // 计算半兰伯特漫反射系数
                fixed diff = dot(worldNormal, worldLightDir);
                diff = (diff * 0.5 + 0.5) * atten;

                // 使用漫反射系数对渐变纹理_Ramp进行采样，并将结果和材质的反射率、光照颜色相乘，得到漫反射颜色
                fixed3 diffuse = _LightColor0.rgb * albedo * tex2D(_Ramp, float2(diff, 0)).rgb;

                fixed spec = dot(worldNormal, worldHalfDir);
                // 使用 fwidth 对高光区域边界进行抗锯齿处理
                fixed w = fwidth(spec) * 2.0;
                // 将计算而得的高光反射系数和高光反射颜色相乘，得到高光反射的光照部分
                // step(0.0001, _SpecularScale) 是为了在 _SpecularScale 为 0 时，可以完全消除高光反射的光照
                fixed3 specular = _Specular.rgb * lerp(0, 1, smoothstep(-w, w, spec + _SpecularScale - 1)) * step(0.0001, _SpecularScale);

                return fixed4(ambient + diffuse + specular, 1);

            }
            ENDCG
        }
    }
    FallBack "Diffuse"
}
```


## 14.2 素描风格的渲染

微软研究院的 Praun 等人在 2001 年的 SIGGRAPH 上发表了一篇非常著名的论文，这篇文章中，他们使用了提前生成的素描纹理来实现实时的素描风格渲染，这些纹理组成了 **色调艺术映射（Tonal Art Map，TAM）**。

如图 14.4 所示，从左到右纹理中的笔触逐渐增多，用于模拟不同光照下的漫反射效果，从上到下则对应了每张纹理的多级渐远纹理（mipmaps）。这些多级渐远纹理的生成并不是简单的对上一层纹理进行降采样，而是需要保持笔触之间的间隔，以便更真实地模拟素描效果。

![图14.4 一个TAM的例子（来源：Praun E, et al. Real-time hatching4）](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/14.4.jpg)

本节将会实现简化版的论文中提出的算法，我们不考虑多级渐远纹理纹理的生成，而是直接使用6张素描纹理进行渲染。

在渲染阶段，我们首先在顶点着色阶段计算逐顶点的光照，根据光照结果来决定6张纹理的混合权重，并传递给片元着色器。然后，在片元着色器中根据这些权重来混合6张纹理的采样结果。

![图14.5 素描风格的渲染效果](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/14.5.jpg)

新建 Shader 命名为 Chapter14-Hatching ：

``` hlsl
Shader "Unlit/Chapter14-Hatching"
{
    Properties
    {
        _Color ("Color", Color) = (1,1,1,1)
        // 纹理平铺系数，值越大，模型上的素描线条越密
        _TileFactor ("Tile Factor", Float) = 8
        _Outline ("Outline", Range(0,1)) = 0.01
        // 6张素描纹理，线条密度依次增大
        _Hatch0 ("Hatch 0", 2D) = "white" {}
        _Hatch1 ("Hatch 1", 2D) = "white" {}
        _Hatch2 ("Hatch 2", 2D) = "white" {}
        _Hatch3 ("Hatch 3", 2D) = "white" {}
        _Hatch4 ("Hatch 4", 2D) = "white" {}
        _Hatch5 ("Hatch 5", 2D) = "white" {}
    }
    SubShader
    {
        Tags { "RenderType"="Opaque" "Queue"="Geometry" }
        
        UsePass "Unity Shaders Book/Chapter 14/Toon Shading/OUTLINE"

        Pass
        {
            Tags { "LightMode"="ForwardBase" }

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #pragma multi_compile_fwdbase

            #include "UnityCG.cginc"
            #include "AutoLight.cginc"
            #include "Lighting.cginc"

            fixed4 _Color;
            float _TileFactor;
            sampler2D _Hatch0;
            sampler2D _Hatch1;
            sampler2D _Hatch2;
            sampler2D _Hatch3;
            sampler2D _Hatch4;
            sampler2D _Hatch5;

            struct a2v
            {
                float4 vertex : POSITION;
                float3 normal : NORMAL;
                float4 texcoord : TEXCOORD0;
            };
            
            struct v2f
            {
                float4 pos : SV_POSITION;
                float2 uv : TEXCOORD0;
                // 六个混合权重存储在两个fixed3中
                fixed3 hatchWeights0 : TEXCOORD1;
                fixed3 hatchWeights1 : TEXCOORD2;
                float3 worldPos : TEXCOORD3;
                SHADOW_COORDS(4)
            };

            v2f vert (a2v v)
            {
                v2f o;

                o.pos = UnityObjectToClipPos(v.vertex);

                // 使用 _TileFactor 得到纹理采样坐标
                o.uv = v.texcoord.xy * _TileFactor;

                // 计算六张纹理的混合权重之前，先计算光照
                fixed3 worldLightDir = normalize(WorldSpaceLightDir(v.vertex));
                fixed3 worldNormal = UnityObjectToWorldNormal(v.normal);
                fixed diff = max(0, dot(worldLightDir, worldNormal));

                // 将权重初始化为0
                o.hatchWeights0 = fixed3(0,0,0);
                o.hatchWeights1 = fixed3(0,0,0);

                // 将 diff 缩放到[0, 7]范围
                float hatchFactor = diff * 7.0;

                // 根据 hatchFactor 计算六张纹理的混合权重
                if (hatchFactor > 6.0){
                    // 白色，不做处理
                } else if (hatchFactor > 5.0){
                    o.hatchWeights0.x = hatchFactor - 5.0;
                } else if (hatchFactor > 4.0){
                    o.hatchWeights0.x = hatchFactor - 4.0;
                    o.hatchWeights0.y = 1.0 - o.hatchWeights0.x;
                } else if (hatchFactor > 3.0){
                    o.hatchWeights0.y = hatchFactor - 3.0;
                    o.hatchWeights0.z = 1.0 - o.hatchWeights0.y;
                } else if (hatchFactor > 2.0){
                    o.hatchWeights0.z = hatchFactor - 2.0;
                    o.hatchWeights1.x = 1.0 - o.hatchWeights0.z;
                } else if (hatchFactor > 1.0){
                    o.hatchWeights1.x = hatchFactor - 1.0;
                    o.hatchWeights1.y = 1.0 - o.hatchWeights1.x;
                } else {
                    o.hatchWeights1.y = hatchFactor;
                    o.hatchWeights1.z = 1.0 - o.hatchWeights1.y;
                }

                o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz;

                TRANSFER_SHADOW(o)

                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                // 对每张纹理进行采样并和它们对应的权重值相乘得到每张纹理的采样颜色
                fixed4 hatchTex0 = tex2D(_Hatch0, i.uv) * i.hatchWeights0.x;
                fixed4 hatchTex1 = tex2D(_Hatch1, i.uv) * i.hatchWeights0.y;
                fixed4 hatchTex2 = tex2D(_Hatch2, i.uv) * i.hatchWeights0.z;
                fixed4 hatchTex3 = tex2D(_Hatch3, i.uv) * i.hatchWeights1.x;
                fixed4 hatchTex4 = tex2D(_Hatch4, i.uv) * i.hatchWeights1.y;
                fixed4 hatchTex5 = tex2D(_Hatch5, i.uv) * i.hatchWeights1.z;

                // 光照最亮的部分是纯白色
                fixed4 whiteColor = fixed4(1,1,1,1) * (1 - i.hatchWeights0.x - i.hatchWeights0.y - i.hatchWeights0.z 
                                                    - i.hatchWeights1.x - i.hatchWeights1.y - i.hatchWeights1.z);

                fixed4 hatchColor = hatchTex0 + hatchTex1 + hatchTex2 + hatchTex3 + hatchTex4 + hatchTex5 + whiteColor;

                UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos)

                return fixed4(hatchColor.rgb * _Color.rgb * atten, 1);
            }

            ENDCG
        }
    }
    FallBack "Diffuse"
}
```


## 14.3 扩展阅读

非真实感渲染相关的扩展阅读材料：

* 国际讨论会NPAR（Non-Photorealistic Animation And Rendering）上的非真实感渲染的论文
* 浙江大学的耿卫东教授编纂的书籍《艺术化绘制的图形学原理与方法》（The Algorithms and Principles of Non-photorealistic Grahpics）
* [Unity Assets Toon Shader Free](https://assetstore.unity.com/packages/vfx/shaders/toon-shader-free-21288)



------------------------------------------------------------------------------



# 第15章 使用噪声

## 15.1 消融效果

消融（dissolve）效果常见于游戏中的角色死亡，地图烧毁等效果。这些效果中，消融往往从不同的区域开始，并向看似随机的方向扩张，最后整个物体都将消失不见。

![图15.1  箱子的消融效果](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/15.1.jpg)

实现方式：噪声纹理+透明度测试。我们使用对噪声纹理采样的结果和某个控制消融程度的阈值比较，如果小于阈值，就使用 clip 函数把它对应的像素裁剪掉，这些部分就对应了图中被“烧毁”的区域。而镂空区域边缘的烧焦效果则是将两种颜色混合，再用 pow 函数处理后，与原纹理颜色混合后的结果。

新建 Shader 命名为 Chapter15-Dissolve ：

``` hlsl
Shader "Unity Shaders Book/Chapter 15/Dissolve"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _BumpMap ("Normal Map", 2D) = "bump" {}
        // 控制消融程度，0为正常效果，1为完全消融
        _BurnAmount ("Burn Amount", Range(0.0, 1.0)) = 0.5
        // 控制模拟烧焦效果时的线宽
        _LineWidth ("Line Width", Range(0.0, 0.2)) = 0.01
        // 控制烧焦效果边缘的两种颜色
        _BurnFirstColor ("Burn First Color", Color) = (1, 0, 0, 1)
        _BurnSecondColor ("Burn Second Color", Color) = (1, 1, 0, 1)
        // 噪声纹理
        _BurnMap ("Burn Map", 2D) = "white" {}
    }
    SubShader
    {
        Pass
        {
            Tags {"LightMode"="ForwardBase"}

            Cull Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #pragma multi_compile_fwdbase

            #include "UnityCG.cginc"
            #include "Lighting.cginc"
            #include "AutoLight.cginc"

            sampler2D _MainTex;
            float4 _MainTex_ST;
            sampler2D _BumpMap;
            float4 _BumpMap_ST;

            fixed _BurnAmount;
            float _LineWidth;
            float4 _BurnFirstColor;
            float4 _BurnSecondColor;

            sampler2D _BurnMap;
            float4 _BurnMap_ST;


            struct appdata
            {
                float4 vertex : POSITION;
                float2 texcoord : TEXCOORD0;
                float3 normal : NORMAL;
                float4 tangent : TANGENT;
            };

            struct v2f
            {
                float4 pos : SV_POSITION;
                float4 uv : TEXCOORD0;
                float2 uvBurnMap : TEXCOORD1;
                float3 lightDir : TEXCOORD2;
                float3 worldPos : TEXCOORD3;
                SHADOW_COORDS(4)
            };

            v2f vert (appdata v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                
                o.uv.xy = TRANSFORM_TEX(v.texcoord, _MainTex);
                o.uv.zw = TRANSFORM_TEX(v.texcoord, _BumpMap);
                o.uvBurnMap = TRANSFORM_TEX(v.texcoord, _BurnMap);

                TANGENT_SPACE_ROTATION;
                // 把光源方向从模型空间变换到切线空间
                o.lightDir = mul(rotation, ObjSpaceLightDir(v.vertex)).xyz;

                o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz;

                TRANSFER_SHADOW(o);

                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                // 对噪声纹理进行采样
                fixed3 burn = tex2D(_BurnMap, i.uvBurnMap).rgb;

                // 将采样结果与消融程度相减传递给clip函数，当结果小于0时，该像素会被剔除
                clip(burn.r - _BurnAmount);

                // 如果通过测试，则进行正常的光照计算
                float3 tangentLightDir = normalize(i.lightDir);
                float3 tangentNormal = UnpackNormal(tex2D(_BumpMap, i.uv.zw));

                fixed3 albedo = tex2D(_MainTex, i.uv.xy).rgb;
                fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo;
                fixed3 diffuse = _LightColor0.rgb * albedo * max(0, dot(tangentNormal, tangentLightDir));

                // 计算烧焦颜色
                // 使用 smoothstep 计算混合系数t，t=1时，表明该像素位于消融边界，t=0时，表明该像素为模型正常颜色
                fixed t = 1 - smoothstep(0.0, _LineWidth, burn.r - _BurnAmount);
                // 用t对两种火焰颜色进行混合
                fixed3 burnColor = lerp(_BurnFirstColor.rgb, _BurnSecondColor.rgb, t);
                // 为了让效果更接近烧焦颜色，我们使用pow对结果进行处理
                burnColor = pow(burnColor, 5);

                UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos);

                // 用t混合正常的光照颜色和烧焦颜色，使用step函数保证当_BurnAmount=0时，不显示任何消融效果
                fixed3 finalColor = lerp(ambient + diffuse * atten, burnColor, t * step(0.0001, _BurnAmount));

                return fixed4(finalColor, 1);
            }
            ENDCG
        }

        // 为了让阴影可以配合透明度测试产生正确的效果，我们需要自定义一个投射阴影的 Pass
        Pass
        {
            Tags {"LightMode"="ShadowCaster"}

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #pragma multi_compile_shadowcaster
            
            #include "UnityCG.cginc"
            #include "AutoLight.cginc"

            sampler2D _BurnMap;
            float4 _BurnMap_ST;
            float _BurnAmount;

            struct v2f
            {
                V2F_SHADOW_CASTER;
                float2 uvBurnMap : TEXCOORD1;
            };


            v2f vert(appdata_base v)
            {
                v2f o;

                TRANSFER_SHADOW_CASTER_NORMALOFFSET(o)

                o.uvBurnMap = TRANSFORM_TEX(v.texcoord, _BurnMap);

                return o;
            }

            fixed4 frag(v2f i) : SV_Target
            {
                fixed3 burn = tex2D(_BurnMap, i.uvBurnMap).rgb;
                clip(burn.r - _BurnAmount);
                SHADOW_CASTER_FRAGMENT(i)
            }

            ENDCG
        }
    }
    Fallback "Diffuse"
}
```


## 15.2 水波效果

在模拟实时水面的过程中，我们往往也会使用噪声纹理。此时，噪声纹理通常会用作一个高度图，以不断修改水面的法线方向。为了模拟水不断流动的效果，我们会使用和时间相关的变量来对噪声纹理进行采样，当得到法线信息后，再进行正常的反射+折射计算，得到最后的水面波动效果。

在本节中，我们将会使用一个由噪声纹理得到的法线贴图，实现一个包含菲涅耳反射的水面效果，如图15.3所示。

![图15.3 包含菲涅耳反射的水面波动效果。在左图中，视角方向和水面法线的夹角越大，反射效果越强。在右图中，视角方向和水面法线的夹角越大，折射效果越强](https://raw.githubusercontent.com/Ineloquent0/notes/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/images/15.3.jpg)

* 我们使用一张立方体纹理（Cubemap）作为环境纹理，模拟反射。
* 为了模拟折射效果，我们使用 GrabPass来获取当前屏幕的渲染纹理，并使用切线空间下的法线方向对像素的屏幕坐标进行偏移，再使用该坐标对渲染纹理进行屏幕采样，从而模拟近似的折射效果。
* 水波的法线纹理是由一张噪声纹理生成而得，而且会随着时间变化不断平移，模拟波光粼粼的效果。
* 使用菲涅耳系数来混合反射和折射颜色： $fresnel = pow(1 - max(0, \vec v \cdot \vec n), 4)$ ，其中 $\vec v$ 是视线方向，$\vec n$ 是法线方向。它们之间的夹角越小，fresnel 值越小，反射越弱，折射越强。

准备工作：
1. 构建一个测试水波效果的场景。
2. 为了得到本场景适用的环境纹理，我们需要新建一个 Cubemap，然后使用 10.1.2 节中实现的创建立方体纹理的脚本（通过 GameObject - Render into Cubemap 打开编辑窗口）来创建它。
3. 新建 Shader 命名为 Chapter15-WaterWave。

``` hlsl
Shader "Unity Shaders Book/Chapter 15/Water Wave"
{
    Properties
    {
        // 水面颜色
        _Color ("Color", Color) = (1,1,1,1)
        // 水面波纹材质纹理
        _MainTex ("Texture", 2D) = "white" {}
        // 由噪声纹理生成的法线纹理
        _WaveMap ("Wave Map", 2D) = "bump" {}
        // 模拟反射的立方体纹理
        _Cubemap ("Cubemap", Cube) = "_Skybox" {}
        // 法线纹理在X/Y方向上的平移速度
        _WaveXSpeed ("Wave X Speed", Range(-0.1, 0.1)) = 0.01
        _WaveYSpeed ("Wave Y Speed", Range(-0.1, 0.1)) = 0.01
        _Distortion ("Distortion", Range(0, 200)) = 10
    }
    SubShader
    {
        // 确保水面在所有不透明物体之后渲染
        Tags { "Queue"="Transparent" "RenderType"="Opaque" }

        // 使用 GrabPass 获取屏幕图像
        GrabPass { "_RefractionTex" }

        Pass
        {
			Tags { "LightMode"="ForwardBase" }
			
			CGPROGRAM
			
			#include "UnityCG.cginc"
			#include "Lighting.cginc"
			
			#pragma multi_compile_fwdbase
			
			#pragma vertex vert
			#pragma fragment frag

            fixed4 _Color;
            sampler2D _MainTex;
            float4 _MainTex_ST;
            sampler2D _WaveMap;
            float4 _WaveMap_ST;
            samplerCUBE _Cubemap;
            fixed _WaveXSpeed;
            fixed _WaveYSpeed;
            float _Distortion;
            // GrabPass 指定的纹理
            sampler2D _RefractionTex;
            // 得到该纹理的纹素大小，做屏幕图像采样偏移时会用到这个值
            float4 _RefractionTex_TexelSize;

            struct appdata
            {
                float4 vertex : POSITION;
                float4 uv : TEXCOORD0;
                float3 normal : NORMAL;
                float4 tangent : TANGENT;
            };

            struct v2f
            {
                float4 pos : SV_POSITION;
                // 当前片元对应屏幕图像中的位置
                float4 scrPos : TEXCOORD0;
                // uv是4维向量，xy存储水面波纹纹理的uv值，zw存储噪声法线纹理的uv值
                float4 uv : TEXCOORD1;
                // 使用3个4维向量存储切线空间转世界空间的3X3的转换矩阵
                // 为了充分利用存储变量，用它们的w分量存储当前片元在世界空间中的位置
                float4 TtoW0 : TEXCOORD2;
                float4 TtoW1 : TEXCOORD3;
                float4 TtoW2 : TEXCOORD4;
            };


            v2f vert (appdata v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);

                // 使用内置的ComputeGrabScreenPos函数，得到当前顶点对应屏幕空间的坐标
                o.scrPos = ComputeGrabScreenPos(o.pos);

                o.uv.xy = TRANSFORM_TEX(v.uv, _MainTex);
                o.uv.zw = TRANSFORM_TEX(v.uv, _WaveMap);

                float3 worldPos = mul(unity_ObjectToWorld, v.vertex).xyz;
                fixed3 worldNormal = UnityObjectToWorldNormal(v.normal);
                fixed3 worldTangent = UnityObjectToWorldDir(v.tangent.xyz);
                fixed3 worldBinormal = cross(worldNormal, worldTangent) * v.tangent.w;

                o.TtoW0 = float4(worldTangent.x, worldBinormal.x, worldNormal.x, worldPos.x);
                o.TtoW1 = float4(worldTangent.y, worldBinormal.y, worldNormal.y, worldPos.y);
                o.TtoW2 = float4(worldTangent.z, worldBinormal.z, worldNormal.z, worldPos.z);

                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                // 获取世界坐标
                float3 worldPos = float3(i.TtoW0.w, i.TtoW1.w, i.TtoW2.w);
                // 利用世界坐标得到该片元对应的视角方向
                fixed3 viewDir = normalize(UnityWorldSpaceViewDir(worldPos));
                // 计算法线贴图的偏移
                float2 speed = _Time.y * float2(_WaveXSpeed, _WaveYSpeed);

                // 为了模拟两层交叉的水面波动的效果，对法线纹理进行两次采样
                fixed3 bump1 = UnpackNormal(tex2D(_WaveMap, i.uv.zw + speed)).rgb;
                fixed3 bump2 = UnpackNormal(tex2D(_WaveMap, i.uv.zw - speed)).rgb;
                // 两次结果相加并归一化后得到切线空间下的法线方向
                fixed3 bump = normalize(bump1 + bump2);

                // 屏幕图像采样坐标偏移，模拟折射效果，_Distortion 值越大，偏移量越大
                float2 offset = bump.xy * _Distortion * _RefractionTex_TexelSize.xy;
                i.scrPos.xy = offset * i.scrPos.z + i.scrPos.xy;
                fixed3 refrColor = tex2D(_RefractionTex, i.scrPos.xy / i.scrPos.w).rgb;

                // 把法线方向从切线空间变换到了世界空间下
                bump = normalize(half3(dot(i.TtoW0.xyz, bump), dot(i.TtoW1.xyz, bump), dot(i.TtoW2.xyz, bump)));

                fixed4 texColor = tex2D(_MainTex, i.uv.xy + speed);
                // 视角方向相对于法线方向的反射方向
                fixed3 reflDir = reflect(-viewDir, bump);
                fixed3 reflColor = texCUBE(_Cubemap, reflDir).rgb * texColor.rgb * _Color.rgb;

                // 菲涅尔系数，用来混合反射和折射的颜色
                fixed fresnel = pow(1 - saturate(dot(viewDir, bump)), 4);
                fixed3 finalColor = reflColor * fresnel + refrColor * (1 - fresnel);

                return fixed4(finalColor, 1);
            }
            ENDCG
        }
    }
    FallBack Off
}
```


## 15.3 再谈全局雾效

我们在 13.3 节中使用了深度纹理来实现基于屏幕后处理的全局雾效。我们由深度纹理重建每个像素在世界空间下的位置，再使用一个基于高度的公式来计算雾效的混合系数，最后使用该系数来混合雾的颜色和原屏幕的颜色。13.3 节中实现效果是一个基于高度的均匀雾效，即在同一个高度上，雾的浓度是相同的。本节中我们使用噪声纹理来模拟一种不均匀的雾效，同时让雾不断飘动，使雾看起来更加飘渺。

绝大多数代码和 13.3 节中的完全一样，我们只是添加了噪声相关的参数和属性，并在 Shader 的片元着色器中对高度的计算添加了噪声的影响。

新建脚本 FogWithNoise.cs ，把该脚本拖拽到摄像机上

``` csharp
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class FogWithNoise : PostEffectsBase
{
    // ...... 与 13.3 节中的参数相同

    // 噪声纹理
    public Texture noiseTexture;

    // 噪声纹理移动速度
    [Range(-0.5f, 0.5f)]
    public float fogXSpeed = 0.1f;
    [Range(-0.5f, 0.5f)]
    public float fogYSpeed = 0.1f;

    // 噪声程度，值为0时，表示不应用任何噪声
    [Range(0.0f, 3.0f)]
    public float noiseAmount = 1.0f;

    void OnEnable()
    {
        // 需要获取相机的深度纹理
        camera.depthTextureMode |= DepthTextureMode.Depth;
    }
    
    void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if(material != null)
        {
            // 存储近裁剪平面的四个角对应的向量
            // 与 13.3 节中的相同
            // ......

            // 将参数传递给材质
            material.SetTexture("_NoiseTex", noiseTexture);
            material.SetFloat("_FogetXSpeed", fogXSpeed);
            material.SetFloat("_FogYSpeed", fogYSpeed);
            material.SetFloat("_NoiseAmount", noiseAmount);

            // 把渲染结果显示到屏幕上
            Graphics.Blit(src, dest, material);
        }
        else
        {
            Graphics.Blit(src, dest);
        }
    }
}
```

新建 Shader 命名为 Chapter15-FogWithNoise 复制 13.3 节中的 Shader 代码，并做如下修改：

``` hlsl
Shader "Unity Shaders Book/Chapter 15/Fog With Noise"
{
    Properties
    {
        ......

        _NoiseTex ("Noise Texture", 2D) = "white" {}
        _FogXSpeed ("Fog X Speed", Float) = 0.1
        _FogYSpeed ("Fog Y Speed", Float) = 0.1
        _NoiseAmount ("Noise Amount", Range(0, 1)) = 1
    }
    SubShader
    {
        CGINCLUDE

        #include "UnityCG.cginc"
        
        ......

        sampler2D _NoiseTex;
        half _FogXSpeed;
        half _FogYSpeed;
        half _NoiseAmount;

        // 顶点着色器与 13.3 节中的相同
        // ......


        fixed4 frag(v2f i) : SV_Target
        {
            // 首先使用 SAMPLE_DEPTH_TEXTURE 对深度纹理进行采样
            // 再使用 LinearEyeDepth 得到视角空间下的线性深度值
            float linearDepth = LinearEyeDepth(SAMPLE_DEPTH_TEXTURE(_CameraDepthTexture, i.uv_depth));
            float3 worldPos = _WorldSpaceCameraPos + linearDepth * i.interpolatedRay.xyz;

            // 计算噪声纹理偏移量
            float2 speed = _Time.y * float2(_FogXSpeed, _FogYSpeed);
            float noise = (tex2D(_NoiseTex, i.uv + speed).r - 0.5) * _NoiseAmount;

            // 计算当前像素高度 worldPos.y 对应的雾效系数
            float fogDensity = (_FogEnd - worldPos.y) / (_FogEnd - _FogStart);
            // 雾效系数与 _FogDensity 相乘后截取到 [0, 1] 范围内
            fogDensity = saturate(fogDensity * _FogDensity * (1 + noise));

            fixed4 finalColor = tex2D(_MainTex, i.uv);
            finalColor.rgb = lerp(finalColor.rgb, _FogColor.rgb, fogDensity);

            return finalColor;
        }
        ENDCG

        Pass
        {
            ZTest Always Cull Off ZWrite Off

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            ENDCG
        }
    }
    FallBack Off
}

```


## 15.4 扩展阅读

噪声纹理可以认为是一种程序纹理（Procedure Texture），它们都是由计算机利用某些算法生成的。Perlin 噪声和 Worley 噪声是两种最常使用的噪声类型。Perlin 噪声可以用于更自然的噪声纹理，Worley 噪声则通常用于模拟诸如石头、水、纸张等多孔噪声。

参考文档：
* [Perlin 噪声](https://adrianb.io/2014/08/09/perlinnoise.html)
* [程序噪声在Unity中的实现](https://github.com/Scrawk/Procedural-Noise)



--------------------------------------------------------------------------



# 第16章 Unity 中的渲染优化技术

感觉内容较多又比较重要，所以单独写了一篇笔记：[04 高级篇 - 渲染优化](https://github.com/Ineloquent0/notes/blob/main/Shader/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/04%20%E9%AB%98%E7%BA%A7%E7%AF%87%20-%20%E6%B8%B2%E6%9F%93%E4%BC%98%E5%8C%96.md)