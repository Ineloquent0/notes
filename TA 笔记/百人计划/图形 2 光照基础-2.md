# 4. 传统经验光照模型

## 4.1 光照模型 illumination model
是一种模拟自然界光照的物理过程的一种计算机模型，即光线与空间中物体表面的交互模型，大致分为两类：
1. 基于物理的光照模型（PBR）（有可依据的公式）
2. 经验模型（进行了一些近似、模拟，所以会不准）

#### 为什么需要光照模型
现实世界光照复杂，使用简化的光照模型对现实世界的光照情况进行模拟

#### 光照模型的发展

![光照模型的发展](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240911205159.jpg)


## 4.2 局部光照模型

### 局部光照的定义
* 只考虑光源的影响，不考虑光线多次反射
* （对应的概念：全局光照=直接光照+间接光照）

![局部和全局光照模型](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240911211456.jpg)

局部光照模型满足叠加原理，可以基本将光线分为四个部分

![局部光照模型](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240911214055.jpg)

### 局部光照模型的组成

局部光照模型满足叠加原理，可以基本将光线分为四个部分：
* 漫反射
* 高光反射
* 环境光
* 自发光

#### 漫反射
* 光线均匀被反射到各个方向
* 漫反射过程中，光线发生了吸收和散射，因此改变颜色和方向
* 漫反射只与光源和表面法线有关

使用 Lambert 余弦定理计算：

![Lambert 余弦定理](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240911212028.jpg)

#### 高光反射
* 描述了光线与物体表面发生的反射（光强不变，方向改变）
* 反射率根据菲涅尔效应决定
* Gloss 影响了高光反射的范围，但高光的亮度是不变的
* 高光反射与观察方向有关

![高光反射](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240911212930.jpg)

#### 环境光

![环境光](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240911213644.jpg)

#### 自发光：物体自身发出的光
物体自身发射的光线，通常作为单独的一项加入光照模型一般使用一张发光贴图描述物体的自发光


## 4.3 经典光照模型

### Lambert 模型
只计算漫反射，没有高光效果
`color = C_light * albedo * dot(normal, light);`

### Phong 模型
`Phong = Ambient + Diffuse + Specular`

使用环境光来模拟间接光照
加上了高光反射（使用反射光线方向和视线方向进行计算）

![Phong 模型](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240911215227.jpg)

### Blinn-Phong 模型
在计算高光反射时，用的是半角向量和法线向量

![Blinn-Phong 模型](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240911215524.jpg)

### Phong 和 Blinn-Phone 模型的区别
Blinn-Phong 模型使用了半角向量与法线的点积代替了反射向量与视线的点积结果。

半角向量的使用带来的变化：
1. 计算更简洁（计算半角向量比计算反射向量更简洁）
2. 半角向量与法线的角度永远不会大于90度（但是反射光线与视线方向的角度会大于90度，导致点乘计算的值钳制在0，出现明显的边缘效果）

![Phong 和 Blinn-Phone 模型的区别](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240911221342.jpg)

### Gourand 模型

* 逐顶点计算
* 镜面高光效果差

![Gourand 模型](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240911221801.jpg)

### Flat 模型

![Flat 模型](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240911222040.jpg)

### 光照模型展示
总结：
* Lambert 模型只考虑表面的漫反射部分
* Phong 模型能够较好的呈现镜面高光的效果，也是四种模型中最接近真实的效果，需要计算较复杂的反射向量
* Blinn-Phong 模型效果与 Phong 模型相近，更偏向艺术性的效果，使用方便计算的半程向量代替较为复杂的反射向量，计算量小于Phong，是效果和效率的最佳选择，也是大多数情况下的默认光照模型
* Gourand 模型计算顶点光照并通过增量法插值计算多边形内部的光强，当顶点密度低时，表现效果很差，对于高光的效果也不尽如意，计算量较小

![光照模型展示](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240911223020.jpg)


-----------------------------------------------------------------


# 5. Gamma 校正

## 5.1 Gamma 校正

### 颜色空间

* 通用：sRGB
* 电影：DCI-P3
* 电视：Rec-709、PAL等
* 印刷：CMYK、Adobe RGB
* 其他

![颜色空间](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/56d2fbac-7cec-48d2-84d4-7bb82e1081ba.jpg)

sRGB 和 Rec-709 能表达的颜色空间范围是差不多的，他的三原色位置是差不多的。它们之间的区别就在于传递函数不同

### 传递函数
把颜色显示到一个电子设备上时，需要把颜色转换成一个视频信号，这时候就需要一个传递函数

一个传递函数分为两部分：
* OETF：光转电传递函数，负责把场景线性光转到非线性视频信号值
* EOTF：电转光传递函数，负责把非线性视频信号值转换成显示光亮度

![传递函数](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922193715.jpg)

### Gamma校正
简单定义：
* $V_{out} = V_{in}^{gamma}$
* **Gamma 是指对线性三色值和非线性视频信号之间进行编码和解码的操作。**
就比如说当相机拍摄到自然中的线性的光信号，传递到电脑中进行存储时，就需要进行一个 Gamma 的编码操作，获得非线性的视频信号；最后通过显示器显示时，又需要将非线性的视频信号还原回一个线性的光信号，这时就需要进行一个 Gamma 的解码操作，最后得到我们所见到的样子（线性的光信号）

![Gamma校正](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/10ef75d8-3fb0-4eb4-83a9-8f6e2cdad18b.jpg)

Gamma 矫正的实例：简单理解为就是线性显示、非线性存储。

![Gamma 校正实例](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922195154.jpg)

* 图像经过 gamma 编码存储在硬盘中，将获取到的物理数据做一次 gamma 值约为 0.45 的映射，这样的过程叫做 gamma 编码。
由曲线可知，此时图像的像素比实际物理像素要更亮
* 在显示图像时，需要将每个像素做一次 gamma 值约为 2.2 的矫正，使最终的结果为正确的物理数据。
经过显示的 gamma 校正之后，之前偏亮的图像亮度降低 


#### 进行 Gamma 矫正的原因
这其实和人眼的特性有关，**人眼对暗部的变化要更加敏感一点**。如果要存储更多的有效信息，就需要更多的位置去存储暗部值，即在存储暗部时用更高精度的值去存储，而对于亮部则可以用相对精度较低的值去存储。

所以，非线性转换的目的主要是为了优化存储空间和带宽，传递函数能够更好地帮我们利用编码空间。


## 5.2 韦伯定律

![感知上的渐变和真实的渐变](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922195906.jpg)

![韦伯定律](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922200334.jpg)

**韦伯定律**：即感觉的差别阈限随原来刺激量的变化而变化，而且表现为一定的规律性，用公式来表示，就是 △Φ/Φ=C，其中 Φ 为原刺激量，△Φ 为此时的差别阈限，C 为常数，又称为韦柏率。

简单来说就是，当所受刺激越大时，需要增加的刺激也要足够大才会让人感觉到明显变化，但是只适用于中等强度的刺激

### 小结
1. 人眼对暗部变化比亮部更加敏感
2. 我们目前所使用的真彩格式 RGBA32，每个颜色通道只有8位用于记录信息，为了合理使用带宽和存储空间，需要进行非线性转换。
3. 目前我们所普遍使用的 sRGB 颜色空间标准，他的传递函数 gamma 值为 2.2(2.4)


### CRT
早期我们使用的图像显示器都是CRT（学名:阴极射线显像管），人们发现这种设备的亮度与电压并不成线性关系，而是 gamma 值约为 2.2 类似幂律的关系。 

![CRT](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922201139.jpg)

#### CRT与转换函数
1. CRT非线性响应的历史原因
2. 人眼对暗部颜色变化更加敏感

![CRT与转换函数](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922201237.jpg)


### 中灰值
**中灰值并非一个明确的值，而是取决于实际的视觉感受**

![纯灰色的条看上去左右部分明暗不同](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922201659.jpg)

![20240922201753](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922201753.jpg)



## 5.3 线性工作流
* 线性工作流就是在生产的各个环节中，正确使用 gamma 编码和 gamma 解码，使得最终得到的颜色数据与最初输入的物理数据保持一致
* 如果使用 Gamma 空间的贴图，在传给着色器前需要从 Gamma 空间转到线性空间，以确保获得正确的计算结果

如果不在线性空间下进行渲染工作：
Gamma 空间中进行亮度计算时，会容易过曝：因为光线信息在经过 Gamma 编码后，他的值比实际上的要大

![Gamma 空间中进行亮度计算容易过曝](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922203151.jpg)

进行颜色混合时，也会让亮度过高，出现 “黑边” 

![颜色重叠区域出现 “黑边”](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922203316.jpg)

进行光照渲染时，如果把中灰色以 0.5 作为实际的物理光强进行计算的话，就会出现如下左图的情况；但是实际上中灰色的物理光强度应该是 0.18

![20240922203803](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922203803.jpg)



## 5.4 Unity 中的颜色空间
通过点击菜单 Edit -> Project Settings -> Player 页签 -> Other Settings 下的 Rendering 部分进行修改，参数 Color Space 可以选择 Gamma 或 Linear

![Unity 中的颜色空间](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922203954.png)

* 当选择 Gamma Space 时，Unity 不会做任何处理
* 当选择 Linear Space 时，引擎的渲染流程在线性空间计算，理想情况下项目使用线性空间的贴图颜色，不需要勾选 sRGB，如果勾选了 sRGB 的贴图，会通过硬件特性采样时进行线性转换。


#### 硬件支持
线性空间需要图形 API 的硬件支持，目前支持的平台有：
1. Windows，Mac OS x 和 Linux
2. Xbox One
3. PS4（新的 PS5 肯定支持）
4. Android
5. IOS
6. WebGL（新的 WebGPU 肯定支持）

#### 硬件特性支持
主要由两个硬件特性来支持：
* sRGB Frame Buffer
    * 将 shader 的计算结果输出到显示器前进行 Gamma 矫正
    * 作为纹理被读取时自动会把存储的颜色从 sRGB 空间转换到线性空间
    * 调用 ReadPixels ()、ReadBackImage () 时，会直接返回 sRGB 空间下的颜色
    * sRGB Frame Buffer 只支持每通道为 8bit 的格式，不支持浮点格式
    * HDR 开启后会先把渲染结果绘制到浮点格式的 Frame Buffer 中，最后绘制到 sRGB Frame Buffer 上输出
* sRGB Sampler
    * 将 sRGB 的贴图进行线性采样的转换

使用硬件特性完成 sRGB 贴图的线性采样和 shader 计算结果的 gamma 矫正，比起在 shader 里对贴图采样和计算结果的矫正要快

## 5.5 资源导出问题

### SP 贴图导出
SP 贴图导出时，线性的颜色值经过了 gamma 变换，颜色被提亮了，所以在 unity 中需要勾选 sRGB 选项，让他在采样时能还原回线性值。

![SP 贴图导出](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922204852.jpg)

### PhotoShop
如果使用线性空间，一般来说 PS 可以什么都不改，导出的贴图只要勾上 sRGB 即可
如果调整 PS 的 gamma 值为 1，导出的贴图在 unity 中也不需要勾选 sRGB 了

![PhotoShop 导出](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922205104.jpg)

* PS 对颜色管理特别精准，Unity 中看到的颜色要经过显示器的 gamma 变换，而 PS 不会，PS 会读取显示器的 Color Profile，反向补偿回去。所以 PS 里使用的是一个真实的颜色值
* PS 有第二个 Color Profile（Document Color Profile）。通常它的默认值就是 sRGB Color Profile，和显示器的 Color Profile 一致，颜色是被这个 Color Profile 压暗了，所以 PS 中看到的结果才和 Unity 的一样（通过一个灰度值来控制颜色的显示，并且这个灰度值和显示器的 gamma 值一致，让我们看着和 unity 一样）

#### 半透明混合
Unity 在进行半透明混合时，会先将它们转换到一个线性空间下，然后再进行混合。但是 PS 中，图层和图层之间做混合的时候，每个上层的图层都会读取他们的 Color Profile，经过一个 Gamma 变换，再做混合，得到的结果就会偏暗一些。

![半透明混合](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240922205515.jpg)


### Unity 中的颜色空间转换函数
Unity CG.cginc 文件中封装的函数
其中，带有 Exact 后缀的的函数计算精确，但消耗高；不带有 Exact 后缀的函数是一种近似解法

``` glsl
inline float GammaToLinearSpaceExact (float value)
{
    if (value <= 0.04045F)
        return value / 12.92F;
    else if (value < 1.0F)
        return pow((value + 0.055F)/1.055F, 2.4F);
    else
        return pow(value, 2.2F);
}

inline half3 GammaToLinearSpace (half3 sRGB)
{
    // Approximate version from http://chilliant.blogspot.com.au/2012/08/srgb-approximations-for-hlsl.html?m=1
    return sRGB * (sRGB * (sRGB * 0.305306011h + 0.682171111h) + 0.012522878h);

    // Precise version, useful for debugging.
    //return half3(GammaToLinearSpaceExact(sRGB.r), GammaToLinearSpaceExact(sRGB.g), GammaToLinearSpaceExact(sRGB.b));
}

inline float LinearToGammaSpaceExact (float value)
{
    if (value <= 0.0F)
        return 0.0F;
    else if (value <= 0.0031308F)
        return 12.92F * value;
    else if (value < 1.0F)
        return 1.055F * pow(value, 0.4166667F) - 0.055F;
    else
        return pow(value, 0.45454545F);
}

inline half3 LinearToGammaSpace (half3 linRGB)
{
    linRGB = max(linRGB, half3(0.h, 0.h, 0.h));
    // An almost-perfect approximation from http://chilliant.blogspot.com.au/2012/08/srgb-approximations-for-hlsl.html?m=1
    return max(1.055h * pow(linRGB, 0.416666667h) - 0.055h, 0.h);

    // Exact version, useful for debugging.
    //return half3(LinearToGammaSpaceExact(linRGB.r), LinearToGammaSpaceExact(linRGB.g), LinearToGammaSpaceExact(linRGB.b));
}
```


---------------------------------------------------------------


# 6. LDR 与 HDR

## 6.1 基本概念
* HDR：High Dynamic Range
* LDR：Low Dynamic Range

动态范围 = 最高亮度 / 最低亮度
将场景中高动态范围的光线信息，转换到显示设备上可显示出来的低动态范围的视频信息，这一过程被称为色调映射（Tone Mapping）

Tips：电脑显示器的最高亮度是由之前经验积累下来的一个，让人眼舒适的，相对统一的亮度。

### LDR
* 通常 LDR 的精度范围是 8 位（2 的 8 次方）
* 0-1 的单通道
* 常用的存储格式为 jpg、png、tga
* 拾色器、一般的图片，电脑屏幕都是 LDR

![拾色器](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240923104735.jpg)

### HDR
* 远高于 8 位的精度
* 单通道的最大值可超过 1
* 常用 HDR 图片存储格式有 hdr、tif、exr、raw
* HDRI、真实世界的光都是 HRD

### 相机记录信息的过程
* 首先，将曝光值进行计算，将曝光值重新映射回相机能够感应的范围内（受到光圈、快门时间，以及传感器灵敏度等的影响）
* 再把值进行输出（线性），存储进数码相机的格式内
* 之后会经过一个线性变换（白平衡、色彩矫正、色调验色以及伽马矫正等这一系列过程），最终会得到一张 LUT 图
* 不同相机厂商 LUT 图的格式是不同的

### 为什么需要 HDR
* LDR 是对现实颜色进行压缩并且呈现出来，具有一定局限性，在进行后期效果调整和后续加工会因为颜色精度不够而难以进行
* HDR 有着更好的色彩，更高的动态范围和更丰富的细节，并且能够有效地防止画面过曝，亮度值超过 1 的颜色也能很好的表现出来。使得像素光亮度变得正常，视觉传达更加真实。

![HDR](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240923105416.jpg)

只有 HDR 才有超过 1 的数值，才有光晕（bloom）的效果, 高质量的 bloom 能体现画面的渲染品质。

![20240923105613](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240923105613.jpg)



## 6.2 在 Unity 中设置 HDR

#### Camera 的 HDR 选项
可以在 Project Setting - Graphics - Tier Settings 中设置是否开启 HDR。

![Camera 的 HDR 选项](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/TA100A2600-4.png)

![Graphics 设置](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240923110844.png)

Unity Camera 基本参数中有 HDR，如果开启的话，Unity 会将场景渲染到 HDR 图像缓冲区，下一步进行后处理，在 Tone mapping 过程中会把 HDR 转换成 LDR，再将 LDR 图像（R8G8B8）发送给显示器

```mermaid
graph LR
A[HDR 图像缓冲区] --> B[后处理] --> C[Tone Mapping] --> D[显示器]
```


#### Lightmap 的 HDR
在 Project Setting - Player - Other Settings 中，可以设置 Lightmap 的编码质量。
将 Lightmap 编码质量设置为 High Quality，将会为 Lightmap 开启 HDR 光照贴图的支持

![Lightmap 的 HDR](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240923112252.png)


#### 拾色器的 HDR
在 Shader 中在颜色属性前添加 `[HDR]` 标签，即可将颜色支持 HDR 。
* 使用 Intensity 滑动条可以调整颜色的强度
* 滑动条每增加 1，提供的光亮增加一倍

### 优缺点
优点：
* 画面中亮度超过 1 的部分不会被截为 1，增加亮部的细节并减少曝光
* 减少画面较暗部分的色阶感（用更大的精度去存储暗部）
* 更好的支持 Bloom 效果

缺点：
* 渲染流程中多了 Tone mapping 这一步，渲染速度慢
* 占据更多显存
* 不支持硬件抗锯齿
* 低端手机不支持 HDR


## 6.3 Bloom
Bloom 流程：渲染出原图后，提取原图中较亮的部分，对提取出来的图片进行高斯模糊，然后再叠加到原图上

![Bloom](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240923115247.jpg)

Unity 中的 Bloom 流程：

![Unity Bloom 流程](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240923115514.jpg)


## 6.4 HDR 与 Tone Mapping

### Tone Mapping 的概念
* 色调映射
* 将 HDR 转化为 LDR
* 如果是线性映射的话效果极差
* 所以一般使用曲线，把亮度区域和阴影区域向中等亮度方向压缩 → S 曲线

![Tone Mapping](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240923115732.jpg)

### Tone Mapping 映射曲线
HDR 的 Tone Mapping 映射曲线会把暗部压得更低一些，亮部也往回缩一点，让整体效果看起来更加真实

![Tone Mapping 映射曲线](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240923121821.png)


#### ACES 映射曲线
* Academy Color Encoding System 学院颜色编码系统
* 最流行、最被广泛使用的 Tonemapping 映射曲线
* 效果：对比度提高，很好地保留暗处和亮处的细节

![ACES 映射曲线](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240923122022.png)


#### 其他类型的Tonemapping曲线

![其他类型的Tonemapping曲线](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240923130059.png)


## 6.5 LUT （Look-Up Table）
LUT （颜色查找表），是一种颜色校正技术，它可以将一组颜色映射到另一组颜色上，用于颜色校正。可以理解为滤镜。

与 Tone mapping 不同的是，LUT 是对 LDR 图进行处理，而 Tone mapping 则是处理 HDR 图

可以调整 RGB 三个通道的 LUT 被称为 3D LUT。
格式有：3DL, CUBE, CSP, ICC 配置文件。

> Tips：可以在 PS 中调整 LUT，导出的 LUT 作为滤镜去调整画面（相当于整个画面的滤镜）

LUT 的效果比较：

![LUT 的效果比较](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240923133215.png)


-------------------------------------------------------------------


# 7. Bump Mapping

## 7.1 Bump Mapping 介绍

### 表达物体细节的三个尺度
* 宏观尺度
    * 覆盖很多个像素
    * 由顶点或三角形或其他的图元来表示的，建模通常就是在宏观尺度上进行处理
* 中观尺度
    * 覆盖几个像素
    * 描述了宏观尺度和微观尺度之间的特征，细节比较复杂，没有办法使用单个三角形来进行渲染，但这些细节又足够大，能够让观察者看出其中的细节
    * 比如：皮肤的褶皱，肌肉组织的细节，砖块之间的缝隙等等。
* 微观尺度
    * 小于一个像素
    * 表现在着色模型中，而着色模型通常在像素着色器中实现。使用纹理贴图作为参数，模拟物体表面围观几何形状的相互作用
    * 比如：高光部分在微观尺度下是光滑的，漫反射部分在微观尺度下是粗糙的

### Bump Mapping 凹凸映射
* 凹凸映射就是模拟中观尺度的常用方法之一，它能让观察者感知到比几何模型的尺度更小的细节。
* 基本思想是将尺度细节相关的信息编码进纹理中，在着色过程中，我们用稍微受到干扰的表面去模拟真实的表面，从而模拟出表面的小尺度的细节。
* 从原理上讲，凹凸贴图映射技术是对物体表面贴图进行变化，然后再进行光照计算的一种技术。例如给法线分量增加噪音，或者在一个保持扰动值的纹理图中进行查找（视差映射贴图，或者浮雕贴图）
* 这是一种提升物体真实感的有效方法，但却不需要额外的提升物体的几何复杂度（不需要添加额外的顶点）。这种方式在提升物体的表面细节或者表面的不规则性方面有显著效果。

![Bump Mapping](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240925104108.jpg)


### 凹凸映射的分类
* Bump Mapping 的种类主要有：法线映射、视差映射、浮雕映射
* 用处广泛，可以用来增加模型细节效果，或者做一些特殊的画面表现效果
* 最常用的是法线映射，一般的增加法线贴图后，会对局部的物体表面进行法线扰动，进而改变明暗关系，从而达到增加表面细节的效果。
* 上述的三种映射都会用到法线贴图

![Bump Mapping 分类](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240925122526.jpg)


## 7.2 Normal Mapping 法线映射

### 法线贴图
* Normal Map 是一张存有物体局部表面法线信息的一张贴图。在计算光照的时候，程序会去读取法线图，并获取到当前像素点的法线信息，结合光线信息进行光照计算。
* 使用法线贴图计算光照，可以让物体表现出更加丰富的细节（小范围的明暗变化），并随着光照方向的变换实时变化。
* 法线贴图一般由高模映射到对应的底模上来生成。但像金属，木头等这些细节丰富的物体，可借助程序化的软件如：Photo Shop，Substance Designer 等生成

![法线贴图](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/3bc8bb17-1853-4aa3-8b4e-57dc073072db.png)


###  切线空间
* 法线的存储，一般会放到模型的切线空间中。
* 切线空间是以物体表面的切线，副切线和法线组成的几何空间
* 切线空间中，坐标原点为顶点位置，顶点法线 **n** 作为 z 轴，切线 **t** 和纹理坐标系一致，副切线 **b** 则由顶点法线和切线叉乘得到
* **计算光照时，需要把光照运算的向量放到统一坐标系下**。读取切线空间法线，需要世界坐标转切线空间的转换矩阵 或 切线空间转世界空间的转换矩阵，将向量统一到同一坐标系后再进行光照操作。

![切线空间](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240925123559.png)


### 世界空间和切线空间的转换 —— TBN矩阵
将世界坐标系下的顶点法线（Normal）和切线（Tangent）以及副切线（Bitangent）作为切线空间坐标系的正交基。并用这三个向量的标准正交基来构建转换矩阵。对应关系为法线作为Z轴，切线作为X轴，副切线作为Y轴。

构建一个3*3的矩阵来做空间向量的坐标系转换。一般的叫该矩阵为 **TBN 矩阵**。

#### 切线空间 → 世界空间
在Unity中，将读取到的切线空间的法线信息右乘TBN矩阵，即可将法线从切线空间转换到世界空间

``` hlsl
float3x3 TBN = float3x3(i.tangentDir, i.biTangentDir, i.normalDir)
float3 worldNormal = normalize(mul(localNormal, TBN));
```

#### 世界空间 → 切线空间
* 因为 TBN 矩阵是一个正交矩阵，所以 TBN 矩阵的逆矩阵就是 TBN 的转置矩阵
* 从世界空间转换到法线空间进行相关处理，只需要顶点右乘 TBN 矩阵的转置矩阵即可

$$
TBN = 
\begin{bmatrix}
T_x & B_x & N_x \\
T_y & B_y & N_y \\
T_z & B_z & N_z \\
\end{bmatrix}
$$

$$
TBN^{-1} = TBN^T = 
\begin{bmatrix}
T_x & T_y & T_z \\
B_x & B_y & B_z \\
N_x & N_y & N_z \\
\end{bmatrix}
$$

### 引入切线空间的好处
实际上法线存在哪个空间下都是可以的，但是实际上我们不仅仅要存储法线信息，更重要的是后续的光照计算。选择哪个坐标系意味着我们需要把光照计算用到的不同信息全部转换到对应的坐标系中。但是，使用切线空间存储法线带来的收益高过空间转换的成本。

优点：
* 自由度高
    * 同一张法线可以作用于不同的模型
    * 如果把法线信息存在模型空间下，得到的则是一个绝对的法线信息，这个法线信息就只是属于一个模型的法线，比如说存到球的模型空间下，法线就无法用于圆环中
* 很方便实现 uv 动画
    * 法线在切线空间中，只是对现有现有法线的扰动。对贴图采样进行偏移时，就可以改动顶点 uv 实现不同的凹凸效果，进而实现 uv 动画。
* 基于同样的原理，可以重用法线贴图
    * 一个立方体一面的法线贴图可以同时用在其他五个面上
* 便于压缩
    * 在切线空间中，法线始终是垂直于表面向外。在归一化的法线向量中，他的值始终是处于0-1之间的，这样的话我们就只用存储副切线和切线，再用 sqrt 算出法线即可
    * 而在模型空间中，法线可以为负，在-1到1之间，这样就不能通过只存储两个值来推导出法线的值，必须要储存三个值（否则无法指明法线的方向，因为作为贴图的颜色通道，只能存0-1的值），这样就无法压缩了。


### Unity 中的法线贴图的压缩格式
* 在 unity 中，非移动平台上，Unity 会把法线贴图转换成 DXRT5nm 格式。这种格式只有两个有效通道 GA 通道，这样可以节省空间
* GA 存储对应法线的 y、x 分量，z 分量需要通过一个简单的运算求得。
* 而移动平台 Unity 使用传统的 RGB 通道

``` hlsl
// DXR5nm
real3 UnpackNormalAG(real4 packedNormal, real scale = 1.0)
{
    real3 normal;
    normal.xy = packedNormal.ag * 2.0 - 1.0;
    normal.z = max(1.0e-16,，sqrt(1.0 - saturate(dot(normal.xy，normal.xy))));
    
    // must scale after reconstruction of normal.z which also
    // mirrors UnpackNormalRGB(). This does imply normal is not returned
    // as a unit length vector but doesn't need it since it will get normalized after TBN transformation.
    // If we ever need to blend contributions with built-in shaders for URP
    // then we should consider using UnpackDerivativeNormalAG() instead like
    // HDRP does since derivatives do not use renormalization and unlike tangent space
    //normals allow you to blend,accumulate and scale contributions correctly.
    normal.xy *= scale;
    return normal;
}

// RGB
real3 UnpackNormalRGB(real4 packedNormal, real scale = 1.0)
{
    real3 normal;
    normal.xyz = packedNormal.rgb * 2.0 - 1.0;
    nromal.xy *= scale;
    return normal;
}
```


## 7.3 Parallax Mapping 视差映射

### 视差映射的概述

* 视差贴图Parallax Mapping，又称为 Offset Mapping，以及virtual displacement mapping)，于2001年由Kaneko引入，由Welsh进行了改进和推广
* 主要为了赋予模型表面遮挡关系的细节。引入了一张高度图
* 可以和法线贴图一起使用，来产生一些真实的效果
* 高度图一般视为顶点位移来使用，此时需要三角形足够多，模型足够精细，否则看起来会有块状
* 如果在有限的三角形面的情况下，怎么办？这就用到了视差映射技术
* 视差映射技术：
    * 核心：改变纹理坐标
    * 需要一张存储模型信息的高度图，利用模型表面高度信息来对纹理进行偏移（例如：低位置的信息被高位置的信息遮挡掉了，所以会采样更高的信息）

![视差映射](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240925133355.jpg)


### 视差映射的实现
* 和法线贴图一样，是欺骗眼睛的做法（只改变纹路，不增加三角形）
* 我们的模型在切线空间下，所有的点都位于切线和副切线组成的平面内（图中0.0点），但实际上物体要有更丰富的细节。例如图中的情况
    * 如果不使用视差贴图，要计算当前视角下，片元A点（黄色）的信息，就是图中的Ha
    * 实际使用视差贴图时，真实的情况应该是视线和A点延长线和物体的交点，也就是B点，相应的就是Hb

![视差映射](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240925134736.png)

**视差映射的具体算法**：如何在知道 A 的 uv 值的情况下，算出 B 的 uv 值
* 知道 AB 两者的偏移量即可
* 偏移量的获得：用近似的方法去求解	
    * 首先拿 A 的高度信息进行采样，得到物体表面距离水平面（0.0）的深度值Ha。
    * 用深度值 Ha 和视线的三角关系算出物体上等比的偏移方向，算出近似的 B 点（可以看到图中近似点 B 和实际点 B 还是有挺大差距的，所以模拟度比较低）
    * 得到偏移之后 B 点的 uv，再去对法线贴图进行采样、计算时，就不会采样 A 点了，而是 B 点

![视差映射](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240925135148.png)

`d = v.xy * ha * scale / v.z`
``` hlsl
float height = 1 - tex2D(_HeightMap, i.uv2.xy).r;
real3 viewDirTS = TransformWorldToTangent(viewDirWS.xyz, T2W);
float2 offset = viewDirTS.xy / viewDirTS.z * height * _HeightScale;
i.uv.xy += offset;
i.uv.zw += offset;
```

**理解**：视差贴图是如何产生遮挡效果的
* 当视线看到的是 A 点这样深度比较大的，那么视差贴图计算出的偏移值也是非常大的，这样 A 点最终被渲染出来的机会就比较小（偏移后就被采样到其他点上了）
* 当视线看到 B 点这样深度比较小的点，计算出来的偏移就比较下，甚至原来点的附近，所以被采样的机会就比较大
* 深度大的点很容易被深度小的点覆盖掉，这样就会表现出遮挡的效果 


### Steep Parallax Mapping 陡峭视差映射
陡峭视差映射也是一个近似的解，但相比于普通的视差映射要精确很多，效果上也更好，并且会对uv坐标进行合理性检查。

陡峭视差映射的基本思想是将深度分为等距的若干层，然后从最顶层进行采样，并且每次沿着视角方向偏移一定值，若当前层的深度大于采样出的深度，则停止检查并返回最后的结果。

![陡峭视差映射](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240925203056.png)


陡峭视差映射的算法：（计算偏移点的过程）
* 首先对A点采样，得到深度大约为0.8的位置，而其对应视线深度为0.0，不符合我们的基本思想，继续采样
* 采样B点，深度为1，视线深度为0.25，不符合，继续采样
* 采样C点，深度大约为0.8，视线深度为0.5，不符合，继续采样
* 采样D点，采样深度为0.5，视线深度约为0.75，符合上述的条件，认为是比较合理的一个偏移点，就返回结果（return）。

``` hlsl
//陡峭视差映射
float2 SteepParallaxMapping(float2 uv, real3 viewDirTS){
    float numLayers = 20.0;
    float layerHeight = 1.0 / numLayers;
    float currentLayerHeight = 0.0;
    float2 offlayerUV = viewDirTS.xy / viewDirTS.z *_HeightScale;
    float2 Stepping = offlayerUV / numLayers;
    float2 currentUV. = uv;
    float2 AddUV = float2(0, 0);
    float currentHeightMapValue = tex2D(_HeightMap, currentUV + AddUV).r;
    for (int i = 0; i < numLayers; i++)
    {
        if (currentLayerHeight > currentHeightMapValue){
            return AddUV;
        }
        AddUV += Stepping;
        currentHeightMapValue = tex2D(_HeightMap，currentUV + AddUuV).r;
        currentLayerHeight += layerHeight;
    }
    return AddUv;
}
```

陡峭视差映射的缺点：
问题在于他的分层机制，如果分的层数过多，步进次数会变多，那么性能开销就会比较大；如果分的层数太少，那么就会有明显的锯齿

优化：可以根据视线v和法线n的角度来做对采样层数的最大值和最小值的阈值的限定


## 7.4 Relief Mapping 浮雕映射

### 浮雕映射的概述
* 浮雕映射是陡峭视差映射的一个改良版本
* 浮雕映射相比于视差映射，能够更精确地计算uv偏移量，更容易提供更多的深度，还可以做自阴影以及闭塞效果

![浮雕映射](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240925204041.jpg)

####  浮雕映射的原理
浮雕映射一般先使用射线步进，再使用二分查找的方法来决定UV偏移量

为什么不直接使用二分查找呢：

![浮雕映射的实现](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240925204821.png)

``` hlsl
//浮雕贴图
float2 ReliefMapping(float2 uv, real3 viewDirTS)
{
    float2 offlayerUV = viewDirTS.xy / viewDirTS.z * _HeightScale;
    float RayNumber = 20;
    float layerHeight = 1.0 / RayNumber;
    float2 SteppingUV = offlayerUV / RayNumber;
    float offlayerUVL = length(offlayerUV);
    float currentLayerHeight = 0;
    
    float2 offuv= float2(0,0);
    for (int i = 0; i < RayNumber; i++)
    {
        offuv += SteppingUV;

        float currentHeight = tex2D(_HeightMap, uv + offuv).r;
        currentLayerHeight += layerHeight;
        if (currentHeight < currentLayerHeight)
        {
            break;
        }
    }

    float2 T0 = uv-SteppingUV, T1 = uv + offuv;

    for (int j = 0;j<20;j++)
    {
        float2 P0 = (T0 + T1) / 2;

        float P0Height = tex2D(_HeightMap, P0).r;

        float P0LayerHeight = length(P0) / offlayerUVL;

        if (P0Height < P0LayerHeight)
        {
            T0 = P0;

        }
        else
        {
            T1= P0;
        }

    }

    return (T0 + T1) / 2 - uv;
}
```

#### 视差闭塞贴图（POM = Parallax  Occlusion Mapping）
相对于浮雕贴图，不同之处在于最后一步
* 浮雕贴图是在确认最后步进之后进行二分查找（在迭代次数比较多的情况下，还是挺耗的）
* 视差闭塞贴图是在最后步进的两端uv值进行采样（下图红色箭头），采样之后再对这两个结果进行插值，插值的结果作为P点最终的偏移值

![视差闭塞贴图](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240925205309.png)


优点：
* 相对于浮雕映射，性能更好（最后只做插值，而浮雕要做二分查找）
* 相对于陡视差贴图，精确性更好

要求：
* 因为最后要做插值，所以要求表面是相对比较平滑/连续的，如果有莫名的凸起结果可能会出错

## 7.5 Bump Mapping

原文链接：https://www.yuque.com/sugelameiyoudi-jadcc/okgm7e/nspda5

``` hlsl
Shader "Bump Mapping"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}

        [Toggle(_NORMALMAP)] _EnableBumpMap("Enable Normal/Bump Map", Float) = 0.0
        _NormalMap("NormalMap",2D) = "bump" {}
        _NormalScale("NormalScale" ,Float) = 1

        [Toggle(_HEIGHTMAP)] _EnableHeightMap("Enable Height Map",Float) = 0.0
        [Toggle(_RELIEFMAP)] _EnableReliefMap("Enable Relief Map",Float) = 0.0
        _HeightMap("HeigheMap",2D) = "white"{}
        _HeightScale("HeightScale",range(0,0.5)) = 0.005

        _Smoothness("Smoothness",range(0,2)) = 0.5
        _Metallic("Metallic",range(0,1)) = 0.2
    }
    SubShader
    {
        Tags
        {
            "RenderType"="Opaque"
        }
        LOD 100

        HLSLINCLUDE
        // Material Keywords
        #pragma shader_feature _NORMALMAP
        #pragma shader_feature _HEIGHTMAP
        #pragma shader_feature _RELIEFMAP
        //#pragma shader_feature _SPECULAR_COLOR

        #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl"
        #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl"
        ENDHLSL

        Pass
        {
            Name "URPLighting"
            Tags
            {
                "LightMode" = "UniversalForward"
            }

            HLSLPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            struct appdata
            {
                float4 positionOS : POSITION;
                float3 normalOS : NORMAL;
                float2 uv : TEXCOORD0;

                float4 tangentOS : TANGENT;
            };

            struct v2f
            {
                float4 positionCS : SV_POSITION;
                float3 normalWS:TEXCOORD0;
                float3 viewDirWS:TEXCOORD1;

                // Note this macro is using TEXCOORD2
                DECLARE_LIGHTMAP_OR_SH(lightmapUV, vertexSH, 2);
                float4 uv : TEXCOORD3;
                #if defined(_HEIGHTMAP) || defined (_NORMALMAP)
				float4 tangentWS:TEXCOORD4;
                #endif
                #ifdef _HEIGHTMAP
                   float4 uv2 : TEXCOORD5;        
                #endif
            };

            sampler2D _MainTex;
            float4 _MainTex_ST;
            float _Smoothness;
            sampler2D _NormalMap;
            float4 _NormalMap_ST;
            float _Metallic;
            float _NormalScale;
            sampler2D _HeightMap;
            float4 _HeightMap_ST;
            float _HeightScale;

            v2f vert(appdata v)
            {
                v2f o;
                VertexPositionInputs positionInputs = GetVertexPositionInputs(v.positionOS.xyz);

                o.positionCS = positionInputs.positionCS;
                o.viewDirWS = GetWorldSpaceViewDir(positionInputs.positionWS);

                o.uv.xy = TRANSFORM_TEX(v.uv, _MainTex);
                VertexNormalInputs normalInputs = GetVertexNormalInputs(v.normalOS, v.tangentOS);
                o.normalWS = normalInputs.normalWS;


                #if defined (_HEIGHTMAP) ||  defined (_NORMALMAP)
                     real sign = v.tangentOS.w * GetOddNegativeScale();
				     half4 tangentWS = half4(normalInputs.tangentWS.xyz, sign);
                     o.tangentWS = tangentWS;
                #endif

                #ifdef _NORMALMAP
                    o.uv.zw = TRANSFORM_TEX(v.uv, _NormalMap);

                #endif
                #ifdef _HEIGHTMAP
                     o.uv2.xy = TRANSFORM_TEX(v.uv, _HeightMap);
                #endif

                OUTPUT_SH(normalInputs.normalWS.xyz, o.vertexSH);
                return o;
            }

            //视差映射
            float2 ParallaxMapping(float2 Huv, real3 viewDirTS)
            {
                float height = tex2D(_HeightMap, Huv).r;
                float2 offuv = viewDirTS.xy / viewDirTS.z * height * _HeightScale;

                return offuv;
            }

            //陡峭视差映射
            float2 SteepParallaxMapping(float2 uv, real3 viewDirTS)
            {
                float numLayers = 20.0;

                float layerHeight = 1.0 / numLayers;

                float currentLayerHeight = 0.0;

                float2 offlayerUV = viewDirTS.xy / viewDirTS.z * _HeightScale;

                float2 Stepping = offlayerUV / numLayers;

                float2 currentUV = uv;

                float2 AddUV = float2(0, 0);

                float currentHeightMapValue = tex2D(_HeightMap, currentUV + AddUV).r;

                for (int i = 0; i < numLayers; i++)
                {
                    if (currentLayerHeight > currentHeightMapValue)
                    {
                        return AddUV;
                    }
                    AddUV += Stepping;
                    currentHeightMapValue = tex2D(_HeightMap, currentUV + AddUV).r;
                    currentLayerHeight += layerHeight;
                }
                return AddUV;
            }

            //浮雕贴图
            float2 ReliefMapping(float2 uv, real3 viewDirTS)
            {
                float2 offlayerUV = viewDirTS.xy / viewDirTS.z * _HeightScale;
                float RayNumber = 20;
                float layerHeight = 1.0 / RayNumber;
                float2 SteppingUV = offlayerUV / RayNumber;
                float offlayerUVL = length(offlayerUV);
                float currentLayerHeight = 0;
                
                float2 offuv= float2(0,0);
                for (int i = 0; i < RayNumber; i++)
                {
                    offuv += SteppingUV;

                    float currentHeight = tex2D(_HeightMap, uv + offuv).r;
                    currentLayerHeight += layerHeight;
                    if (currentHeight < currentLayerHeight)
                    {
                        break;
                    }
                }

                float2 T0 = uv-SteppingUV, T1 = uv + offuv;

                for (int j = 0;j<20;j++)
                {
                    float2 P0 = (T0 + T1) / 2;

                    float P0Height = tex2D(_HeightMap, P0).r;

                    float P0LayerHeight = length(P0) / offlayerUVL;

                    if (P0Height < P0LayerHeight)
                    {
                        T0 = P0;

                    }
                    else
                    {
                        T1= P0;
                    }

                }

                return (T0 + T1) / 2 - uv;
            }

            half4 frag(v2f i) : SV_Target
            {
                #if defined(_HEIGHTMAP) || defined (_NORMALMAP)

                float sgn = i.tangentWS.w;

				float3 bitangent = sgn * cross(i.normalWS.xyz, i.tangentWS.xyz);
                half3x3 T2W = half3x3(i.tangentWS.xyz, bitangent.xyz, i.normalWS.xyz);
                
                #endif

                float3 viewDirWS = normalize(i.viewDirWS);
                //视差映射
                #ifdef _HEIGHTMAP
                    real3 viewDirTS = normalize(TransformWorldToTangent(-viewDirWS.xyz,T2W));
                    float2 offuv = float2(0,0);
                    #ifdef _RELIEFMAP
                        offuv = ReliefMapping( i.uv2.xy, viewDirTS);  //陡峭视差映射
                    #else
                        offuv = ParallaxMapping( i.uv2.xy, viewDirTS); //普通视差映射
                    #endif

                    i.uv.xy += offuv;
                    i.uv.zw += offuv;
                
                #endif
                // sample the texture
                half4 col = tex2D(_MainTex, i.uv.xy);

                // URP 光照
                SurfaceData surfaceData = (SurfaceData)0;

                surfaceData.albedo = col;
                surfaceData.alpha = col.a;
                surfaceData.smoothness = _Smoothness;
                surfaceData.metallic = _Metallic;
                surfaceData.occlusion = 1;
                InputData inputData = (InputData)0;
                inputData.viewDirectionWS = viewDirWS;
                //法线映射
                #ifdef _NORMALMAP
                 float3 normalTS = UnpackNormalScale(tex2D(_NormalMap,i.uv.zw), _NormalScale);
				 i.normalWS = TransformTangentToWorld(normalTS, T2W);
                #endif

                inputData.bakedGI = SAMPLE_GI(i.lightmapUV, i.vertexSH, i.normalWS);
                inputData.normalWS = i.normalWS;

                half4 color = UniversalFragmentPBR(inputData, surfaceData.albedo, surfaceData.metallic,
                                                   surfaceData.specular, surfaceData.smoothness, surfaceData.occlusion,
                                                   surfaceData.emission, surfaceData.alpha);

                return color;
            }
            ENDHLSL
        }
    }
}
```


----------------------------------------------------------------------


# 8. Flow Map 的实现

## 8.1 Flow Map 是什么
* Flow map 实际上是一张记录了 2D 向量信息的纹理
* Flow map 上的颜色（通常为RG通道）记录该处向量场的方向，让模型上某一点表现定量的特征。
* 通过在 shader 中偏移 uv 再对纹理进行采样，来模拟流动效果

![Flow Map](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926113152.jpg)
![Flow Map](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926113316.jpg)

### UV 映射
对一个贴图进行纹理查找，就要用到uv坐标

如图为Unity中的uv坐标，类似于xy轴，用此uv坐标查找右边贴图的颜色值，采样会得到和贴图一模一样的结果

![用 uv 坐标采样会得到和贴图一模一样的结果](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926123845.jpg)

用正常的 u 坐标和相同的 v 坐标，那么采样结果就是右图的条纹状的结果

![用正常的 u 坐标和相同的 v 坐标采样会得到条纹状的结果](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926123904.jpg)

如果用同一个uv值采样的话，结果就会是同一的颜色

![用同一个 uv 值采样会得到同一的颜色](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926124419.jpg)


### Flowmap 的原理
原理：通过所带有的向量场信息对 uv 进行了一个偏移，来干扰我们采样时候的这个过程。 

如图可以看到，经过flowmap发生偏移后，让原本正常的采样变成了一个扭曲的效果

注意：不同软件的不同 uv 坐标：UE4 与 Unity 相比，反转了绿通道，所以使用的 flowmap 也会发生变化。（根据不同的引擎需求进行调整）

![Flow Map 偏移](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926124809.jpg)


### 为什么要使用 flowmap
flowmap 类似于 UV 动画，而非顶点动画。换而言之，无需对模型进行操作，易实现，运算开销小
不仅仅是水面，任何和流动相关的效果都可以采用 flowmap

![Flowmap 例子](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926125226.jpg)



## 8.2 Flowmap Shader

### 借助 Shader Graph 理解 Flow map
1. 采样 Flow map 获取向量场信息
    * 得到的是将来用于乘time的方向信息
    * flowmap不能直接使用，要将色值从[0,1]的范围映射到方向向量的范围[-1,1]
    ``` hlsl
    //从flowmap中获取流向
    float3 flowDir = tex2D(_FlowMap, i.uv) * 2.0 - 1.0;
    ```
2. 用向量场信息，使采样贴图时的 UV 随时间变化
    * UV - time
    * 为什么是用减法：用减法从视觉上来看，采样得到的图象是在向右移动，与直观的运算法则相同
3. 对同一贴图以半个周期的相位差采集两次，并线性插值，使贴图流动连续
    * 随着时间的进行，变形越来越夸张，为了将偏移控制在一定范围内，使用 frac 函数进行牵制
        ``` hlsl
        float phase = frac(_Time);
        ```

        ![不连续的周期三角波](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926132424.jpg)
    
    * 这样得到的效果会“跳变”，所以就构造周期相同，相位相差半个周期的波形函数
        ``` hlsl
        float phase0 = frac(_Time * 0.1 * _TimeSpeed);
        float phase1 = frac(_Time * 0.1 * _TimeSpeed + 0.5);
        ```

        ![相差半个周期的波形函数](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926132500.jpg)

    * 用相位差半个周期的两层采样进行加权混合，使纹理流动一个周期重新开始时的不自然情况被另一层采样覆盖
        ``` hlsl
        //平铺贴图用的uv
        float2 tiling_uv = i.uv * _MainTex_ST.xy + _MainTex_ST.zw;

        //用波形函数周期化向量场方向，用偏移后的uv对材质进行偏移采样
        half3 tex0 = tex2D(_MainTex, tiling_uv - flowDir.xy * phase0);
        half3 tex1 = tex2D(_MainTex, tiling_uv - flowDir.xy * phase1);

        //构造函数计算波形函数变化的权值，使得MainTex采样在接近最大偏移时有权值为0，并因此消隐，构造较平滑的循环
        float flowLerp = abs((0.5 - phase0) / 0.5);
        float3 finalColor = lerp(tex0, tex1, flowLerp); 
        ```

        ![缓解突变](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926133613.jpg)


### 用 Flowmap 修改法线贴图

``` hlsl
fixed4 frag(v2f i) : SV_Target
{
    float3 flowDir = tex2D(_FlowMap, i.uv) * 2.0 - 1.0;

    // 影响向量场的强度，越大不同位置流速差越明显
    flowDir *= -_Velocity;

    // 构造周期相同，相位相差半个周期的波形函数
    float phase0 = frac(_Time * 0.1 * _TimeSpeed + 0.5);
    float phase1 = frac(_Time * 0.1 * _TimeSpeed + 1.0);

    float2 tiling_uv = i.uv * _BumpMap_ST.xy + _BumpMap_ST.zw;

    // 用偏移后的uv对法线贴图（切线空间）进行采样
    half3 tnormal0 = UnpackNormal(tex2D(_BumpMap, tiling_uv + flowDir.xy * phase0));
    half3 tnormal1 = UnpackNormal(tex2D(_BumpMap, tiling_uv + flowDir.xy * phase1));
    float flowLerp = abs((0.5 - phase0) / 0.5);
    half3 tnormal = lerp(tnormal0, tnormal1, flowLerp);

    // 在法线从切线空间转化到世界空间前修改法线
    half3 worldNormal;
    worldNormal.x = dot(i.tspace0, tnormal);
    worldNormal.y = dot(i.tspace1, tnormal);
    worldNormal.z = dot(i.tspace2, tnormal);
    half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos));
    half3 worldRefl = reflect(-worldViewDir, worldNormal);

    ......

}
```


## 8.3 Flowmap 的制作

### Flowmap Painter

![Flowmap Painter](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926202832.jpg)

* 使用Flowmap Painter绘制得到的Flowmap为线性空间下的颜色（gamma1.0），不需要gamma矫正，取消勾选sRGB
* 否则会出现“没画的地方在流动”，“画的地方流动方向错误”等问题

![取消勾选sRGB](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926203634.png)

#### flowmap 的烘培和相关设置
* 注意 gamma 矫正选项、UV 匹配
    * 用 Labs UV transfer 节点来匹配高模和低模的 UV
* flowmap 贴图设置：
    * 无压缩或高质量
    * 确认色彩空间

![flowmap 的烘培和相关设置](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240926203919.jpg)


### Houdini Labs
Houdini Labs是内置在Houdini中的一组游戏开发相关的节点

//等到之后要用在项目里的时候再来学吧，不然现在直接学很快也就忘了

提前记录几个关键节点：
* flowmap：生成一张flowmap
* 修改flowmap的向量场：
    * flowmap_brush：手动绘制flowmap
    * flowmap_guide：使用绘制的曲线来控制flowmap（需要输入一条曲线）
    * flowmap_obstacle：让流向与物体进行碰撞检测（需要输入一个网格）
* flowmap_to_color：将flowmap向量场信息转化为颜色信息
* flowmap_visual：可视化flowmap
* maps_baker：烘焙（一个用于高模烘低模的节点）

视频地址：https://www.bilibili.com/list/7398208?sid=1067039&oid=546079941&bvid=BV1Zq4y157c9&p=2
