# 1. 模板测试和深度测试

## 1.1 模板测试 (Stencil Test)

![模板测试](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240928120606.jpg)

* 左图为颜色缓冲区中的一张图，在模板缓冲区中我们会给这张图的每一个片元分配一个0-255的数字（8位，默认为0）
* 中、右图可以看到，我们修改了一些0为1，通过自定义的一些准则，如输出模板缓冲区中1对应的片元的颜色；0的不输出，最后通过模板测试的结果就如右图所示

#### 模板测试的应用

<div align=center>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/a1bef728-9bbe-43fd-8146-92c1956a6e48.gif" height=240 alt="3D卡牌效果"/>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/688bcb20-4ebb-4de9-b3c7-fd7cf6353787.gif" height=240 alt="侦探镜效果"/>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/f41b20e2-2564-4569-a30c-7d70b6b27662.gif" height=240 alt="传送门效果"/>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/71794d66-dd32-4374-bbaf-8c2eeddc5f5b.gif" height=240 alt="侦探镜效果"/>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/00792bd1-b951-4f4e-9d9d-5e64d773c54d.gif" height=240 alt="笼中窥梦效果"/>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/29f13d23-90d8-4bed-86d8-6f506e2b6a21.gif" height=240 alt="笼中窥梦效果"/>
</div>

### 1.1.1 模板测试是什么

#### 从渲染管线出发

![渲染管线中的逐片元操作阶段](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/33535ecf-22ee-4eb1-99c2-609f40b2cb73.jpg)

渲染管线中的逐片元操作阶段：
* Pixel Ownership Test：像素所有权测试，控制对当前屏幕像素的使用权限，比如PIE时只对Scene窗口或者Game窗口有使用权限，其他窗口没有使用权限
* Scissor Test：裁剪测试，对可渲染内部部分内容的控制
* Alpha Test：透明度测试，只能实现不透明效果或全透明效果
* Stencil Test：模板测试
* Depth Test：深度测试
* Blending：混合操作，对不同像素的颜色混合

逐片元操作阶段是不可编程的，他是由渲染管线和硬件自身自我规定好的，但是我们可以对其中的内容进行配置（高度可配置）

#### 从逻辑上理解
通过一定条件来判断是对该片元或片元属性执行抛弃操作还是保留操作

![通过一定条件来判断是对该片元或片元属性执行抛弃操作还是保留操作](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/772d5164-f5b3-4675-a5c8-61b956e9a3e3.png)

* referenceValue：当前片元的模板缓冲的参考值
* stencilBufferValue：模板缓冲区里的值
* 中间的comparisonFunction，就是做一个比较

#### 从书面概念上理解

说到模板测试，就要先说到模板缓冲区。
* 模板缓冲区：与颜色缓冲区和深度缓冲区类似，模板缓冲区可以为屏幕上的每个像素点保存一个无符号整数值(通常的话是个8位整数)。这个值的具体意义视程序的具体应用而定。

* 模板测试：在渲染的过程中，可以用这个值与一个预先设定的参考值相比较，根据比较的结果来决定是否更新相应的像素点的颜色值。这个比较的过程被称为模板测试。模板测试发生在透明度测试（alpha test）之后，深度测试（depth test）之前。如果模板测试通过，则相应的像素点更新，否则不更新。


### 1.1.2 语法表示

``` hlsl
stencil {
    Ref referenceValue
    ReadMask readMask
    WriteMask writeMask
    Comp compareFunction
    Pass stencilOperation
    Fail stencilOperation
    ZFail stencilOperation
}
```

*  Ref：当前片元的参考值（0-255）
*  ReadMask：读掩码
*  WriteMask：写掩码
*  Comp：比较操作函数
*  Pass：测试通过，之后进行操作（StencilOperation，后边有详细讲解）
*  Fail：测试未通过，也会进行一个操作
*  ZFail：模板测试通过，深度测试未通过

#### 比较函数（Comparison Function）

语义    | 解释
---     |---
Greater | 相当于 “>” 操作，即仅当左边 > 右边，模板测试通过，渲染像素
GEqual  | 相当于 “>=” 操作，即仅当左边 >= 右边，模板测试通过，渲染像素
Less    | 相当于 “<” 操作，即仅当左边 < 右边，模板测试通过，渲染像素
LEqual  | 相当于 “<=” 操作，即仅当左边 <= 右边，模板测试通过，渲染像素
Equal   | 相当于 “=” 操作，即仅当左边 = 右边，模板测试通过，渲染像素
NotEqual| 相当于 “!=” 操作，即仅当左边！= 右边，模板测试通过，渲染像素
Always  | 不管公式两边为何值，模板测试总是通过，渲染像素
Never   | 不敢公式两边为何值，模板测试总是失败 ，像素被抛弃


#### 更新值

语义    | 解释
---     |---
Keep    | 保留当前缓冲中的内容，即 stencilBufferValue 不变
Zero    | 将 0 写入缓冲，即 stencilBufferValue 值变为 0
Replace | 将参考值写入缓冲，即将 referenceValue 赋值给 stencilBufferValue
IncrSat | stencilBufferValue 加 1，如果 stencilBufferValue 超过 255 了，那么保留为 255，即不大于 255
DecrSat | stencilBufferValue 减 1，如果 stencilBufferValue 超过为 0，那么保留为 0，即不小于 0
Invert  | 将当前模板缓冲值（stencilBufferValue）按位取反
IncrWrap| 当前缓冲的值加 1，如果缓冲值超过 255 了，那么变成 0，（然后继续自增）
DecrWrap| 当前缓冲的值减 1，如果缓冲值已经为 0，那么变成 255，（然后继续自减）


### 1.1.3 案例

<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/a1bef728-9bbe-43fd-8146-92c1956a6e48.gif" height=240 alt="3D卡牌效果"/>

#### 遮罩 Shader：
``` hlsl
Shader "Custom/StencilMask"
{
    Properties
    {
        // 蒙版ID
        _ID("Mask ID", Int) = 0
    }
    SubShader
    {
        // 渲染类型为不透明物体，队列为Geometry+1（在默认的不透明物体渲染队列之后）
        Tags { "RenderType"="Opaque" "Queue"="Geometry+1" }
        
        // 颜色遮罩，0表示不渲染任何颜色，也就是完全透明
        ColorMask 0 // RGBA、RGB、R、G、B、A、0
        // 关闭深度写入，防止后面要显示的东西被剔除
        ZWrite Off

        // 模板测试部分
        Stencil // input stencil
        {
            Ref [_ID] // 参考值
            Comp Always // 默认always
            Pass Replace // 默认keep
            // Fail Keep
            // ZFail Keep
        }

        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"

            struct appdata
            {
                float4 vertex : POSITION;
            };

            struct v2f
            {
                float4 pos : SV_POSITION;
            };

            v2f vert (appdata v)
            {
                v2f o;
                o.pos = UnityObjectToClipPos(v.vertex);
                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                // 随便给个颜色就行，因为颜色遮罩关闭了，不会输出任何颜色
                return fixed4(1, 1, 1, 1);
            }

            ENDCG
        }
    }
}

```

#### 物体 Shader：
``` hlsl
Shader "Custom/StencilObject"
{
    Properties
    {
        _Color ("Color", Color) = (1,1,1,1)
        _MainTex ("Albedo (RGB)", 2D) = "white" {}
        _Glossiness ("Smoothness", Range(0,1)) = 0.5
        _Metallic ("Metallic", Range(0,1)) = 0.0
        // 模板测试ID
        _ID("Mask ID", Int) = 0
    }
    SubShader
    {
        // 渲染队列在蒙版之后
        Tags { "RenderType"="Opaque" "Queue"="Geometry+2" }
        LOD 200

        Stencil
        {
            Ref [_ID] // 模板缓冲区的索引值
            Comp equal // 当给定的索引值和当前模板缓冲区的值相等时，才会渲染这个片元
        }

        CGPROGRAM
        #pragma surface surf Standard fullforwardshadows

        #pragma target 3.0

        sampler2D _MainTex;

        struct Input
        {
            float2 uv_MainTex;
        };

        half _Glossiness;
        half _Metallic;
        fixed4 _Color;

        UNITY_INSTANCING_BUFFER_START(Props)
        UNITY_INSTANCING_BUFFER_END(Props)

        void surf (Input IN, inout SurfaceOutputStandard o)
        {
            fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color;
            o.Albedo = c.rgb;
            o.Metallic = _Metallic;
            o.Smoothness = _Glossiness;
            o.Alpha = c.a;
        }
        ENDCG
    }
    FallBack "Diffuse"
}
```


### 总结
* 使用模板缓冲区最重要的两个值：当前模板缓冲值（stencil Buffer Value）和模板参考值（reference Value）
* 模板测试主要是对这两个值使用特定的比较操作：Never、Always、Less、LEqual、Greater、Equal
* 模板测试之后要对模板缓冲区的值（stencil Buffer Value）进行更新操作，更新操作包括：Keep、Zero、Replace、IncrSat、DecrSat、Invert 等等
* 模板测试之后可以根据结果对模板缓冲做不同的更新操作，比如模板测试成功操作 Pass，模板测试失败操作 Fail，深度测试失败操作 ZFail，还有正对正面和背面精确更新操作 PassBack，PassFront，FailBack 等等


### 扩展
掌握模板测试的核心思想，利用模板测试的特性同其他测试或图形算法结合使用

![模板测试扩展](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240929195259.jpg)



## 1.2 深度测试 (Depth Test)
深度测试主要用于处理物体的遮挡关系。

![深度测试](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240929195726.png)

### 1.2.1 深度测试是什么

#### 从渲染管线出发

在逐片元操作之中，模板测试之后，透明度混合之前进行的，同样是高度可配置的
其中在片元着色器之前还有一个 Early-Z 技术，利用了与深度测试相同的技术

#### 从逻辑上理解

``` hlsl
// 如果ZWrite On，且当前深度值和深度缓冲区的值作比较，如果通过就写入深度，不通过就忽略深度
if(Zwrite On && (correntDepthValue ComparisonFunction DepthBufferValue)){
		写入深度
}else{
		忽略深度
}
```
``` hlsl
// 当前深度值和深度缓冲区中的值做比较，如果通过就写入颜色缓冲区，不通过就不写入颜色缓冲区
if(correntDepthValue ComparisonFunction DepthBufferValue){
		写入深度缓冲
}else{
		不写入深度缓冲
}
```

#### 从书面概念上理解
所谓深度测试，就是针对当前对象在屏幕上（更准确的说是 frame buffer）对应的像素点，将对象自身的深度值与当前该像素点缓存的深度值进行比较，如果通过了，本对象在该像素点才会将颜色写入颜色缓冲区，否则否则不会写入颜色缓冲

#### 深度测试的发展

![深度测试的发展](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240929201206.png)

1. 控制渲染顺序
   * 画家算法：
      * 这里是指油画的画法，也就是画一幅油画，是从远处开始画，然后近处的东西一点点叠加在上面（GAMES系列的课提到过多次）
      * 存在的问题：例如一列物体，最前面的物体最大，站在正前面看只能看到最前面的物体，这样一来后边的就不用画了，不然就是性能浪费（OverDraw）。
   * Z-Buffer算法：通过深度缓冲区来控制渲染顺序
2. 控制Z-Buffer对深度的存储
   * 例如：什么时候更新深度缓冲区、什么时候使用深度缓冲区
   * 两个典型的功能： Z Test、Z Write
3. 控制不同类型物体的渲染顺序
   * 透明物体
   * 不透明物体
   * 渲染队列（很有用的概念，后边会讲）
4. 减少OverDraw
   * Early-Z，一种优化手段，后边会讲
      * Z-cull（优化手段）
      * Z-check（确认正确遮挡关系）


### 1.2.2 深度缓冲区（Z-Buffer）

* 深度缓冲就像颜色缓冲（储存所有的片段颜色：视觉输出）一样，在每个片段中储存了信息，并且（通常）和颜色缓冲有着一样的宽度和高度。
* 深度缓冲是由窗口系统自动创建的，它会以16、24或32位float的形式储存它的深度值。在大部分的系统中，深度缓冲的精度都是24位的
* **z-buffer中存储的是当前的深度信息，对于每个像素存储一个深度值**
* 我们可以通过Z-Write 、Z-Test来调用Z-Buffer，来达到想要的渲染效果

### 1.2.3 深度写入（ZWrite）
深度写入包括两种状态：ZWrite On 与 ZWrite Off

当我们开启深度写入的时候，物体被渲染时针对物体在屏幕（更准确地说是frame buffer）上每个像素的深度都写入到深度缓冲区；反之，如果是 ZWrite Off，那么物体的深度就不会写入深度缓冲区。

但是，物体是否会写入深度，除了 ZWrite 这个状态之外，更重要的是需要深度测试通过，也就是 ZTest 通过，如果 ZTest 都没通过，那么也就不会写入深度了

ZTest 分为通过和不通过两种情况，ZWrite 分为开启和关闭两种情况的话，一共就是四种情况：
1. 深度测试通过，深度写入开启：写入深度缓冲区，写入颜色缓冲区；
2. 深度测试通过，深度写入关闭：不写深度缓冲区，写入颜色缓冲区；
3. 深度测试失败，深度写入开启：不写深度缓冲区，不写颜色缓冲区；
4. 深度测试失败，深度写入关闭：不写深度缓冲区，不写颜色缓冲区；


### 1.2.4 深度测试（ZTest）

#### 比较操作

ZTest 状态  | 描述
---         | ---
Greater     | 深度大于当前缓存则通过
LEqual      | 深度小于等于当前缓存则通过
Less        | 深度小于当前缓存则通过
GEqual      | 深度大于等于当前缓存则通过
Equal       | 深度等于当前缓存则通过
NotEqual    | 深度不等于当前缓存则通过
Always（Off）| 深度不论如何都通过
Never       | 深度不论如何都不通过

**默认是ZWrite On 和ZTest Lequal，深度缓存一开始为无穷大**


### 1.2.5 渲染队列
Unity中内置的渲染队列，按照渲染顺序，从先往后进行排序，**队列数越小的越先渲染，队列数越大的越后渲染**
* BackGround（1000）：最早被渲染的物体的队列。
* Geometry（2000）：不透明物体的渲染队列。大多数物体都应该使用该队列进行渲染，也是Unity默认的渲染队列。
* AlphaTest（2450）：有透明通道，需要进行 Alpha Test 的物体的队列，比在 Geometry 中更有效。
* Transparent（3000）：半透明物体的渲染队列。一般是不写入深度的物体，Alpha Blend 等的物体在该队列中渲染。
* Overlay（4000）：最后被渲染的物体的队列，一般是覆盖效果，比如镜头光晕，屏幕贴片之类的。

在Unity中设置渲染队列：`Tags { “Queue” = “Transparent”}`，默认是 Geometry

* **不透明物体的渲染顺序：从前往后**
* **透明物体的渲染顺序：从后往前（OverDraw）**

可以在 shader 的 Inspector 面板查看相关属性

![在 shader 的 Inspector 面板查看相关属性](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240930121734.png)


### 1.2.6 Early-Z 技术
传统的渲染管线中，ZTest其实是在Blending阶段，这时候进行深度测试，所有对象的像素着色器都会计算一遍，没有什么性能提升，仅仅是为了得出正确的遮挡结果，会造成大量的无用计算，因为每个像素点上肯定重叠了很多计算。
因此 **现代 GPU 中运用了Early-Z的技术，在 Vertex 阶段和 Fragment 阶段之间（光栅化之后，fragment 之前）进行一次深度测试**，如果深度测试失败，就不必进行 fragment 阶段的计算了，因此在性能上会有很大的提升。但是最终的 ZTest 仍然需要进行，以保证最终的遮挡关系结果正确。

前面的一次主要是 Z-Cull 为了裁剪以达到优化的目的，后一次主要是 Z-Check，为了检查，如下图：

![3e2d4fed-438e-4b20-b608-8404d5f61c7f](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/3e2d4fed-438e-4b20-b608-8404d5f61c7f.jpg)
![0d609c6a-65c7-46ef-a69b-c57875effe1e](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/0d609c6a-65c7-46ef-a69b-c57875effe1e.png)


### 1.2.7 深度值

模型在渲染管线中的几次空间变换

![深度值](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20240930123132.jpg)

* 模型一开始所在的模型空间：无深度。
* 通过M矩阵变换到世界空间，此时模型坐标已经变换到了齐次坐标（x，y，z，w）：深度存在z分量。
* 通过V矩阵变换到观察空间（摄像机空间）：深度存在z分量（线性）
* 通过P矩阵变换到裁剪空间：深度缓冲中此空间的z/w中（已经变成了非线性的深度）
* 最后通过一些投影映射变换到屏幕空间 


#### 为什么深度缓冲区中要存储一个非线性的深度？
参考链接：https://learnopengl-cn.readthedocs.io/zh/latest/04%20Advanced%20OpenGL/01%20Depth%20testing/

1.给近处更多的精度

* 在精度有限的深度缓冲区中合理分配存储到精度，**较近区域的深度信息分配更高的精度，较远区域的深度信息分配更低的精度**（和gamma映射异曲同工）

* 正确的投影特性的非线性深度方程是和 1/Z 成正比的，这样基本做的是在Z很近的时候高精度和Z很远的时候低精度。例如 1.0 和 2.0 之间的 z 值，将变为 1.0 到 0.5 之间，这样在 z 很小的时候给了我们很高的精度，而 50.0 和 100.0 之间的 z 值将只占 2% 的浮点数的精度，这正是我们想要的。


2.防止深度冲突

* **深度值的精度不足可能会带来深度冲突的问题**。所以在深度缓冲区中使用非线性深度，在近处用更高精度的数据存储，可以有效减少近处的深度冲突；而远处的因为不容易被看见，即便发生深度冲突也不会造成太多影响

* 深度冲突无法避免，但是有技巧可以防止出现：
    * 让物体之间不要离得太近
    * 尽可能把近平面设置得远一些，可以有效在椎体中提高精度
    * 牺牲性能，使用更高精度的深度值（24→32位）



### 案例

* 先对渲染队列进行排列，再在同一个渲染队列中按深度进行排序
* 对于多pass shader，unity会从所以pass的所有的queue中挑选最靠前的，然后把这个物体放到一个队列中，然后他会根据pass的顺序逐pass执行

X-Ray Shader：
``` hlsl
Shader "Custom/XRay"
{
    Properties
    {
        _MainTex ("Texture", 2D) = "white" {}
        _XRayColor ("XRay Color", Color) = (1,1,1,1)
    }
    SubShader
    {
        // XRay Pass
        Pass
        {
            Tags { "RenderType"="Transparent" "Queue"="Transparent" }
            Blend SrcAlpha One
            ZTest Greater // 核心
            ZWrite Off
            Cull Back

            CGPROGRAM
            #include "UnityCG.cginc"

            fixed4 _XRayColor;

            struct appdata
            {
                float4 vertex : POSITION;
                float3 normal : NORMAL;
            };

            struct v2f
            {
                float4 vertex : SV_POSITION;
                fixed4 color : TEXCOORD0;
            };

            v2f vert(appdata v)
            {
                v2f o;

                o.vertex = UnityObjectToClipPos(v.vertex);

                float3 normal = normalize(v.normal);
                float3 viewDir = normalize(ObjSpaceViewDir(o.vertex));
                fixed fresnel = 1 - dot(normal, viewDir);

                o.color = _XRayColor * fresnel;

                return o;
            }

            fixed4 frag(v2f i) : SV_Target
            {
                return i.color;
            }

            #pragma vertex vert
            #pragma fragment frag

            ENDCG
        }


        // 正常 Pass
        Pass
        {
            Tags { "RenderType"="Opaque" }
            ZTest LEqual
            ZWrite On

            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag

            #include "UnityCG.cginc"

            struct appdata
            {
                float4 vertex : POSITION;
                float2 uv : TEXCOORD0;
            };

            struct v2f
            {
                float2 uv : TEXCOORD0;
                float4 vertex : SV_POSITION;
            };

            sampler2D _MainTex;
            float4 _MainTex_ST;

            v2f vert (appdata v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                o.uv = TRANSFORM_TEX(v.uv, _MainTex);
                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                fixed4 col = tex2D(_MainTex, i.uv);
                return col;
            }
            ENDCG
        }
    }
}
```

### 总结
* 使用深度缓冲区最重要的两个值：当前深度缓冲值 ZBufferValue 和深度参考值 referenceValue，并通过比较操作获取理想的渲染效果
* Unity 中的渲染顺序：先渲染不透明物体，顺序是从前往后；再渲染透明物体，顺序是从后到前
* 通过 ZWrite 和 ZTest 组合使用控制半透明物体的渲染
* 引入 Early-Z 技术后的深度测试相关渲染流程
* 深度缓冲区中存储的深度值为0到1范围的浮点值，且为非线性

### 扩展

![深度测试扩展](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241010192122.jpg)

### 参考资料
https://blog.csdn.net/puppet_master/article/details/53900568
https://learnopengl-cn.readthedocs.io/zh/latest/04%20Advanced%20OpenGL/01%20Depth%20testing/
https://docs.unity3d.com/cn/2018.4/Manual/SL-CullAndDepth.html
https://blog.csdn.net/yangxuan0261/article/details/79725466
《Unity ShaderLab 开发实战详解》
《Unity Shader 入门精要》
https://roystan.net/articles/toon-water.html



-------------------------------------------------------


# 2. 混合模式及剔除

## 2.1 混合模式

参考文档：
https://docs.unity3d.com/cn/2018.4/Manual/SL-Blend.html


### 2.1.1 定义

混合（Blend）就是**把两种颜色混在一起**。具体就是把某一像素位置原来颜色和将要会上去的颜色，通过某种方式或者算法混在一起，从而实现新的效果。

例如：PS内的正片叠底，叠加都属于混合模式的一种。

![混合](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241209195708.png)

### 2.1.2 混合模式的公式

![混合模式公式](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241209195845.png)

最终颜色 = Shader计算后的颜色值(当前计算出来的颜色，Output) * **源因子(SrcFactor)** + 累积颜色(颜色缓冲区中的颜色) * **目标因子(DstFactor)**.

累计颜色可以理解为渲染当前物体后面的颜色即GBuffer中的像素（颜色缓冲区中的颜色）。

混合模式控制的就是源因子和目标因子，在脚本里会看到的就是 ：Blend SrcFactor DstFactor。

### 2.1.3 ShaderLab 内的混合

参考笔记：[Shader 入门精要 初级篇笔记-透明度混合](https://github.com/Ineloquent0/notes/blob/main/TA%20%E7%AC%94%E8%AE%B0/Unity%20Shader%20%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81/02%20%E5%88%9D%E7%BA%A7%E7%AF%87.md#84-%E9%80%8F%E6%98%8E%E5%BA%A6%E6%B7%B7%E5%90%88)


### 2.1.4 混合模式实现方式

``` hlsl
//正常
float3 Normal(float3 Src, float3 Dst)
{
    Dst = 0.;
    return Src.rgb + Dst.rgb;
}

float3 Alphablend(float4 Src, float4 Dst)
{
    float4 C = Src.a * Src + (1.0 - Src.a) * Dst;
    return C.rgb;
}

//变暗
float3 Darken(float3 Src, float3 Dst)
{
    return min(Src, Dst);
}

//正片叠底
float3 Multiply(float3 Src, float3 Dst)
{
    return Src * Dst;
}

//颜色加深
float3 ColorBurn(float3 Src, float3 Dst)
{
    return 1.0 - (1.0 - Dst) / Src;
}

// 线性加深
float3 LinearBurn(float3 Src, float3 Dst)
{
    return Src + Dst - 1.0;
}

//深色
float3 DarkerColor(float3 Src, float3 Dst)
{
    return(Src.x + Src.y + Src.z < Dst.x + Dst.y + Dst.z) ? Src : Dst;
}

//变亮
float3 Lighten(float3 Src, float3 Dst)
{
    return max(Src, Dst);
}

//滤色
float3 Screen(float3 Src, float3 Dst)
{
    return Src + Dst - Src * Dst;
}

//颜色减淡
float3 ColorDodge(float3 Src, float3 Dst)
{
    return Dst / (1.0 - Src);
}

//线性减淡
float3 LinearDodge(float3 Src, float3 Dst)
{
    return Src + Dst;
}

//浅色
float3 LighterColor(float3 Src, float3 Dst)
{
    return(Src.x + Src.y + Src.z > Dst.x + Dst.y + Dst.z) ? Src : Dst;
}

//叠加
float overlay(float Src, float Dst)
{
    return(Dst < 0.5) ? 2.0 * Src * Dst : 1.0 - 2.0 * (1.0 - Src) * (1.0 - Dst);
}

//柔光
float SoftLight(float Src, float Dst)
{
    return(Src < 0.5) ? Dst - (1.0 - 2.0 * Src) * Dst * (1.0 - Dst)
    : (Dst < 0.25) ? Dst + (2.0 * Src - 1.0) * Dst * ((16.0 * Dst - 12.0) * Dst + 3.0)
    : Dst + (2.0 * Src - 1.0) * (sqrt(Dst) - Dst);
}

float3 SoftLight(float4 Src, float4 Dst)
{
    float3 C;
    C.x = SoftLight(Src.x, Dst.x);
    C.y = SoftLight(Src.y, Dst.y);
    C.z = SoftLight(Src.z, Dst.z);
    return C;
}

//强光
float HardLight(float Src, float Dst)
{
    return(Src < 0.5) ? 2.0 * Src * Dst : 1.0 - 2.0 * (1.0 - Src) * (1.0 - Dst);
}

float3 HardLight(float3 Src, float3 Dst)
{
    float3 C;
    C.x = HardLight(Src.x, Dst.x);
    C.y = HardLight(Src.y, Dst.y);
    C.z = HardLight(Src.z, Dst.z);
    return C;
}

//亮光
float VividLight(float Src, float Dst)
{
    return(Src < 0.5) ? 1.0 - (1.0 - Dst) / (2.0 * Src) : Dst / (2.0 * (1.0 - Src));
}

float3 VividLight(float3 Src, float3 Dst)
{
    float3 C;
    C.x = VividLight(Src.x, Dst.x);
    C.y = VividLight(Src.y, Dst.y);
    C.z = VividLight(Src.z, Dst.z);
    return C;
}

// 线性光
float3 LinearLight(float3 Src, float3 Dst)
{
    return 2.0 * Src + Dst - 1.0;
}

//点光
float PinLight(float Src, float Dst)
{
    return(2.0 * Src - 1.0 > Dst) ? 2.0 * Src - 1.0 : (Src < 0.5 * Dst) ? 2.0 * Src : Dst;
}

float3 PinLight(float3 Src, float3 Dst)
{
    float3 C;
    C.x = PinLight(Src.x, Dst.x);
    C.y = PinLight(Src.y, Dst.y);
    C.z = PinLight(Src.z, Dst.z);
    return C;
}

//实色混合
float3 HardMix(float3 Src, float3 Dst)
{
    return floor(Src + Dst);
}

//差值
float3 Difference(float3 Src, float3 Dst)
{
    return abs(Dst - Src);
}

//排除
float3 Exclusion(float3 Src, float3 Dst)
{
    return Src + Dst - 2.0 * Src * Dst;
}

//减去
float3 Subtract(float3 Src, float3 Dst)
{
    return Src - Dst;
}

//划分
float3 Divide(float3 Src, float3 Dst)
{
    return Src / Dst;
}

// RGB转HSV
float3 RGB2HSV(float3 C)
{
    float4 K = float4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0);
    float4 p = lerp(float4(C.bg, K.wz), float4(C.gb, K.xy), step(C.b, C.g));
    float4 q = lerp(float4(p.xyw, C.r), float4(C.r, p.yzx), step(p.x, C.r));
    
    float Dst = q.x - min(q.w, q.y);
    float e = 1.0e-10;
    return float3(abs(q.z + (q.w - q.y) / (6.0 * Dst + e)), Dst / (q.x + e), q.x);
}

// HSV转RGB
float3 HSV2RGB(float3 C)
{
    float3 rgb;
    rgb = clamp(abs(fmod(C.x * 6.0 + float3(0.0, 4.0, 2.0), 6.0) - 3.0) - 1.0, 0.0, 1.0);

    return C.z * lerp(1.0, rgb, C.y);
}

//色相
float3 Hue(float3 Src, float3 Dst)
{
    Dst = RGB2HSV(Dst);
    Dst.x = RGB2HSV(Src).x;
    return HSV2RGB(Dst);
}

//颜色
float3 Color(float3 Src, float3 Dst)
{
    Src = RGB2HSV(Src);
    Src.z = RGB2HSV(Dst).z;
    return HSV2RGB(Src);
}

//饱和度
float3 Saturation(float3 Src, float3 Dst)
{
    Dst = RGB2HSV(Dst);
    Dst.y = RGB2HSV(Src).y;
    return HSV2RGB(Dst);
}

//明度
float3 Luminosity(float3 Src, float3 Dst)
{
    float dLum = dot(Dst, float3(0.3, 0.59, 0.11));
    float sLum = dot(Src, float3(0.3, 0.59, 0.11));
    float lum = sLum - dLum;
    float3 C = Dst + lum;
    float minC = min(min(C.x, C.y), C.z);
    float maxC = max(max(C.x, C.y), C.z);
    if (minC < 0.0)
        return sLum + ((C - sLum) * sLum) / (sLum - minC);
    else if (maxC > 1.0)
        return sLum + ((C - sLum) * (1.0 - sLum)) / (maxC - sLum);
    else
        return C;
}

```


## 2.2 剔除
* **法线剔除**：也被称为背面消隐，根据法线朝向判断哪个面被剔除掉。可以用来控制是否双面渲染。
语法：Cull Off/ Front / Back

* **面裁剪**：clip函数会将参数小于某个值的片元直接丢弃，常用于制作溶解，裁剪等效果。
语法：clip()；默认会切掉小于0.5的部分。
    * Clip()，在某些PowerVR机型上效率很低，最好在AlphaTest队列中使用



------------------------------------------------------------------------


# 3. 曲面细分与几何着色器

## 3.1 应用

### 曲面细分着色器的应用

相较于直接增加模型的面数，曲面细分着色器可以在游戏进行时，根据自定义的规则（距离等），动态调整模型的复杂度，可以带来更好的性能。

![曲面细分应用](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241017105058.jpg)

### 几何着色器的应用

* 几何动画

<div align=center>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/4cf6b995-469a-40ff-baf6-518a247da6be.gif" alt="几何动画"/>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/80e0933d-9cbe-4826-8ca7-0ff280496339.gif" alt="几何动画"/>
</div>

* 草地（与曲面细分着色器结合，动态调整草地的疏密）

![草地](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241017105744.jpg)


## 3.2 着色器执行顺序

![着色器执行顺序](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/2bb24a05-c801-4cf0-8809-b377c2fb2ad7.png)

整体顺序：顶点 → 曲面细分 → 几何 → 片元

曲面细分又分为：Hull shader 、Tessellation Primitive Generator 、 Domain shader
* Hull Shader：可编程，定义细分的参数
* Tessellation Primitive Generator：不可编程不可控制
* Domain Shader：曲面细分得到的点是在重心空间的，需要在Domain Shader中将点转换到我们要用的空间中



## 3.3 TESS

### TESS 的输入与输出
**输入**：Patch，可以看成是多个顶点的集合，包含每个顶点的属性，可以指定一个 Patch 包含的顶点数以及自己的属性
**功能**：将图元细分(可以是三角形，矩形等)
**输出**：细分后的顶点

### TESS 流程
* HULL Shader
    1. 决定细分的数量(设定 Tessellation factor 以及 Inside Tessellation factor)
    2. 对输入的 Patch 参数进行改变(如果需要)
* Tessellation Primitive Generation：进行细分操作
* Domain Shader：对细分后的点进行处理，从重心空间(Barycentric coordinate system)转换到屏幕空间


### HULL Shader 各参数解析

参考：https://zhuanlan.zhihu.com/p/94646062

**Tessellation Factor**：决定将一条边分成几部分
* **equal_Spacing**：均匀分割，每一部分的长度相同
    ![equal_Spacing](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/equal_Spacing.gif)
* **fractional_even_spacing**：段数为向上取最近的偶数，最小值是2，会把周长分为n-2的等长部分、以及两端不等长的部分（两端部分和小数有关）
    ![fractional_even_spacing](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/fractional_even_spacing.gif)
* **fractional_odd_spacing**：段数为向上取最近的奇数，最小值是1，会把周长分为n-2的等长部分、以及两端不等长的部分（两端部分和小数有关）
    ![fractional_odd_spacing](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/fractional_odd_spacing.gif)


**Inner Tessellation Factor**：控制内部图形的切割方式

![图形内部切割方式](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241017113639.png)


## 3.4 GS
#### 输入与输出
**输入**：图元（三角形、矩形、线等），据图元的不同，shader 中会出现对应不同数量的顶点
**输出**：输出同样为图元，一个或多个，需要自己从定点构建，顺序很重要；同时需要定义最大输出的顶点数


------------------------------------------------------------------------------------


# 4. 延迟渲染原理介绍

## 4.1 渲染路径
渲染路径是决定光照的实现方式，简而言之就是当前渲染目标使用光照的流程

## 4.2 渲染方式

前向渲染和延迟渲染对比

![前向渲染和延迟渲染对比](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241123190425.jpg)

### 4.2.1 前向渲染（Forward Rendering）

**流程**：待渲染几何体 → 顶点着色器 → 片元着色器 → 渲染目标

在渲染每一帧时，**每个顶点/片元都要执行一次片元着色器代码**，这时需要将所有的光照信息都传递到片元着色器中。
虽然大部分情况下的光源都趋向于小型化，而其照亮的区域也不大，但即便是光源离这个像素所对应的世界空间中的位置很远，但计算光照时，还是会把所有的光源都考虑进去。
例如，物体受n个光源影响，那么在每一个片元执行着色器代码时，都必须吧这n个光源都传递进着色器中执行光照计算。

伪代码：

```
For Each light:
	For each object affected by light:
		framebuffer += object * light
```

### 4.2.2 延迟渲染（Deferred Rendering）

延迟渲染是主要**解决大量光照渲染的方案**。

延迟渲染的实质是先不要做迭代三角形做光照计算，而是**先找出来你能看到的所有像素，再去迭代光照**。直接迭代三角形的话，由于大量三角形你是看不到的，无疑是极大的浪费。

**流程**：待渲染几何体 → 顶点着色器 → MRT → 光照计算 → 渲染目标

**将渲染过程拆分成两个渲染通路（pass）**：
* 第一个 pass 称为**几何处理通路**。首先将场景渲染一次，获取到待渲染对象的各种几何信息存储到名为 G-buffer 的缓冲区中，这些缓冲区将会在之后用作更复杂的光照计算。由于有深度测试，所以最终写入 G-buffer 中的各个数据都是离摄像机最近的片元的几何属性，这意味着最后在 G-buffer 中的片元必定要进行光照计算的。
* 第二个 pass 称为**光照处理通路**。该 pass 会遍历所有 G-buffer 中的位置、颜色、法线等参数，执行一次光照计算。

> * 实际上 Fragment Shader 也是输出了片元颜色的，只不过没有进行光照计算
> * 延迟渲染无法支持半透明物体的渲染，在延迟渲染管线下渲染半透明物体，只能是在延迟渲染处理完成之后，最后再用前向渲染的方式去渲染半透明物体
> * G-Buffer 中的数据都是 2D 的，所以最终的光照计算相当于 2D 的光照后处理

伪代码：

```
For each object:
    Render to multiple targets
For each light:
    Apply light as a 2D postprocess
```


### 4.2.3 不同渲染路径的优劣及特性

#### 不同渲染路径的特性
1. **后处理方式不同**：如果需要深度信息来进行后处理，前向渲染就需要单独渲染出一张深度图，而延迟渲染直接从 G-Buffer 中拿深度图即可
2. **着色计算不同(shader)**：由于延迟渲染光照计算统一是在 LightPass 中完成的，所以只能计算一个光照模型。如果需要其他的光照模型，只能切换 Pass
3. **抗锯齿方式不同**

#### 不同渲染路径的优劣

**前向渲染**
* 优点：
    * 支持半透明渲染
    * 支持使用多个光照 pass
    * 支持自定义光照计算方式
* 缺点：
    * 光源数量对计算复杂度影响巨大
    * 访问深度等数据需要额外计算

**延迟渲染**
* 优点：
    * 大量光照场景优势明显
    * 只渲染可见像素，节省计算量
    * 对后处理支持良好
    * 用更少的 shader
* 缺点：
    * 对MSAA支持不友好
    * 透明物体渲染存在问题
    * 占用大量的显存带宽


## 4.3 其他

#### 渲染路径设置

![渲染路径设置](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241123210004.png)

#### 移动端优化

两个 TBDR 的优化方式
* SIGGRAPH2010 上提出的，通过分块来解决降低带宽内存用量
    ![dde48c5b-f787-4dc1-bcc6-21bd36f7ac57](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/dde48c5b-f787-4dc1-bcc6-21bd36f7ac57.png)
* PowerVR 基于手机 GPU 的 TBR 架构提出的，通过 HSR 减少 overdraw（做一些可见性测试）
    ![15d7f7b0-4c90-448e-8a11-4ae1d0b305ed](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/15d7f7b0-4c90-448e-8a11-4ae1d0b305ed.png)

#### 其他渲染路径
参考资料：https://zhuanlan.zhihu.com/p/54694743

* 延迟光照 （Light Pre-Pass / Deferred Lighting）
    减少 G-buffer 占用的过多开销，支持多种光照模型
    ![8ffc1ba4-9df1-4b82-919f-f62e1d0a6245](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/8ffc1ba4-9df1-4b82-919f-f62e1d0a6245.png)

* Forward+（即Tiled Forward Rendering，分块正向渲染）
    减少带宽，支持多光源，强制需要一个preZ
    ![6e723b53-eeaa-4b76-86f9-4b65073e64d0](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/6e723b53-eeaa-4b76-86f9-4b65073e64d0.png)

* 群组渲染 Clustered Rendering
    带宽相对减少，多光源下效率提升

#### MSAA 与延迟渲染的不兼容
MSAA 在延迟渲染中的问题是：像素在进行光照计算前已经被光栅化了，MSAA 需要基于 sub-pixel 数据进行处理，光栅化后的每个像素的 sub-pixel 们是一样的，处理无效，所以没有办法用精度更高的像素来进行渲染。

![https://catlikecoding.com/unity/tutorials/rendering/part-13/](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/2c5c3413-e11d-403c-af8f-441345cb4e8d.png)

参考资料：
https://docs.microsoft.com/en-us/windows/win32/direct3d9/multiple-render-targets
https://docs.nvidia.com/gameworks/index.html#gameworkslibrary/graphicssamples/d3d_samples/antialiaseddeferredrendering.htm


#### 不同path下光源shader编写

![不同path下光源shader编写](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/2f79032b-14fb-430e-adc8-d0250022b9f1.png)


#### PreZ/Zprepass

实际上就是一个深度计算，与深度图一样都是用一个 pass 去计算深度。PreZ 与深度图的区别在于，深度图是将深度信息绘制到了一张 RT 上，为了记录数据并方便进行数据间的传输；而 PreZ/Zprepass 是硬件自动进行的，只是计算深度然后在 shader 中使用（做屏幕等距边缘光的时候用到了），或者透明排序的时候会涉及三段深度排序的方式，来让透明的物体深度正确。

如果在 shader 中要用到 DepthOnlyPass 获取到的深度图的话，需要勾选 Depth Texture 的生成 —— 屏幕空间等距边缘光

![12d67842-10a4-4882-b30c-819e06368e45](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/12d67842-10a4-4882-b30c-819e06368e45.png)


### 参考资料
https://zhuanlan.zhihu.com/p/28489928
https://zhuanlan.zhihu.com/p/259760974
https://www.3dgep.com/forward-plus/#Forward
https://gamedevelopment.tutsplus.com/articles/forward-rendering-vs-deferred-rendering--gamedev-12342



----------------------------------------------------------------------------


# 5. Early-Z 和 Z-Prepass

## 5.1 深度测试（Depth Test）

#### 传统渲染管线中的深度测试

![传统渲染管线中的深度测试](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241129205030.png)


#### 为什么做深度测试

深度测试可以解决：物体的可见遮挡性问题

<div align=center>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/8e2ac7c7-0090-4915-848e-501a37b22366.png" alt="8e2ac7c7-0090-4915-848e-501a37b22366" />
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/858c5a79-6782-45d5-bb46-449a9acfcf2d.png" alt="858c5a79-6782-45d5-bb46-449a9acfcf2d" />
</div>

深度缓冲区默认值为∞，当渲染一个物体时，它与深度缓冲区中的值做比较，如果它的值符合深度测试(ZTest)条件，那么它才会被渲染，否则它就被丢弃。

例如上图：
* 首先先渲染紫色三角形，紫色三角形的深度值为5，当渲染它的时候，它与深度缓冲区中的∞做比较，因为默认的test比较条件为LEqual，所以5小于∞，并且写入了深度缓冲器
* 之后进行黄色三角形的渲染，和上一步同理，会进行深度对比并进行相关操作，渲染完成后的结果如图中右下角所示


#### 深度测试流程

![深度测试流程](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/c9ac637e-6930-4aa2-8f64-f66d2d9256ca.png)


#### 伪代码

```
for(each triangle T){                       //对每一个三角形	
    for(each fragment(x,y,z) in T){	        //对每一个三角形中的片元    	
        if(fragment.z < ZBuffer[x,y]){	    //深度测试：如果片元深度小于ZBuffer深度       
            FrameBuffer[x,y] = fragment.rgb;//更新颜色            
            ZBuffer[x,y] = fragment.z;		//更新深度        
        }        
        else{					            //深度测试失败        	
            discard;					    //什么都不做，片元数据被丢弃        
        }    
    }
}
```

#### 深度测试带来的问题

![深度测试带来的问题](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/5516bc50-3d4a-44d6-b8b6-422df48e38e9.png)

物体2与物体3会被物体1遮挡，所以它们计算得到的片元都将会被丢弃，会造成性能问题。

这个问题的解决方法就是：early-z


## 5.2 提前深度测试 Early-Z

在传统管线中的光栅化阶段之后、片元着色器之前加一步操作，如果没有通过深度测试的物体就直接舍弃，不进行后续计算。

![提前深度测试](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241129211916.png)

![6570d5ae-d23b-458e-b2b0-6f27159b223b](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/6570d5ae-d23b-458e-b2b0-6f27159b223b.png)

在Early-Z先计算了物体1的片元情况下，物体2与物体3将不会进行片元计算

补充：Early-Z同样可以搭配使用模板测试

### Early-Z 失效的情况

1. 开启 Alpha Test 或 clip/discard 等手动丢弃片元操作
2. 手动修改 GPU 插值得到的深度
3. 开启 Alpha Blend
4. 关闭深度测试 Depth Test


### 高效利用Early-Z

![深度测试带来的问题](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/5516bc50-3d4a-44d6-b8b6-422df48e38e9.png)

如果按照 3-2-1 顺序渲染测试，Early-Z 将不会带来任何优化效果

在渲染前，将不透明物体从近往远渲染的话，Early-Z能发挥最大的性能优化
具体怎么排序？ 
* 可以让 cpu 将物体按照由近到远的顺序排好，再交付给 gpu 进行渲染

问题：
* 复杂的场景，cpu性能消耗很大
* 严格按照由近到远的顺序渲染，将不能同时搭配批处理优化手段。

解决方法：Z-Prepass


## 5.3 Z-Prepass

### 方式1：双Pass法
使用两个Pass：
* 在第一个 Pass 即 Z-Prepass 中仅仅只写入深度，不计算输出任何颜色
* 在第二个 Pass 中关闭深度写入，并且将深度比较函数设置为相等。

效果：每个物体都会渲染两个 Pass，且所有物体的 Z-Prepass 的结果就自动形成了一个最小深度值的缓冲区 Z-buffer，无需 CPU 进行排序

``` hlsl
Shader "Custom/EarlyZPresentation" {
    Properties { //.......}
    SubShader {
        Tags { "RenderType"="Opaque" }
        
        //Z-Prepass部分
        Pass
        {
            ZWrite On	//开启深度写入
            ColorMask 0	//关闭颜色输出

            CGPROGRAM
            //...这里省略了单纯地顶点变换计算部分
            ENDCG
        }

        //正常地计算输出颜色的Pass
        Pass
        {
            Zwrite Off	//关闭深度写入
            ZTest Equal	//深度相等为通过

            CGPROGRAM
            //...这里省略了顶点变换和颜色等计算部分
            ENDCG
        }
    }
}
```

#### 问题
* 问题1：动态批处理。多 Pass Shader 无法进行动态批处理 --> Draw Call 问题
* 问题2：Draw Call。使用 Z-Prepass Shader 的物体，Draw Call 会多一倍

![aPr4NF1sP0bn5oL0xVo_0g](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/aPr4NF1sP0bn5oL0xVo_0g.png)


### 方式2：提前分离的 PrePass

仍然使用两个Pass，但：
* 将原先第一个 Pass（即 Z-Prepass）单独分离出来为单独一个 Shader，并先使用这个 Shader 将整个场景的 Opaque 的物体渲染一遍。
* 而原先材质只剩下原先的第二个 Pass，仍然关闭深度写入，并且将深度比较函数设置为相等。

![20241129215839](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241129215839.png)


### Z-Prepass 也是透明渲染的一种解决方案

![Z-Prepass也是透明渲染的一种解决方案](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241129220105.png)


### Z-Prepass 的其他问题

Z-prepass的性能消耗是否能被忽视

国外论坛一位名为lipsryme的老哥做了一项实验：

![4a110aba-e3a3-46d5-a2d9-f04a041d65cf](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/4a110aba-e3a3-46d5-a2d9-f04a041d65cf.png)

* 可以看到，Z-prepass的消耗为2.0ms，而带来的优化只减少了0.3ms（2.7-2.4）
* Z-prepass 是需要根据项目的实际情况来决定是否采用的。

总结有以下建议：
* 当一个有非常多 OverDraw 的场景，且不能很好的将不透明物体从前往后进行排序时，可以考虑使用 Z-prepass 进行优化
* 注意，Z-prepass 会增加 DrawCall，不恰当的使用可能是负优化


## 5.4 Early-Z 和 Z-Prepass的实例应用

### 面片叠加的头发渲染
对于半透明的面片来说，需要从后往前进行排序渲染才能得到正确的透明度混合结果

![面片叠加的头发渲染](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241129222109.png)

### 排序后的头发渲染

![排序后的头发渲染](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/b2a9d1bf-307c-43f1-a8dc-ce6ce951c4ac.png)

Pass 1 - 不透明部分
- 启用 alpha test，仅通过不透明像素
- 禁用背面剔除
- 启用 Z writes，将 Z test 设置为 Less

Pass 2 - 透明背面部分
- 启用 alpha test，通过所有非不透明像素
- 剔除正面多边形
- 禁用 Z writes，将 Z test 设置为 Less

Pass 3 - 透明正面部分
- 启用 alpha test，通过所有非不透明像素
- 剔除背面多边形
- 启用 Z Write，将 Z test 设置为 Less

### 性能改善

![性能改善](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/9d54edcc-828b-4c70-810c-af7549f85be0.png)

使用 Early-Z 剔除
透明度测试开启时 Early-Z 无法使用的解决方案：    
* 使用一个简单的 shader 进行透明度测试形成 Z-Buffer（就是我们上边说的提前分离的Z-Prepass）


### 改善的渲染方案

![改善的渲染方案](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/f5829ba1-19a7-4b5a-82c7-f7d25518a268.png)

Pass 1：准备 Z-Buffer
  * 开启透明度测试
  * 关闭背面剔除
  * 开启深度写入，深度测试设置为less
  * 关闭颜色缓冲区写入
  * 用于一个简单的片元着色器来返回透明度值

Pass 2、Pass 3、Pass 4 参考之前排序后的头发渲染部分，同理


## 5.5 参考资料
https://www.cnblogs.com/ghl_carmack/p/10166291.html 	---深入剖析GPU Early Z优化



--------------------------------------------------------------------------------


# 6. 纹理压缩

## 6.1 什么是纹理压缩
纹理压缩是为了解决内存、带宽问题，专为在计算机图形渲染系统中存储纹理而使用的图像压缩技术；


## 6.2 为什么要纹理压缩
* 纹理压缩格式基于块压缩，能够更快读取像素所属字节块进行解压缩以支持随机访问；
* 图片压缩格式基于整张图片进行压缩，无法直接实现单个像素的解析；
* 图片压缩格式无法被GPU识别，还需要经CPU解压缩成非压缩纹理格式才能被识别；

#### 图片与纹理

* **图片格式**是图片文件的存储格式，通常在磁盘、内存中储存和传输文件时使用；例如：JPG、PNG、GIF、BMP等；
* **纹理格式**是显卡能够直接进行采样的纹理数据格式，通常在向显卡中加载纹理时使用；

#### 纹理管线
![纹理管线](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241208200318.png)

**图片格式**：
* 图片压缩格式是**基于整张图片进行压缩，像素之间解码过程中存在依赖关系**
* 无法实现单个像素级的解析，发挥不了显卡的并行能力
* 并且，无论什么格式在显卡解码后都是RGBA的纹理格式
* 总结：无法减少显存的占用率，且需要CPU解压后才能被GPU读取，增加了CPU的时间和带宽

**纹理格式**：
* **基于块压缩，能够更快读取像素所属字节块进行解压缩以支持随机访问**
* 随机访问：如果渲染一个物体时，需要在某个坐标上采样纹理，那么GPU只需要读取该像素所属固定大小字节块，对其进行解压即可。
* 举例理解：
    * 如果拿到一张贴图，设置纹理压缩格式，CPU会按照我们设定的格式进行压缩，然后传递给GPU读取
    * 如果不设置纹理压缩格式，以图片格式进行，CPU也会进行压缩，但是会压缩为RGBA32格式，但其实这个格式是非常大的，并没有起到压缩的作用

总结：纹理压缩相对正常图片格式，能够直接被GPU采样，发挥GPU强大的并行能力，且优化了带宽问题


## 6.3 常见纹理压缩格式

![常见纹理压缩格式](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241208201841.png)

注：黄色为常用格式

### 非压缩格式
* RGBA8888（RGBA32）：一个像素32位（RGBA通道各占8位），包含A通道，即一个像素消耗4字节（8位等于1个字节）；
* RGBA4444（RGBA16）：一个像素16位，包含A通道，即一个像素消耗2字节；
* RGB888（RGB24）：一个像素24位，无A通道，即一个像素消耗3字节；
* RGB565（RGB16）：一个像素16位，无A通道，即一个像素消耗2字节；

### DXTC 系列
DCTC纹理压缩格式来源于S3公司提出的S3TC算法，基本思想是把4×4的像素块压缩成一个64或128位的数据块，优点为创建了一个固定大小且独立的编码片段，没有共享查找表或其他依赖关系，简化了解码过程；

#### DXT1（也叫做BC1）

![1884db0f-5f10-474d-b405-551c255c8867](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/1884db0f-5f10-474d-b405-551c255c8867.png)

* 它将4\*4像素块压缩成了一个64位数据块，这个64位数据块包含：
    * 两个16位RGB颜色，这两个RGB颜色是4\*4像素块中的两个极端颜色值，可以看到它已经用了16*2，32位的数据
    * 剩余的32位平均分配给了16个像素，每个像素两位作为索引值。

![20241209180751](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241209180751.jpg)

64位分别为：
* 其中32位是：蓝色的A、B两个16位RGB颜色，它们是极端颜色（格式为RGB565）
    * 极端颜色通过线性插值得到的中间颜色：红色的C、D
* 另外32位是：16个像素的颜色索引值，每个像素占2位（可能是 00 01 10 11，可以分别表示上边的A、B、C、D四种颜色）

注意：
* 存储极端颜色值的格式是RGB565，即G通道精度会比另外两个通道更高一些，这是底层所决定的，这就是有些人说把信息放绿通道精度更高的原因
* DXT1是适用于不具有透明度信息或具有1位透明度信息，也就是表示完全透明或完全不透明贴图
    * 对于没有Alpha信息贴图，就像上述过程一样进行压缩
    * 对于有Alpha信息贴图
        * 那它就不会插值出中间两个颜色，只会插值会中间1个颜色，另一个选项则是透明或者完全不透明
        * 每一个像素索引的时候，如果索引到ABC3个颜色，那就表示它是完全不透明的颜色，如果索引到第1那，就表示这一个像素是完全透明。

**DXT1的压缩率：**
我们已知DXT1主要用于没有Alpha信息贴图，它参照对象是RGB24，DXT1总的数据块为64位，一共是4*4，16个像素共用的，所以我们得出它一个像素用了4位数据，它的压缩率为6:1.

#### DXT2

DXT2也称为BC2，它与DXT1的区别在于它增加了Alpha信息，它的颜色信息是和DXT1压缩是一样的模式，它多出了64位，用于记录Alpha信息，但是并没有插值，就是单纯的为每一个像素多出了4位，记录Alpha信息，它的整个数据块变成了128位。

#### DXT4

而DXT4对于Alpha信息，就和颜色信息一样，是通过线性插值所得，它依然是128位，其中它也是记录了两个8位Alpha极端值信息，并且给每一个像素分配了3位的索引值

#### DXT3/DXT5

DXT3/DXT5和DXT2/DXT4的计算是一样的，主要的区别在于，DXT2/DXT4已经完成了颜色与Alpha混合，当透明度发生改变的时候，是直接改变整体的颜色值，不再单独进行复合，而DXT3/DXT5的Alpha信息则是相对独立.

#### DXT2/3/4/5的压缩率：

首先它们针对的是RGBA，RGBA是32位，它压缩之后的数据都是128位的数据块，分配给16个像素，我们可以得出它的压缩比是4：1.

#### 补充

在Unity内贴图类型选为法线后会采用DXTnm压缩格式（基于DXT5），该格式会把法线贴图R通道存入A通道，然后RB通道清除为1，这样可以将法线XY信息分别存入到RGB/A中分别压缩，以获得更高的精度，然后再根据XY构建出Z通道数据；


### ATI 系列

#### ATI1
ATI1为ATI公司开发的纹理压缩格式，也被称为BC4，其每个数据块存储单个颜色的数据通道，以与DXT5中的Alpha数据相同的方式进行编码，常用于存储高度图、光滑度贴图，效果与原始图像基本无差异；

**ATI1的压缩比**：ATI1是针对于单通道的，单通道8位，它的一个数据块一共是64位除以16个像素，可以得出它的压缩比是2：1.

#### ATI2
ATI2也被称为BC5，每一个块中存储两个颜色通道的数据，同上以与DXT5中Alpha数据相同的方式进行编码，相当于存储了两个BC4块

如果是在将法线存储在XY双通道中采用BC5格式压缩，由于每个通道都有自己的索引，因此法线贴图XY信息可以比在BC1中保留更多的保真度，缺点是需要使用两倍内存，也需要更多的带宽才能将纹理传递到着色器中

**ATI2的压缩比**：它是两个通道16位数据块为128位除以16，它的压缩比也是2：1

#### BC6/7
BC6和BC7仅在D3D11级图形硬件中受支持，他们每个块占用16字节，BC7针对8位RGB或RGBA数据，而BC6针对RGB半精度浮点数据，因此BC6是唯一一个可以原生存储HDR的BC格式

* BC6是专门针对HDR（高动态范围）图像设计的压缩算法，压缩比为6:1
* BC7是专门针对LDR（低动态范围）图像设计的压缩算法，压缩比为3:1，该格式用于高质量的RGBA压缩，可以显著减少由于压缩法线带来的错误效果
* 一般使用BC7给端游高质量图像进行压缩。

BC6和BC7的官方原理说明：
https://docs.microsoft.com/zh-cn/windows/uwp/graphics-concepts/bc6h-format
https://docs.microsoft.com/zh-cn/windows/uwp/graphics-concepts/bc7-format

### ETC 系列

上述我们说的DirectX选择了DXTC作为标准压缩格式，那对于OpenGL则选择了爱立信研发的ETC格式，几乎所有的安卓设备都可以支持ETC压缩，所以其在移动平台上被广泛应用

ETC方案与DXTC具有相同特点，将4×4的像素单元压缩成64位数据块，并将像素单元水平或竖直朝向分为两个区块，每个像素颜色等于基础颜色加上索引指向的亮度范围

![20241209184327](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241209184327.png)

ETC的压缩原理：
* 它64位数据块都是分为两个区块进行存储
* 它采用了24位来存储两个颜色信息，分别是两个RGB444*2或者说1个RGB333加一个RGB555，每一个区块中平均下来有一个12位来存储颜色
* 每一个区块中还有16位来存储其中8个像素的像素索引，每一个像素分到了两位
* 每一个区块中还有一个4位亮度索引值

#### ETC1
* 每一个分块中有一个4位亮度索引值，4位可以表示0-15，也就是16个选项，正好可以从16个内置亮度表中选取当前分块需要用哪一个亮度表
* 每一个亮度表示有4个值，每一个像素也有一个2位的亮度索引值，这两位可以表示0-3，可以从这4个亮度值中选择出该像素对应一个亮度的补偿值
* 这个亮度补偿值再叠加到每一个分块中用12位存储哪一个基础颜色上，就得出了当前像素一个的最终颜色。

![20241209185605](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241209185605.png)

**ETC1的压缩率**：ETC1针对的是RGB24的贴图进行压缩，它一共是64位的数据块针对16个像素，每个像素4位，压缩为是6：1.

因为ETC压缩在安卓设备上良好的兼容性以及不错效果，现在大多数安卓平台手游都采用了ETC压缩方案。

但是未来解决ETC1不支持Alpha通道问题，我们可以采用两张纹理混合的方式，来实现带Alpha通道ETC1的压缩。

#### ETC2
ETC1和ETC2的区别：
* ETC1要求长宽位2的幂次贴图，它适用于所有安卓设备，压缩率比较高，但它不适用于带Alpha通道的贴图
* ETC2是ETC1的扩展，它要求长款能够被4整除的贴图，硬件要求OpenGLES3.0和OpenGL4.3以上，它支持Alpha通道压缩，但它内存占用大于ETC1。


### ASTC

ASTC是由ARM和AMD联合开发的纹理压缩格式，ASTC在各项指标上都挺不错
* 优点：可根据不同图片选择不同压缩率的算法，图片不需要为2的幂次，同时支持LDR和HDR
* 缺点：兼容性不够完善且解码时间较长

ASTC也是**基于块的压缩算法**，与BC7类似，其数据块大小固定为128位，不同的是块中的像素数量可变，从4×4到12×12像素都有
* 每一个数据块中存储了两个插值端点，但不一定存储的是颜色信息，也可能是Layer信息，这样可以用来对Normal或Alpha进行更好的压缩
* 对于块中每一个纹素，存储其对应插值端点的权重，存储的权重数量可以少于纹素数量，可通过插值得到每一个纹素的权重值，然后再进行颜色的计算

128位数据块中存储的信息：
* 11位，权重、高度信息、特殊块标识
* 2位，Part数量
* 4位，16中插值端点模式（LDR/HDR、RGB/RGBA）
* 111位，插值端点信息、纹素权重值、配置信息

### PVRTC
PVRTC由Imagination公司专为PowerVR显卡设计，仅支持Iphone、Ipad和部分安卓机

不同于DXTC和ETC这类基于块的算法，PVRTC将图像分为了低频信号和高频信号
* 低频信号由两张低分辨率图像AB组成
* 高频信号则是低精度的调制图像，记录了每个像素混合的权重，解码时AB图像经过双线性插值放大，然后根据调制图像的权重进行混合

**PVRTC的压缩原理**：PVRTC分为4-bpp和2-bpp（bpp = Bit Per Pixel，即每个像素占的位数）

#### 4-bpp
将4*4像素块压缩成了一个64位数据块
* 这64位数据块中首先是包含两张图像A和B，A和B都是在原图像基础上长宽分别压缩了4倍之后的一个低分辨率图像
* 不同模式下每个像素调制数据可以得到不同的混合值，根据这个混合值用A和B混合得出最终颜色值

位数占比：
* 32位的调制数据（2*16）
* 1位的调制标志（也称为模式）
* 15位的颜色A（554或4433），1位颜色A的不透明标志
* 14位颜色B（555或4443），1位颜色B的不透明标志

压缩率：
* 以RGB为参照标准：压缩率 = 24 / (64/16) = 6:1
* 以RGBA为参照标准：压缩率 = 32 / (64/16) = 8:1

2-bpp：把一个8×4的像素单元压成了64位数据块


## 6.4 实际应用中的选择

#### 画质比较
RGBA > ASTC 4×4 > ASTC 6×6 > ETC2 ≈ ETC1

#### 压缩比

压缩格式 | 压缩比
--- | ---
DXT1 | 6：1
DXT2/3 | 4：1
DXT4/5 | 4：1
ATI1 | 4：1
ATI2 | 4：1
BC6 | 6：1
BC7 | 3：1
PVRTC | 6：1
ASTC | 4：1~35.95：1


### 总结

PC：
* 低质量使用DXT1格式不支持A通道，使用DXT5格式支持A通道
* 高质量使用BC7格式，支持A通道

安卓：
* 低质量使用ETC1格式，但不支持A通道
* 低质量使用ETC2格式，支持A通道，需要在OpenGL ES 3.0/OpenGL 4.3以上版本
* 高质量使用ASTC格式，需要在Android 5.0/OpenGL ES 3.1以上版本

IOS：
* 高质量使用ASTC格式，需要Iphone6以上版本
* 低质量使用PVRTC2格式，支持Iphone6以下版本


英伟达和Unity官方对于不同类型贴图给出了不同的压缩方案建议，感兴趣的同学可以看下：
[Using ASTC Texture Compression for Game Assets | NVIDIA Developer](https://developer.nvidia.com/astc-texture-compression-for-game-assets)
[Unity - Manual: Recommended, default, and supported texture compression formats, by platform (unity3d.com)](https://docs.unity3d.com/2021.1/Documentation/Manual/class-TextureImporterOverride.html)



---------------------------------------------------------------------------------


# 7. 移动端TB(D)R架构基础 

## 7.1 当前移动端 GPU 概况

虽然移动端gpu型号很多，但市场上的厂家主要就三家：adrno、Mali、PowerVR

![当前移动端 GPU 概况](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/e48e255477a3da6b01602001f658f970.jpg)

### 各类电子设备功耗对比
* 桌面级主流性能平台，功耗一般为300W（R7/I7+X60级别显卡），游戏主机150-200W
* 入门和旗舰游戏本平台功耗为100W
* 主流笔记本为50-60W，超极本为15-25W，旗舰平板为8-15W
* 旗舰手机为5-8W，主流手机为3-5W。

![cc49c46cedfa437c8c99da59b8210483](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/cc49c46cedfa437c8c99da59b8210483.jpg)

### 桌面和移动端带宽比较

![桌面和移动端带宽比较](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/c7d49c92-590c-4f08-8609-2769d71610cc.png)

* 桌面端是将近100起步，最多可到400左右
* 移动端相比之下就少的可怜（骁龙888的移动带宽是32）
* 带宽方面，两者有差不多十倍的差距


## 7.2 名词解释

* **System on Chip（Soc）**
Soc是把CPU、GPU、内存、通信基带、GPS模块等等整合在一起的芯片的称呼。
    * 常见有A系Soc（苹果），骁龙Soc（高通），麒麟Soc（华为），联发科Soc，猎户座Soc（三星），去年苹果推出的M系Soc，暂用于Mac，但这说明手机、笔记本和PC的通用芯片已经出现了

* **System Memory**
Soc中GPU和CPU共用一块片内LPDDR物理内存，就是我们常说的手机内存，也叫System Memory，大概几个G。
    * 此外CPU和GPU还分别有自己的高速SRAM的Cache缓存，也叫On-chip Memory，一般几百K~几M。不同距离的内存访问存在不同的时间消耗，距离越近消耗越低，读取System Memory的时间消耗大概是On-chip Memory的几倍到几十倍。
    * soc 上 gpu 和 cpu 共享一个内存地址空间

* **On-Chip Buffer**
    * 在TB(D)R架构下会存储Tile的颜色、深度和模板缓冲，读写修改都非常快。
    * 如果Load/Store指令中缓冲需要被Preserve，将会被写入一份到System Memory中。

* **Stall**
当一个GPU核心的两次计算结果之间有依赖关系而必须串行时，等待的过程便是Stall。

* **FillRate**
像素填充率 = ROP运行的时钟频率 x ROP的个数 x 每个时钟ROP可以处理的像素个数

* **TBR（Tile-Based (Deferred) Rendering）**
目前主流的移动GPU渲染架构，对应一般PC上的GPU渲染架构则是IMR（Immediate Mode Rendering ）。
    * TB(D)R 简单的意思：屏幕被分块（16*16像素或32*32像素）渲染
    * TBR流程：VS - Defer - RS - PS
    * TBDR流程：VS - Defer - RS - Defer - PS

* **Defer**
字面是延迟，但从渲染数据的角度来看，Defer就是“阻塞+批处理” GPU 的“一帧”的多个数据，然后一起处理。


## 7.3 立即渲染（IMR）

伪代码：

```
for draw in renderPass:
    for primitive in draw:
        for vertex in primitive:
            execute_vertex_shader(vertex)
        if primitive not culled:
            for fragment in primitive:
                execute_fragment_shader(fragment)
```

流程：
1. 先经过vertex shader的处理
2. 再进过一个类似管道（先进先出）的顺序，最终提交给Fragment Shader
3. Fragment Shader最终将结果刷到FrameBuffer里

![立即渲染（IMR）流程](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/dc438ea1-148f-4ba3-9897-f90b69c13820.png)

#### 详细流程：
出处：https://www.imgtec.com/blog/a-look-at-the-powervr-graphics-architecture-tile-based-rendering/

![IMR流程](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/442151c9-1b27-4e40-b415-1fcaf17e95ec.png)

用户数据 → 顶点处理 → 投影 剔除 → Raster → Early Visibility Test → Alpha Test → Late Visibility Test → Alpha Blend 最终将结果刷到FrameBuffer里

整个过程是直接和系统内存交互

## 7.4 基于块元的渲染（TB(D)R）
TB(D)R 宏观上总共分2个阶段：
1. 第一阶段执行所有与几何相关的处理，并生成Primitive List(图元列表)，并且确定每个tile上面有哪些primitive
2. 第二阶段将逐块执行光栅化及其后续处理，并在完成后将Frame Buffer 从Tile Buffer写回到System Memory中。

伪代码：

```
# Pass one
for draw in renderPass:
    for primitive in draw:
        for vertex in primitive:
            execute_vertex_shader(vertex)
        if primitive not culled:
            append tile list(primitive)

# Pass two
for tile in renderPass:
    for primitive in tile:
        for fragment in primitive:
            execute fragment shader(fragment)
```

## 7.5 TB(D)R 的硬件渲染顺序

简略流程：
数据通过Vertex shader的处理 → 经过Tiler（图元分好的块） → 刷到一块On-chip Memory（每个块元的内存）上

![TB(D)R流程](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/6034bb72-4c6a-4a1c-8644-5f916c29f8c2.png)

#### 详细流程：
出处：https://www.imgtec.com/blog/a-look-at-the-powervr-graphics-architecture-tile-based-rendering/

![TB(D)R流程详细](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/f486df39-e0ab-4051-beff-5bb123ba04cf.png)

* 上边虚线框代表片上的内存（On-Chip Buffer(Memory)），下边虚线框代表系统内存（System Memory）

与IMR的区别
* TBDR架构中，多了一步Tiling的过程，这一步是：将顶点处理经过剔除、投影的几何数据刷到系统内存(System Memory)上
* 经过光栅化（Raster）等流水线fragment shader、ROP等，最终把结果刷在片上内存(On Chip Memory)上
* 最终片上内存上的会刷到FrameBuffer上（FrameBuffer在System Memory上）

## 7.6 IMR 和 TB(D)R 对比

![IMR和TB(D)R对比](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/8ca6d040-3a7c-44a4-9799-81decd02370c.png)
图（a）TBR架构
* 几何处理数据形成了FrameData（放在System Memory上）
* 这些Frame Data经过片段处理，结果放在了Tile Buffer上（On-Chip Memory上）
* 最后的结果会刷到FrameBuffer中（System Memory上）

图（b）IMR架构
* 对比TBR有以下两种区别
    * 几何处理数据直接到片段处理，没有中间数据（Frame Data）
    * 直接刷到System Memory上了，没有经过片上内存（On-Chip Memory）

总体上看，TBR降低了功耗和带宽，但帧率上并不比IMR快

### TBR 示意图

![TBR示意图](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/e62b784b-8a46-4dad-af6c-9779fbb61bac.gif)

实际中 GPU 硬件中的乱序执行

![乱序执行](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241201225134.jpg)


### 总结
**TBR的核心目的是降低带宽，减少功耗，但渲染帧率上并不比IMR快**

**优点：**
1. TBR给消除Overdraw提供了机会，PowerVR用了HSR技术，Mali用了Forward Pixel Killing技术，目标一样，就是要最大限度减少被遮挡pixel的texturing和shading。
2. TBR主要是 cached friendly, 在cache里头的速度要比全局内存的速度快的多，以及有可能降低render rate的代价，降低带宽，省电

**缺点：**
1. 这个操作需要在vertex阶段之后，将输出的几何数据写入到DDR，然后才被fragment shader读取。这之间也就是tile写入DDR的开销和fragment shader渲染读取DDR开销的平衡。另外还有一些操作(比如tessellation)也不适用于TBR；
2. 如果某些三角形叠加在数个图块（Overdraw），则需要绘制数次。这意味着总渲染时间将高于即时渲染模式。


## 7.7 Binning 过程（第一个Defer）
将图元分配到对应的块元

> ![Binning过程](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/18d82964-50fc-4c59-b854-9d2c6fb49279.png)
> * 第二幅图里的红色三角形，只用一个块元就能渲染，所以它只会被分配到一个块元中
> * 第四幅图里的棕色三角形，需要多个块元才能渲染，所以它需要分配到9个块元中一起渲染

**测试工具**：Adreno Profiler
可以显示每个块元的性能参数，红线代表tile的范围

![测试工具：Adreno Profiler](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241202213455.png)

**binning过程的耗时占比参考**

![binning过程的耗时占比参考](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241202214018.png)

如果项目中binning过程相比其他耗时长的话，就要考虑一下是不是几何数据过多了


## 7.8 不同GPU的 Early-Depth-Test（第二个Defer）

### 7.8.1 Qualcomm Adreno的LDR（Android）
Qualcomm Adreno 采用外置模块LRZ。**在正常渲染管线前，先多执行一次VS 生成低精度depth texture**，以提前剔除不可见的triangles（或像素块？实现细节不知）。

说白了，**直接用硬件做occlusion culling**，功能类似软光栅遮挡剔除。因为做LRZ时执行VS只需用到position信息，所以单独抽出position stream，能带来bandwidth和cache的优化。


### 7.8.2 Mali的FPK（Android）
Arm Mali 采用Forward Pixel Kill技术 -Mali-T880

* 位于管线的位置：发生在Early-z之后
* 数据模型：先进先出的队列

简单概括一下：
* 队列中有4个Quad（可以理解为2×2像素的平面），每个Quad有屏幕上位置的数据和Z数据
* Z越大代表离摄像机越远
* 根据屏幕上相同位置（pos）的不同z，对不透明的像素进行替换（有近的就不渲染远的），这个过程叫作killed

FPK 过程：
![FPK 过程](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20241202215349.png)


### 7.8.3 PowerVR的HSR（iOS）
PowerVR以及后续自研GPU，采用内置模块HSR。 修改原渲染管线架构 ，增强rasterizer硬件模块为 HSR

* HSR = Hide Surface Removal隐形面剔除
* 大体实现原理：虚拟出一个射线，当它遇到第一个不透明的物体时就会停下来，这样就会打断后面三角形的后续ps处理

HSR 过程：
![HSR 过程](https://raw.githubusercontent.com/Ineloquent0/notes/main/images/d3bc377c-74ca-4d8e-adc5-934ebc1b5e2c.png)


## 7.9 优化建议

1. **记得不使用Framebuffer的时候clear或者discard**
    * 主要是清空积存在tile buff上的 frame Data
    * 在unity中，当不再使用这个render texture之前，调用一次Discard。
    * 在OpenGL ES上善用glClear，gllnvalidateFrameBuffer避免不必要的Resolve（刷system memory）行为

2. **不要在一帧里面频繁的切换framebuffer的绑定**
本质上就是减少tile buffer 和system memory之间的 的stall 操作

3. **对于移动平台，建议你使用 Alpha 混合，而非 Alpha 测试。**
在实际使用中，你应该分析并比较 Alpha 测试和 Alpha 混合的表现，因为这取决于具体内容，因此需要测量，通常在移动平台上应避免使用 Alpha 混合来实现透明。需要进行 Alpha 混合时，尝试缩小混合区域的覆盖范围

4. **手机上必须要做Alpha Test，先做一遍Depth prepass**，参考：[Alpha Test的双pass 优化思路](https://zhuanlan.zhihu.com/p/58017068)

5. **图片尽量压缩**，例如：ASTC  ETC2

6. **图片尽量走 mipmap**

7. **尽量使用从Vertex Shader传来的Varying变量UV值采样贴图（连续的），不要在FragmentShader里动态计算贴图的UV值（非连续的），否则CacheMiss**

8. **在延迟渲染尽量利用Tile Buffer 存储数据** 

9. 如果你在Unity 里面调整 ProjectSetting/Quality/Rendering/Texture Quality 不同的设置，或者不同的分辨率下，帧率有很多的变化，那么十有八九是带宽出问题啦

10. MSAA(增加对framebuffer读取的次数)其实在TBDR上反而是非常快速的。

11. **少在FS 中使用 discard 函数**，调用gl_FragDepth从而打断Early-DT( HLSL中为Clip，GLSL中为discard )

12. **尽可能的在Shader里使用Half Float**，如果Shader中仅有少量FP16的运算，且FP16需和FP32混合计算，则统一使用Float，好处：
    1. 带宽用量减少
    2. GPU中使用的周期数减少，因为着色器编译器可以优化你的代码以提高并行化程度。
    3. 要求的统一变量寄存器数量减少，这反过来又降低了寄存器数量溢出风险。具体有哪些数据类型适合用half或者float 或者fix，请查看：[熊大的优化建议](http://www.xionggf.com/post/unity3d/shader/u3d_shader_optimization/)

13. **在移动端的TB(D)R架构中，顶点处理部分，容易成为瓶颈，避免使用曲面细分shader，置换贴图等负操作**，提倡使用模型LOD,本质上减少FrameData的压力


## 参考资料
* [GPU性能指标 ]
https://www.gpuinsight.com/gpu_performance/
* [三星的GPU-FrameBuff指导]
https://developer.samsung.com/galaxy-gamedev/resources/articles/gpu-framebuffer.html
* [英伟达的TBR教学文章]
https://www.techpowerup.com/231129/on-nvidias-tile-based-rendering
* [ARM的TBR教学文章]
https://developer.arm.com/solutions/graphics-and-gaming/developer-guides/learn-the-basics/tile-based-rendering/single-page
* [苹果OpenGL程序开发指南]
https://developer.apple.com/library/archive/documentation/3DDrawing/Conceptual/OpenGLES_ProgrammingGuide/Performance/Performance.html
* [OpenGL Insights] 
https://www.seas.upenn.edu/~pcozzi/OpenGLInsights/OpenGLInsights-TileBasedArchitectures.pdf
* [知乎文章：Tile-based 和 Full-screen 方式的 Rasterization 相比有什么优劣]
https://www.zhihu.com/question/49141824
* [移动设备GPU架构知识汇总]
https://zhuanlan.zhihu.com/p/112120206
* [再议移动平台的AlphaTest效率问题]
https://zhuanlan.zhihu.com/p/33127345
* [移动平台GPU硬件学习与理解]
https://zhuanlan.zhihu.com/p/347001411
* [PowerVR开发者指南]
http://cdn.imgtec.com/sdk-documentation/Introduction_to_PowerVR_for_Developers.pdf
* [Performance Tunning for Tile-Based Architecture Tile-Based架构下的性能调校]
https://www.cnblogs.com/gameknife/p/3515714.html
* [TBDR的HSR流程细节和使用AlphaBlend的效率提升程度]
https://www.zhihu.com/question/49141824
* [当我们谈优化时，我们谈些什么]
https://zhuanlan.zhihu.com/p/68158277
https://edu.uwa4d.com/course-intro/1/179
* [Alpha Test的双pass 优化思路]
https://zhuanlan.zhihu.com/p/58017068
* [个人收藏]
https://github.com/killop/anything_about_game#gpu-architecture
* [Adreno Hardware Tutorial 3: Tile Based Rendering]
https://www.youtube.com/watch?v=SeySx0TkluE&pbjreload=101
* [Mali GPU的独有特性]
https://www.cnblogs.com/hamwj1991/p/12404551.html
* [Mali-T880]
http://grmanet.sogang.ac.kr/ihm/cs170/20/HC27.25.531-Mali-T880-Bratt-ARM-2015_08_23.pdf
* [[熊大的优化建议](http://www.xionggf.com/)]
http://www.xionggf.com/post/unity3d/shader/u3d_shader_optimization/
* [GPU画像素的顺序是什么]
https://zhuanlan.zhihu.com/p/22232448
* [Tile-based Rasterization in Nvidia GPUs with David Kanter of Real World Tech]
https://www.youtube.com/watch?v=Nc6R1hwXhL8&t=973s&pbjreload=101



----------------------------------------------------------------------


# 8. Command Buffer及URP概述

## 8.1 什么是 Command Buffer
Command Buffer：用来存储渲染命令的缓冲区。

Command Buffer保存着渲染命令列表，如(set render target, draw mesh等等)，可以设置为在摄像机渲染期间的不同点执行。

## 8.2 Command Buffer用来干什么
参考：
https://github.com/Doppelkeks/Unity-CommandBufferRefraction
https://github.com/Arihide/unity-selective-outline

<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/20250112223254.png" alt="20250112223254">

上图的扭曲效果，使用Command Buffer去获取，
* 在渲染着图中的三个物体之前的Color Buffer，也就是背景，将其保存下来，作为一张Render Texture。
* 当渲染图中这三个物体的时候，在片元着色器中使用切线空间的法线XY对其屏幕空间的坐标做一个偏移。
* 再使用UV去采样刚才保存的那个RenderTexture。

第二个例子是使用多Pass让物体在被选择的时候，渲染外扩描边，这个例子的主要做法是
* 挂了一个MonoBehaviour脚本，在场景中使用的是ExcuteInEditorMode的HeaderAttribute,可以让脚本能够在EditMode也能够运行。
* 然后为了让保存着绘制外扩描边的Command Buffer在befor Image Effect这个时间节点进行注入，绘制的过程简单的来说是申请了一张RenderTexture,将其设置为RenderTarget，然后使用绘制描边的Material去绘制，选择中的Render用的是Draw Renderer的指令。

当然了，绘制的方式，使用的函数，也不止一种。
用刚才绘制的结果和相机BackBuffer进行合成，**Command Buffer实际上是用来帮助我们告诉管线，下一步应该如何绘制，该绘制什么**

### 从Opengl指令分析CommandBuffer原理

<div align=center>
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/1ca4c362-7f4c-48ed-b4c2-2e6081ec873d.png" alt="1ca4c362-7f4c-48ed-b4c2-2e6081ec873d" height="500">
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/c935f602-551b-4170-a728-ce805918e077.png" alt="c935f602-551b-4170-a728-ce805918e077" height="500">
</div>

第一张图是在对BRDF的镜面高光IBL项进行预积分渲染一张LUT，对每个粗糙度和入射角组合的响应结果存储在一张2D的LUT当中，计算出菲涅项的系数值(R通道)和偏移值(G通道)计算间接光的镜面高光项时：`vec3 indirectSpecular = prefilteredColor * (F * envBRDF.x + envBRDF.y)`，然后再正式渲染的时候就是需要引用绑定刚刚预积分渲染的Texture。

在整个渲染流程，如图1所示，给每一个小球设置好MVP矩阵，然后渲染skybox，如此循环。这里看不到整个LUTTextureGeneratePass是因为我把预积分项计算移动到Loop之外，这里的Debug是在Loop之内。所以整个渲染流程是分为一个一个Pass进行编写的话，更加容易辨识在每个渲染流程中应该做什么，计算出什么结果。

<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/414a9f4e-e21e-4095-9269-6a065672903b.png" alt="414a9f4e-e21e-4095-9269-6a065672903b" width="500">

在Opengl中，渲染过程中，出于渲染流程的需要，会频繁绑定VAO,VBO,FBO等。这样重复性的操作，很是让人感觉比较麻烦，也不利于程序的扩展。

因此，在我自己写Opengl的时候，也会根据自己的需要，会将一些重复性的操作”提取”出来：**使用Shader类负责编译，在Model类添加一些读取模型，绑定VAO,VBO操作函数**等等。在渲染流程中常常也会需要用到上一个渲染阶段的结果作为输入，最常见的操作就是后处理阶段，需要把上一阶段的frame buffer 绑定到当前输入。

出于跨平台的需要Unity对这些底层的API也做了一层封装处理，然后就产生了Command Buffer.


<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/d32b3579-c362-44f2-a3a7-f4f1fcb556b9.png" alt="d32b3579-c362-44f2-a3a7-f4f1fcb556b9">

回到Unity里，打开FrameDebugger，你可以看到场景的物体的渲染流程，对应ForwardRenderer.cs脚本中的

FrameDebugger | ForwardRenderer.cs
-- | --
DepthOnlyPass | m_DepthPrepass
DepthNormalOnlyPass | m_DepthNormalPrepass
MainLightShadowCasterPass | m_MainLightShadowCasterPass
DepthOnlyPass | m_DepthPrepass
ColorGradingLutPass | m_ColorGradingLutPass
DrawObjectsPass | m_RenderOpaqueForwardPass
DrawSkyboxPass | m_DrawSkyboxPass
CopyColorPass | m_CopyColorPass
PostProcessPass | m_PostProcessPass
PostProcessPass | m_FinalPostProcessPass


<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/416e50ab-4fb5-4f7a-8a41-6cbf12ffba01.png" alt="416e50ab-4fb5-4f7a-8a41-6cbf12ffba01">
<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/691b47f5-3b01-474b-a271-c6c6f253a920.png" alt="691b47f5-3b01-474b-a271-c6c6f253a920">

而为了更好扩展unity的渲染管线，unity提供了CommandBuffer，让你根据自己的需求，在不同的渲染阶段插入绘制指令。
例如插入DrawRenderer，DrawMesh，DrawProcedure，绘制的时候也可以根据需要设置绘制时材质(Material)的MaterialPropertyBlock更改当前绘制的材质的属性。

这里我用了RenderFeature在BeforeRenderingOpaques的时候用DrawMesh指令渲染了一个Box。

PS:MaterialPropertyBlock比直接修改Material的优势是：不会创建出新的材质实例。


### 从自定义 RenderPipeline 分析 ScriptableRenderContext 与 CommandBuffer
相信接触过SRP的编写的同学都看过CatlikeCoding 。
本节就先从最简单的自定义RenderPipeline开始分析如何构造一个Scriptable Render Pipeline。然后再分析Pass的概念以及CommandBuffer和ScriptableRenderContext之间的关系。

#### RenderPipeline结构

<img src="https://raw.githubusercontent.com/Ineloquent0/notes/main/images/46c66fd1-2451-4d9d-a2f0-462457859f99.png" alt="46c66fd1-2451-4d9d-a2f0-462457859f99">

* 首先先写一个继承自RenderPipeline的自定义管线类。
类中有一个CameraRenderer,CameraRenderer主要负责的是渲染的主要逻辑，Render的方法需要重写(Override)参数也是固定的，可以看到这里的Cameras是一个数组,说明了当场景里有多个相机时，需要我们在这个RenderPipeline的Render函数中去遍历所有的Camera进行一个处理操作。
















